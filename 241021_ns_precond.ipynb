{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from typing import *\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import functools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "x = U S V^T\n",
    "xn = U V^T\n",
    "xn.T @ x = V S V^T\n",
    "x @ xn.T = U S U^T\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(345, 543).to(torch.bfloat16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from fractions import Fraction\n",
    "\n",
    "import logging\n",
    "logger: logging.Logger = logging.getLogger('test_muon')\n",
    "\n",
    "def _matrix_root_eigen(\n",
    "    A: Tensor,\n",
    "    root: Union[Fraction, int],\n",
    "    epsilon: float = 0.0,\n",
    "    exponent_multiplier: float = 1.0,\n",
    "    make_positive_semidefinite: bool = True,\n",
    "    retry_double_precision: bool = True,\n",
    ") -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Compute matrix inverse root using eigendecomposition of symmetric positive (semi-)definite matrix.\n",
    "\n",
    "            A^{-1/r} = Q L^{-1/r} Q^T\n",
    "\n",
    "    Assumes matrix A is symmetric.\n",
    "\n",
    "    Args:\n",
    "        A (Tensor): Square matrix of interest.\n",
    "        root (int): Root of interest. Any natural number.\n",
    "        epsilon (float): Adds epsilon * I to matrix before taking matrix root. (Default: 0.0)\n",
    "        exponent_multiplier (float): exponent multiplier in the eigen method (Default: 1.0)\n",
    "        make_positive_semidefinite (bool): Perturbs matrix eigenvalues to ensure it is numerically positive semi-definite. (Default: True)\n",
    "        retry_double_precision (bool): Flag for re-trying eigendecomposition with higher precision if lower precision fails due\n",
    "            to CuSOLVER failure. (Default: True)\n",
    "\n",
    "    Returns:\n",
    "        X (Tensor): (Inverse) root of matrix. Same dimensions as A.\n",
    "        L (Tensor): Eigenvalues of A.\n",
    "        Q (Tensor): Orthogonal matrix consisting of eigenvectors of A.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # check if root is positive integer\n",
    "    if root <= 0:\n",
    "        raise ValueError(f\"Root {root} should be positive!\")\n",
    "\n",
    "    # compute matrix power\n",
    "    alpha = -exponent_multiplier / root\n",
    "\n",
    "    # compute eigendecomposition and compute minimum eigenvalue\n",
    "    try:\n",
    "        L, Q = torch.linalg.eigh(A)\n",
    "\n",
    "    except Exception as exception:\n",
    "        if retry_double_precision and A.dtype != torch.float64:\n",
    "            logger.warning(\n",
    "                f\"Failed to compute eigendecomposition in {A.dtype} precision with exception {exception}! Retrying in double precision...\"\n",
    "            )\n",
    "            L, Q = torch.linalg.eigh(A.double())\n",
    "        else:\n",
    "            raise exception\n",
    "\n",
    "    lambda_min = torch.min(L)\n",
    "\n",
    "    # make eigenvalues >= 0 (if necessary)\n",
    "    if make_positive_semidefinite:\n",
    "        L += -torch.minimum(lambda_min, torch.as_tensor(0.0))\n",
    "\n",
    "    # add epsilon\n",
    "    L += epsilon\n",
    "\n",
    "    # compute inverse preconditioner\n",
    "    X = Q * L.pow(alpha).unsqueeze(0) @ Q.T\n",
    "\n",
    "    return X, L, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "xn_long = muon_renorm.zeropower_via_newtonschulz5(\n",
    "    x.float(), steps=18,\n",
    "    abc=muon_renorm.make_schedule(8, 2, 8),\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "xn = muon_renorm.zeropower_via_newtonschulz5(\n",
    "    x, steps=5,\n",
    ")\n",
    "U, S, V = x.double().svd()\n",
    "xn_cheat = (U @ V[..., :xn.shape[0]].T).to(torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "_xn = x.float().svd().U @ x.float().svd().V.T\n",
    "_xn = _xn.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0010, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xn_long - _xn).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13.3335, -0.3297,  0.4140,  ..., -0.0603,  0.1835, -0.0317],\n",
       "        [-0.3297, 14.3710,  0.0423,  ...,  0.4852,  0.3412, -0.9058],\n",
       "        [ 0.4140,  0.0423, 12.7372,  ...,  0.8726,  0.2346,  0.9779],\n",
       "        ...,\n",
       "        [-0.0603,  0.4852,  0.8726,  ..., 12.9309,  1.3419, -0.3291],\n",
       "        [ 0.1835,  0.3412,  0.2346,  ...,  1.3419, 14.7677, -0.6373],\n",
       "        [-0.0317, -0.9058,  0.9779,  ..., -0.3291, -0.6373, 12.6676]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.float().svd().V @ x.float().svd().S.diag() @ x.float().svd().V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.6875, -0.3086,  0.4766,  ..., -0.1875,  0.2734, -0.0513],\n",
       "        [-0.3184, 13.5625, -0.0713,  ...,  0.4961,  0.2021, -0.7969],\n",
       "        [ 0.4590, -0.0762, 12.1250,  ...,  0.5625,  0.0645,  0.8711],\n",
       "        ...,\n",
       "        [-0.1924,  0.5117,  0.5664,  ..., 12.1875,  1.0156, -0.3223],\n",
       "        [ 0.2617,  0.2334,  0.0703,  ...,  1.0234, 14.0000, -0.7930],\n",
       "        [-0.0645, -0.8008,  0.8594,  ..., -0.3008, -0.7773, 11.9375]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.8125, -0.3535,  0.4297,  ..., -0.2324,  0.2852, -0.0718],\n",
       "        [-0.3633, 13.8125, -0.0957,  ...,  0.4473,  0.2129, -0.8047],\n",
       "        [ 0.4375, -0.0422, 12.3125,  ...,  0.5859,  0.0566,  0.8398],\n",
       "        ...,\n",
       "        [-0.2139,  0.4844,  0.6328,  ..., 12.3750,  1.0391, -0.3047],\n",
       "        [ 0.2949,  0.2168,  0.0581,  ...,  1.0469, 14.1875, -0.7969],\n",
       "        [-0.0267, -0.8086,  0.8516,  ..., -0.2949, -0.8242, 12.1250]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xn.T @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to compute eigendecomposition in torch.bfloat16 precision with exception \"linalg_eigh_cuda\" not implemented for 'BFloat16'! Retrying in double precision...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.5655e-01, -5.5646e-03,  4.4095e-03,  ...,  5.1869e-03,\n",
       "          -4.6841e-03,  5.9985e-03],\n",
       "         [-5.5646e-03,  2.5677e-01,  1.4152e-03,  ..., -2.3429e-03,\n",
       "           7.6731e-04, -2.8111e-03],\n",
       "         [ 4.4095e-03,  1.4152e-03,  2.5578e-01,  ...,  3.9613e-03,\n",
       "           4.4327e-03, -6.1242e-04],\n",
       "         ...,\n",
       "         [ 5.1869e-03, -2.3429e-03,  3.9613e-03,  ...,  2.6039e-01,\n",
       "           1.1045e-04, -3.8337e-03],\n",
       "         [-4.6841e-03,  7.6731e-04,  4.4327e-03,  ...,  1.1045e-04,\n",
       "           2.4128e-01,  2.3840e-03],\n",
       "         [ 5.9985e-03, -2.8111e-03, -6.1242e-04,  ..., -3.8337e-03,\n",
       "           2.3840e-03,  2.4497e-01]], device='cuda:0', dtype=torch.float64),\n",
       " tensor([ 3.7722,  4.0749,  4.4163,  4.8264,  4.9394,  5.1295,  5.2883,  5.3255,\n",
       "          5.5114,  5.5390,  5.5397,  5.5595,  5.5858,  5.5981,  5.6070,  5.6165,\n",
       "          5.6262,  5.6388,  5.6643,  5.6742,  5.6976,  5.7026,  5.7205,  5.7441,\n",
       "          5.7581,  5.7875,  5.8012,  5.8225,  5.8370,  5.8377,  5.8633,  5.8799,\n",
       "          5.8889,  5.9209,  5.9291,  5.9916,  6.0575,  6.1240,  6.2052,  6.2788,\n",
       "          6.5397,  6.5683,  6.9622,  7.0805,  7.2071,  7.5205,  7.5740,  7.9491,\n",
       "          8.1441,  8.1845,  8.4432,  8.9084,  9.2963,  9.3757,  9.5974,  9.6931,\n",
       "          9.8861, 10.0482, 10.3522, 10.5459, 10.7949, 11.0512, 11.1869, 11.3502,\n",
       "         11.4711, 11.6142, 11.9750, 12.2174, 12.4422, 12.6513, 12.9622, 13.0076,\n",
       "         13.2113, 13.3079, 13.4966, 13.6181, 13.7094, 13.8288, 13.9759, 14.1174,\n",
       "         14.2226, 14.5218, 14.5631, 14.7138, 14.7867, 14.9123, 15.0398, 15.2869,\n",
       "         15.3180, 15.4146, 15.5255, 15.6083, 15.7221, 15.8104, 15.8982, 15.9729,\n",
       "         16.0433, 16.0632, 16.0859, 16.1075, 16.1399, 16.1717, 16.1883, 16.1965,\n",
       "         16.2048, 16.2193, 16.2296, 16.2604, 16.2737, 16.2969, 16.3103, 16.3144,\n",
       "         16.3466, 16.3716, 16.3864, 16.4082, 16.4160, 16.4302, 16.4457, 16.4617,\n",
       "         16.4745, 16.5005, 16.5070, 16.5279, 16.5395, 16.5524, 16.5627, 16.5987,\n",
       "         16.6010, 16.6236, 16.6505, 16.6620, 16.6830, 16.6843, 16.7221, 16.7449,\n",
       "         16.7711, 16.7873, 16.8069, 16.8263, 16.8509, 16.8572, 16.8717, 16.9122,\n",
       "         16.9219, 16.9421, 16.9575, 16.9760, 17.0022, 17.0143, 17.0272, 17.0464,\n",
       "         17.0778, 17.0936, 17.1102, 17.1383, 17.1718, 17.1801, 17.1874, 17.2265,\n",
       "         17.2331, 17.2425, 17.2457, 17.2569, 17.2735, 17.2901, 17.3048, 17.3370,\n",
       "         17.3403, 17.3449, 17.3543, 17.3710, 17.3893, 17.3981, 17.4052, 17.4260,\n",
       "         17.4298, 17.4444, 17.4485, 17.4676, 17.4847, 17.4924, 17.5289, 17.5601,\n",
       "         17.5930, 17.6270, 17.7399, 17.7863, 17.9829, 18.0508, 18.1196, 18.3943,\n",
       "         18.5742, 18.7698, 18.8140, 18.9879, 19.2589, 19.4728, 19.6544, 19.8127,\n",
       "         19.8731, 20.1914, 20.3099, 20.4426, 20.7802, 21.0699, 21.2563, 21.4699,\n",
       "         21.5041, 21.6707, 21.9139, 22.1150, 22.2486, 22.7376, 23.0533, 23.1331,\n",
       "         23.3829, 23.6377, 23.8730, 24.0792, 24.4513, 24.8377, 24.9839, 25.3672,\n",
       "         25.4761, 25.6785, 25.9966, 26.3709, 26.5339, 26.6092, 26.6369, 26.7010,\n",
       "         26.7139, 26.7367, 26.7734, 26.8327, 26.9215, 26.9770, 27.0073, 27.0428,\n",
       "         27.0769, 27.1355, 27.1549, 27.2039, 27.3875, 27.4259, 27.5243, 27.6183,\n",
       "         27.6965, 27.7379, 27.9030, 27.9383, 27.9810, 28.1085, 28.2420, 28.4164,\n",
       "         28.5647, 28.5998, 28.7006, 28.9033, 29.1219, 29.2509, 29.4535, 29.4690,\n",
       "         29.6170, 29.6629, 29.7756, 29.9077, 29.9257, 30.1856, 30.2764, 30.3167,\n",
       "         30.4861, 30.5542, 30.7819, 30.8508, 30.9865, 31.0058, 31.1456, 31.3279,\n",
       "         31.3571, 31.4450, 31.5057, 31.5303, 31.7542, 31.8662, 31.8909, 32.0509,\n",
       "         32.1333, 32.2579, 32.3638, 32.3811, 32.4556, 32.5606, 32.6678, 32.7469,\n",
       "         32.8823, 32.9719, 33.1195, 33.2578, 33.2661, 33.3692, 33.4138, 33.6291,\n",
       "         33.6517, 33.7002, 33.7274, 33.8361, 33.9384, 33.9840, 33.9992, 34.1105,\n",
       "         34.2343, 34.2862, 34.3072, 34.3750, 34.3994, 34.4582, 34.5244, 34.5534,\n",
       "         34.6195, 34.6367, 34.6600, 34.7340, 34.7859, 34.7931, 34.8549, 34.9118,\n",
       "         34.9504, 34.9582, 34.9887, 35.0470, 35.0691, 35.1394, 35.1422, 35.1599,\n",
       "         35.1731, 35.2024, 35.2430, 35.2724, 35.3091, 35.3303, 35.3561, 35.4069,\n",
       "         35.4220], device='cuda:0', dtype=torch.float64),\n",
       " tensor([[ 0.0268, -0.1423,  0.0123,  ..., -0.0100,  0.0686, -0.0125],\n",
       "         [-0.0069,  0.0137, -0.0548,  ...,  0.0308,  0.1032,  0.0404],\n",
       "         [-0.0299, -0.0846, -0.0819,  ..., -0.0459,  0.1157, -0.0296],\n",
       "         ...,\n",
       "         [-0.0407, -0.0519, -0.0386,  ...,  0.0311,  0.0062, -0.0138],\n",
       "         [-0.0436,  0.0164, -0.0839,  ..., -0.0100,  0.0013,  0.0465],\n",
       "         [ 0.0580, -0.0801,  0.0753,  ...,  0.0188, -0.0067, -0.0206]],\n",
       "        device='cuda:0', dtype=torch.float64))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_matrix_root_eigen(x @ xn.T, 2, epsilon=1e-12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to compute eigendecomposition in torch.bfloat16 precision with exception \"linalg_eigh_cuda\" not implemented for 'BFloat16'! Retrying in double precision...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+03, 1.4783e+01, 9.5640e+00, 7.6839e+00, 6.0208e+00, 5.6663e+00,\n",
       "        5.2810e+00, 4.9966e+00, 4.6449e+00, 4.5857e+00, 4.2507e+00, 4.1610e+00,\n",
       "        4.0042e+00, 3.9881e+00, 3.9507e+00, 3.7453e+00, 3.6993e+00, 3.6328e+00,\n",
       "        3.5699e+00, 3.5113e+00, 3.4616e+00, 3.4022e+00, 3.3537e+00, 3.3207e+00,\n",
       "        3.2948e+00, 3.1739e+00, 3.1164e+00, 3.1013e+00, 3.0609e+00, 2.9961e+00,\n",
       "        2.9704e+00, 2.9365e+00, 2.9206e+00, 2.8937e+00, 2.8411e+00, 2.8160e+00,\n",
       "        2.8048e+00, 2.7577e+00, 2.7483e+00, 2.7209e+00, 2.7112e+00, 2.6802e+00,\n",
       "        2.6138e+00, 2.6037e+00, 2.5827e+00, 2.5814e+00, 2.5549e+00, 2.5319e+00,\n",
       "        2.5116e+00, 2.4976e+00, 2.4842e+00, 2.4593e+00, 2.4305e+00, 2.4083e+00,\n",
       "        2.3956e+00, 2.3949e+00, 2.3691e+00, 2.3569e+00, 2.3419e+00, 2.3243e+00,\n",
       "        2.3080e+00, 2.2944e+00, 2.2883e+00, 2.2756e+00, 2.2584e+00, 2.2527e+00,\n",
       "        2.2404e+00, 2.2180e+00, 2.2151e+00, 2.2106e+00, 2.1897e+00, 2.1705e+00,\n",
       "        2.1556e+00, 2.1511e+00, 2.1481e+00, 2.1358e+00, 2.1243e+00, 2.1139e+00,\n",
       "        2.1045e+00, 2.1000e+00, 2.0915e+00, 2.0840e+00, 2.0820e+00, 2.0773e+00,\n",
       "        2.0690e+00, 2.0407e+00, 2.0346e+00, 2.0332e+00, 2.0275e+00, 2.0215e+00,\n",
       "        2.0024e+00, 1.9983e+00, 1.9882e+00, 1.9824e+00, 1.9798e+00, 1.9618e+00,\n",
       "        1.9534e+00, 1.9485e+00, 1.9469e+00, 1.9439e+00, 1.9363e+00, 1.9299e+00,\n",
       "        1.9097e+00, 1.9068e+00, 1.9003e+00, 1.8980e+00, 1.8898e+00, 1.8807e+00,\n",
       "        1.8716e+00, 1.8695e+00, 1.8647e+00, 1.8585e+00, 1.8538e+00, 1.8486e+00,\n",
       "        1.8461e+00, 1.8359e+00, 1.8332e+00, 1.8297e+00, 1.8221e+00, 1.8132e+00,\n",
       "        1.8097e+00, 1.8031e+00, 1.7994e+00, 1.7905e+00, 1.7836e+00, 1.7801e+00,\n",
       "        1.7758e+00, 1.7678e+00, 1.7623e+00, 1.7559e+00, 1.7504e+00, 1.7494e+00,\n",
       "        1.7450e+00, 1.7379e+00, 1.7315e+00, 1.7285e+00, 1.7197e+00, 1.7174e+00,\n",
       "        1.7081e+00, 1.7022e+00, 1.6998e+00, 1.6920e+00, 1.6880e+00, 1.6796e+00,\n",
       "        1.6770e+00, 1.6694e+00, 1.6582e+00, 1.6544e+00, 1.6498e+00, 1.6444e+00,\n",
       "        1.6434e+00, 1.6396e+00, 1.6331e+00, 1.6275e+00, 1.6212e+00, 1.6168e+00,\n",
       "        1.6091e+00, 1.6071e+00, 1.6008e+00, 1.5951e+00, 1.5928e+00, 1.5849e+00,\n",
       "        1.5814e+00, 1.5743e+00, 1.5720e+00, 1.5694e+00, 1.5644e+00, 1.5615e+00,\n",
       "        1.5567e+00, 1.5529e+00, 1.5513e+00, 1.5469e+00, 1.5381e+00, 1.5280e+00,\n",
       "        1.5223e+00, 1.5200e+00, 1.5172e+00, 1.5043e+00, 1.5012e+00, 1.4966e+00,\n",
       "        1.4900e+00, 1.4815e+00, 1.4793e+00, 1.4711e+00, 1.4657e+00, 1.4618e+00,\n",
       "        1.4565e+00, 1.4475e+00, 1.4442e+00, 1.4417e+00, 1.4359e+00, 1.4250e+00,\n",
       "        1.4183e+00, 1.4113e+00, 1.4025e+00, 1.3916e+00, 1.3881e+00, 1.3761e+00,\n",
       "        4.9810e-01, 4.7858e-01, 4.6187e-01, 4.4079e-01, 4.3887e-01, 4.2845e-01,\n",
       "        4.2366e-01, 4.2250e-01, 4.1514e-01, 4.1470e-01, 4.1407e-01, 4.1367e-01,\n",
       "        4.1309e-01, 4.1265e-01, 4.1223e-01, 4.1177e-01, 4.1122e-01, 4.1102e-01,\n",
       "        4.1039e-01, 4.0994e-01, 4.0968e-01, 4.0897e-01, 4.0861e-01, 4.0752e-01,\n",
       "        4.0670e-01, 4.0653e-01, 4.0576e-01, 4.0572e-01, 4.0502e-01, 4.0479e-01,\n",
       "        4.0357e-01, 4.0342e-01, 4.0317e-01, 4.0229e-01, 4.0177e-01, 4.0044e-01,\n",
       "        3.9907e-01, 3.9503e-01, 3.9261e-01, 3.9098e-01, 3.8397e-01, 3.8307e-01,\n",
       "        3.7112e-01, 3.6909e-01, 3.6541e-01, 3.5836e-01, 3.5756e-01, 3.4937e-01,\n",
       "        3.4452e-01, 3.4388e-01, 3.3873e-01, 3.2965e-01, 3.2352e-01, 3.2235e-01,\n",
       "        3.1887e-01, 3.1659e-01, 3.1421e-01, 3.1175e-01, 3.0637e-01, 3.0435e-01,\n",
       "        3.0064e-01, 2.9755e-01, 2.9530e-01, 2.9309e-01, 2.9179e-01, 2.8994e-01,\n",
       "        2.8602e-01, 2.8269e-01, 2.8032e-01, 2.7810e-01, 2.7511e-01, 2.7426e-01,\n",
       "        2.7221e-01, 2.7100e-01, 2.6973e-01, 2.6810e-01, 2.6725e-01, 2.6614e-01,\n",
       "        2.6496e-01, 2.6346e-01, 2.6235e-01, 2.6022e-01, 2.5981e-01, 2.5818e-01,\n",
       "        2.5749e-01, 2.5688e-01, 2.5548e-01, 2.5366e-01, 2.5334e-01, 2.5239e-01,\n",
       "        2.5147e-01, 2.5102e-01, 2.4995e-01, 2.4891e-01, 2.4820e-01, 2.4773e-01,\n",
       "        2.4750e-01, 2.4741e-01, 2.4735e-01, 2.4695e-01, 2.4685e-01, 2.4672e-01,\n",
       "        2.4649e-01, 2.4638e-01, 2.4631e-01, 2.4622e-01, 2.4603e-01, 2.4590e-01,\n",
       "        2.4577e-01, 2.4566e-01, 2.4552e-01, 2.4543e-01, 2.4538e-01, 2.4507e-01,\n",
       "        2.4499e-01, 2.4493e-01, 2.4482e-01, 2.4477e-01, 2.4469e-01, 2.4446e-01,\n",
       "        2.4431e-01, 2.4421e-01, 2.4412e-01, 2.4403e-01, 2.4394e-01, 2.4377e-01,\n",
       "        2.4369e-01, 2.4350e-01, 2.4343e-01, 2.4335e-01, 2.4318e-01, 2.4301e-01,\n",
       "        2.4299e-01, 2.4268e-01, 2.4266e-01, 2.4263e-01, 2.4235e-01, 2.4215e-01,\n",
       "        2.4214e-01, 2.4193e-01, 2.4178e-01, 2.4165e-01, 2.4135e-01, 2.4132e-01,\n",
       "        2.4120e-01, 2.4106e-01, 2.4087e-01, 2.4072e-01, 2.4051e-01, 2.4044e-01,\n",
       "        2.4035e-01, 2.4020e-01, 2.4014e-01, 2.3994e-01, 2.3983e-01, 2.3959e-01,\n",
       "        2.3947e-01, 2.3941e-01, 2.3935e-01, 2.3924e-01, 2.3911e-01, 2.3906e-01,\n",
       "        2.3896e-01, 2.3887e-01, 2.3874e-01, 2.3860e-01, 2.3852e-01, 2.3841e-01,\n",
       "        2.3837e-01, 2.3828e-01, 2.3818e-01, 2.3813e-01, 2.3806e-01, 2.3797e-01,\n",
       "        2.3787e-01, 2.3778e-01, 2.3768e-01, 2.3763e-01, 2.3740e-01, 2.3737e-01,\n",
       "        2.3724e-01, 2.3705e-01, 2.3688e-01, 2.3680e-01, 2.3677e-01, 2.3642e-01,\n",
       "        2.3571e-01, 2.3524e-01, 2.3401e-01, 2.3385e-01, 2.3320e-01, 2.3169e-01,\n",
       "        2.3034e-01, 2.2903e-01, 2.2880e-01, 2.2784e-01, 2.2616e-01, 2.2502e-01,\n",
       "        2.2403e-01, 2.2317e-01, 2.2269e-01, 2.2112e-01, 2.2044e-01, 2.1978e-01,\n",
       "        2.1788e-01, 2.1652e-01, 2.1538e-01, 2.1459e-01, 2.1428e-01, 2.1340e-01,\n",
       "        2.1222e-01, 2.1127e-01, 2.1070e-01, 2.0856e-01, 2.0709e-01, 2.0654e-01,\n",
       "        2.0577e-01, 2.0461e-01, 2.0351e-01, 2.0261e-01, 2.0147e-01, 1.9975e-01,\n",
       "        1.9910e-01, 1.9751e-01, 1.9723e-01, 1.9637e-01, 1.9522e-01, 1.9386e-01,\n",
       "        1.9306e-01, 1.9288e-01, 1.9274e-01, 1.9261e-01, 1.9248e-01, 1.9231e-01,\n",
       "        1.9228e-01, 1.9218e-01, 1.9198e-01, 1.9169e-01, 1.9156e-01, 1.9130e-01,\n",
       "        1.9117e-01, 1.9106e-01, 1.9102e-01, 1.9078e-01, 1.9014e-01, 1.9006e-01,\n",
       "        1.8951e-01, 1.8932e-01, 1.8917e-01, 1.8900e-01, 1.8861e-01, 1.8822e-01,\n",
       "        1.8808e-01, 1.8774e-01, 1.8719e-01, 1.8662e-01, 1.8616e-01, 1.8608e-01,\n",
       "        1.8576e-01, 1.8516e-01, 1.8434e-01, 1.8386e-01, 1.8349e-01, 1.8333e-01,\n",
       "        1.8284e-01, 1.8275e-01, 1.8243e-01, 1.8208e-01, 1.8202e-01, 1.8133e-01,\n",
       "        1.8087e-01, 1.8073e-01, 1.8025e-01, 1.7999e-01, 1.7956e-01, 1.7923e-01,\n",
       "        1.7896e-01, 1.7888e-01, 1.7855e-01, 1.7798e-01, 1.7782e-01, 1.7743e-01,\n",
       "        1.7738e-01, 1.7715e-01, 1.7659e-01, 1.7639e-01, 1.7633e-01, 1.7592e-01,\n",
       "        1.7558e-01, 1.7545e-01, 1.7503e-01, 1.7491e-01, 1.7465e-01, 1.7452e-01,\n",
       "        1.7437e-01, 1.7409e-01, 1.7365e-01, 1.7345e-01, 1.7306e-01, 1.7269e-01,\n",
       "        1.7255e-01, 1.7249e-01, 1.7228e-01, 1.7179e-01, 1.7175e-01, 1.7166e-01,\n",
       "        1.7146e-01, 1.7123e-01, 1.7098e-01, 1.7081e-01, 1.7079e-01, 1.7053e-01,\n",
       "        1.7017e-01, 1.7007e-01, 1.6997e-01, 1.6991e-01, 1.6976e-01, 1.6960e-01,\n",
       "        1.6955e-01, 1.6951e-01, 1.6941e-01, 1.6927e-01, 1.6915e-01, 1.6896e-01,\n",
       "        1.6886e-01, 1.6883e-01, 1.6871e-01, 1.6867e-01, 1.6860e-01, 1.6851e-01,\n",
       "        1.6842e-01, 1.6824e-01, 1.6823e-01, 1.6822e-01, 1.6810e-01, 1.6802e-01,\n",
       "        1.6792e-01, 1.6790e-01, 1.6780e-01, 1.6773e-01, 1.6763e-01, 1.6757e-01,\n",
       "        1.6751e-01, 1.6741e-01, 1.6734e-01], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_matrix_root_eigen(xn.T @ x, 2, epsilon=1e-6)[0].svd().S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0359, -0.0028,  0.0004,  ..., -0.0027,  0.0005,  0.0005],\n",
       "        [-0.0028,  0.0418,  0.0038,  ...,  0.0005,  0.0003, -0.0004],\n",
       "        [ 0.0004,  0.0038,  0.0367,  ..., -0.0014, -0.0019, -0.0018],\n",
       "        ...,\n",
       "        [-0.0027,  0.0005, -0.0014,  ...,  0.0428,  0.0019,  0.0011],\n",
       "        [ 0.0005,  0.0003, -0.0019,  ...,  0.0019,  0.0393,  0.0018],\n",
       "        [ 0.0005, -0.0004, -0.0018,  ...,  0.0011,  0.0018,  0.0394]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.double().svd().V @ x.double().svd().S.pow(-1).diag() @ x.double().svd().V.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41.4167, 40.8357, 40.4905, 40.3890, 39.9276, 39.6737, 39.5611, 39.2824,\n",
       "        39.1043, 38.9433, 38.7521, 38.6324, 38.5083, 38.2517, 37.9836, 37.8075,\n",
       "        37.6660, 37.5274, 37.4257, 37.2361, 37.0193, 36.9609, 36.6816, 36.5950,\n",
       "        36.3440, 36.2487, 36.1401, 35.9429, 35.6863, 35.5173, 35.3303, 35.2366,\n",
       "        35.1712, 34.9059, 34.8248, 34.7916, 34.6254, 34.5272, 34.3327, 34.1458,\n",
       "        34.0266, 33.8670, 33.6637, 33.5804, 33.5521, 33.4323, 33.3144, 33.2121,\n",
       "        33.0976, 32.9569, 32.8536, 32.7094, 32.6467, 32.4949, 32.3802, 32.1979,\n",
       "        32.1814, 32.0908, 31.9404, 31.8065, 31.7277, 31.5810, 31.3712, 31.2353,\n",
       "        31.0953, 30.9324, 30.7981, 30.7030, 30.6192, 30.5217, 30.4655, 30.3025,\n",
       "        30.1895, 30.1083, 30.0822, 29.8451, 29.7453, 29.6892, 29.5754, 29.4528,\n",
       "        29.3788, 29.2818, 29.1536, 29.1182, 28.9359, 28.7137, 28.6792, 28.5064,\n",
       "        28.4297, 28.3591, 28.2697, 28.1397, 28.0334, 27.9820, 27.8972, 27.7871,\n",
       "        27.6875, 27.6282, 27.5785, 27.5216, 27.3783, 27.3518, 27.2151, 27.0862,\n",
       "        27.0488, 26.9899, 26.9696, 26.7488, 26.5773, 26.5512, 26.3232, 26.2387,\n",
       "        26.2013, 26.0468, 25.9697, 25.9397, 25.8368, 25.7321, 25.5929, 25.4722,\n",
       "        25.3830, 25.3480, 25.1873, 25.1468, 25.0057, 24.8787, 24.7833, 24.7267,\n",
       "        24.6083, 24.5375, 24.4808, 24.4058, 24.2248, 24.1522, 24.0637, 23.9723,\n",
       "        23.9029, 23.8738, 23.8255, 23.6930, 23.5806, 23.4310, 23.3922, 23.3461,\n",
       "        23.1987, 23.1626, 23.1216, 22.9998, 22.8573, 22.7064, 22.6210, 22.5859,\n",
       "        22.4837, 22.3602, 22.2035, 22.1595, 22.0861, 21.9220, 21.8565, 21.8082,\n",
       "        21.7043, 21.6037, 21.5579, 21.4388, 21.3687, 21.2336, 21.0843, 20.9951,\n",
       "        20.9111, 20.7674, 20.7371, 20.6758, 20.5576, 20.4856, 20.4320, 20.3508,\n",
       "        20.1737, 20.1423, 20.1318, 20.0507, 19.9404, 19.7808, 19.7185, 19.6278,\n",
       "        19.5786, 19.4697, 19.4109, 19.2093, 19.1386, 19.0715, 19.0335, 18.9349,\n",
       "        18.8229, 18.7703, 18.6444, 18.5630, 18.5489, 18.4632, 18.2425, 18.2140,\n",
       "        17.9787, 17.9042, 17.8496, 17.8153, 17.7361, 17.7202, 17.6063, 17.4767,\n",
       "        17.3797, 17.3082, 17.2899, 17.1881, 17.1260, 17.0526, 16.9138, 16.8095,\n",
       "        16.7078, 16.6151, 16.5709, 16.5433, 16.4883, 16.3521, 16.2726, 16.1498,\n",
       "        16.0907, 16.0082, 15.9498, 15.8429, 15.7292, 15.6283, 15.6112, 15.5753,\n",
       "        15.4710, 15.2830, 15.2223, 15.1526, 15.1316, 14.8922, 14.8477, 14.7643,\n",
       "        14.7239, 14.6394, 14.5613, 14.5130, 14.4073, 14.2981, 14.2322, 14.1395,\n",
       "        14.1053, 14.0223, 13.9914, 13.8931, 13.8090, 13.6870, 13.6328, 13.5174,\n",
       "        13.4672, 13.4272, 13.2542, 13.1744, 13.0922, 13.0346, 12.9381, 12.9140,\n",
       "        12.7300, 12.6827, 12.5821, 12.5077, 12.4493, 12.4000, 12.3041, 12.2259,\n",
       "        12.1856, 12.0816, 12.0535, 11.8921, 11.8082, 11.7170, 11.5915, 11.4432,\n",
       "        11.3863, 11.3103, 11.2735, 11.1839, 11.1040, 11.0017, 10.9464, 10.7943,\n",
       "        10.7315, 10.6738, 10.6242, 10.5510, 10.5057, 10.3625, 10.1809, 10.0627,\n",
       "        10.0532,  9.9669,  9.8127,  9.7922,  9.6413,  9.5727,  9.5247,  9.2954,\n",
       "         9.2631,  9.1230,  9.0765,  8.9675,  8.9110,  8.6913,  8.6282,  8.5466,\n",
       "         8.5255,  8.3884,  8.3728,  8.2288,  8.1332,  8.0221,  7.9472,  7.9025,\n",
       "         7.8747,  7.7433,  7.7193,  7.4917,  7.3935,  7.3215,  7.2744,  7.1142,\n",
       "         7.0118,  6.9848,  6.9265,  6.8220,  6.5342,  6.5030,  6.3537,  6.1973,\n",
       "         6.1024,  5.9025,  5.8706,  5.7832,  5.6274,  5.5745,  5.3133,  5.0978,\n",
       "         4.9185], device='cuda:0')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0315, -0.0615, -0.0310,  ..., -0.0308, -0.0479,  0.0107],\n",
       "        [-0.0889,  0.0217, -0.0330,  ..., -0.0718, -0.0649, -0.0146],\n",
       "        [ 0.0198, -0.0374,  0.0043,  ..., -0.0030,  0.0070,  0.0767],\n",
       "        ...,\n",
       "        [ 0.0205,  0.0352,  0.0014,  ...,  0.0171,  0.0087, -0.0125],\n",
       "        [ 0.0027, -0.0142, -0.0194,  ..., -0.0037,  0.0219, -0.0708],\n",
       "        [-0.0005,  0.0299,  0.0952,  ..., -0.0581,  0.0430, -0.0143]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xn_cheat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13.3335, -0.3297,  0.4139,  ..., -0.0602,  0.1834, -0.0317],\n",
       "        [-0.3297, 14.3710,  0.0423,  ...,  0.4852,  0.3412, -0.9059],\n",
       "        [ 0.4139,  0.0423, 12.7372,  ...,  0.8727,  0.2347,  0.9779],\n",
       "        ...,\n",
       "        [-0.0602,  0.4852,  0.8727,  ..., 12.9310,  1.3420, -0.3291],\n",
       "        [ 0.1834,  0.3412,  0.2347,  ...,  1.3420, 14.7677, -0.6373],\n",
       "        [-0.0317, -0.9059,  0.9779,  ..., -0.3291, -0.6373, 12.6676]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(V @ S.diag() @ V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5198, 1.5137, 1.5106, 1.5045, 1.5026, 1.5012, 1.4986, 1.4957, 1.4925,\n",
       "         1.4909, 1.4895, 1.4835, 1.4821, 1.4805, 1.4796, 1.4752, 1.4713, 1.4667,\n",
       "         1.4616, 1.4594, 1.4552, 1.4516, 1.4481, 1.4434, 1.4394, 1.4312, 1.4210,\n",
       "         1.4194, 1.4151, 1.4128, 1.4030, 1.3972, 1.3905, 1.3811, 1.3728, 1.3685,\n",
       "         1.3670, 1.3575, 1.3534, 1.3472, 1.3428, 1.3352, 1.3242, 1.3181, 1.3101,\n",
       "         1.3072, 1.3038, 1.2989, 1.2931, 1.2866, 1.2718, 1.2708, 1.2688, 1.2672,\n",
       "         1.2654, 1.2637, 1.2622, 1.2617, 1.2597, 1.2590, 1.2579, 1.2575, 1.2545,\n",
       "         1.2533, 1.2515, 1.2509, 1.2500, 1.2489, 1.2475, 1.2463, 1.2456, 1.2443,\n",
       "         1.2420, 1.2405, 1.2393, 1.2378, 1.2360, 1.2351, 1.2329, 1.2305, 1.2298,\n",
       "         1.2272, 1.2257, 1.2248, 1.2220, 1.2209, 1.2189, 1.2148, 1.2142, 1.2123,\n",
       "         1.2114, 1.2099, 1.2085, 1.2061, 1.2048, 1.2033, 1.2012, 1.1999, 1.1980,\n",
       "         1.1979, 1.1942, 1.1937, 1.1885, 1.1860, 1.1850, 1.1844, 1.1805, 1.1783,\n",
       "         1.1767, 1.1743, 1.1704, 1.1684, 1.1671, 1.1637, 1.1618, 1.1597, 1.1567,\n",
       "         1.1553, 1.1526, 1.1503, 1.1492, 1.1483, 1.1448, 1.1439, 1.1413, 1.1374,\n",
       "         1.1358, 1.1334, 1.1316, 1.1296, 1.1271, 1.1259, 1.1253, 1.1229, 1.1216,\n",
       "         1.1201, 1.1171, 1.1167, 1.1135, 1.1118, 1.1105, 1.1101, 1.1084, 1.1071,\n",
       "         1.1065, 1.1049, 1.1020, 1.1012, 1.0987, 1.0975, 1.0965, 1.0936, 1.0913,\n",
       "         1.0878, 1.0873, 1.0862, 1.0842, 1.0816, 1.0758, 1.0730, 1.0718, 1.0704,\n",
       "         1.0683, 1.0638, 1.0636, 1.0598, 1.0586, 1.0557, 1.0533, 1.0530, 1.0507,\n",
       "         1.0501, 1.0448, 1.0434, 1.0427, 1.0405, 1.0386, 1.0373, 1.0328, 1.0319,\n",
       "         1.0304, 1.0266, 1.0222, 1.0187, 1.0163, 1.0143, 1.0134, 1.0117, 1.0099,\n",
       "         1.0095, 1.0070, 1.0040, 1.0024, 1.0020, 0.9988, 0.9964, 0.9954, 0.9945,\n",
       "         0.9923, 0.9913, 0.9897, 0.9869, 0.9847, 0.9833, 0.9821, 0.9781, 0.9771,\n",
       "         0.9748, 0.9738, 0.9726, 0.9711, 0.9676, 0.9667, 0.9635, 0.9626, 0.9610,\n",
       "         0.9606, 0.9595, 0.9589, 0.9574, 0.9547, 0.9523, 0.9513, 0.9508, 0.9501,\n",
       "         0.9479, 0.9465, 0.9449, 0.9439, 0.9430, 0.9403, 0.9382, 0.9367, 0.9356,\n",
       "         0.9349, 0.9328, 0.9317, 0.9308, 0.9295, 0.9284, 0.9269, 0.9261, 0.9247,\n",
       "         0.9239, 0.9222, 0.9214, 0.9205, 0.9189, 0.9185, 0.9164, 0.9161, 0.9154,\n",
       "         0.9149, 0.9139, 0.9131, 0.9124, 0.9120, 0.9110, 0.9097, 0.9095, 0.9089,\n",
       "         0.9078, 0.9071, 0.9060, 0.9052, 0.9040, 0.9035, 0.9030, 0.9020, 0.9016,\n",
       "         0.9007, 0.9004, 0.8990, 0.8983, 0.8976, 0.8970, 0.8966, 0.8953, 0.8945,\n",
       "         0.8938, 0.8932, 0.8928, 0.8925, 0.8911, 0.8908, 0.8904, 0.8900, 0.8893,\n",
       "         0.8888, 0.8880, 0.8876, 0.8874, 0.8873, 0.8863, 0.8859, 0.8848, 0.8845,\n",
       "         0.8840, 0.8836, 0.8829, 0.8828, 0.8823, 0.8819, 0.8815, 0.8808, 0.8805,\n",
       "         0.8803, 0.8801, 0.8793, 0.8790, 0.8788, 0.8784, 0.8779, 0.8775, 0.8771,\n",
       "         0.8762, 0.8761, 0.8759, 0.8757, 0.8754, 0.8750, 0.8746, 0.8744, 0.8739,\n",
       "         0.8733, 0.8731, 0.8726, 0.8724, 0.8720, 0.8715, 0.8710, 0.8706, 0.8703,\n",
       "         0.8698, 0.8696, 0.8686, 0.8684, 0.8683, 0.8678, 0.8675, 0.8666, 0.8660,\n",
       "         0.8656, 0.8644, 0.8642], device='cuda:0'),\n",
       " tensor(0, device='cuda:0', dtype=torch.int32))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, info = torch.linalg.cholesky_ex((x @ xn.T).float())\n",
    "(torch.cholesky_inverse(L).to(torch.bfloat16) @ x).float().svd().S, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
