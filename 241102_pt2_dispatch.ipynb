{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import abc\n",
    "import torch.nn.modules.module\n",
    "from torch.export import export, Dim\n",
    "from torch.export.exported_program import ExportedProgram, InputKind, OutputKind\n",
    "import uuid\n",
    "import torch._dynamo\n",
    "from torch._functorch.aot_autograd import aot_module, aot_module_simplified\n",
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func\n",
    "from torch._decomp import core_aten_decompositions\n",
    "from torch._dynamo.backends.inductor import inductor\n",
    "from torch._subclasses.fake_tensor import FakeTensorMode\n",
    "from torch._guards import detect_fake_mode\n",
    "from typing import *\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import functools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import inspect\n",
    "import contextlib\n",
    "from torch.utils._python_dispatch import TorchDispatchMode\n",
    "from torch.overrides import enable_reentrant_dispatch\n",
    "from collections import defaultdict\n",
    "from torch._subclasses.fake_tensor import FakeTensorMode\n",
    "from torch.utils._pytree import tree_map\n",
    "from typeguard import typechecked\n",
    "from multimethod import multimethod\n",
    "\n",
    "ENABLE_NORM_DISPATCH = True\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def disable_norm_dispatch():\n",
    "    global ENABLE_NORM_DISPATCH\n",
    "    old_flag = ENABLE_NORM_DISPATCH\n",
    "    ENABLE_NORM_DISPATCH = False\n",
    "    yield\n",
    "    ENABLE_NORM_DISPATCH = old_flag\n",
    "\n",
    "HANDLED_FUNCTIONS: Dict[Callable, Callable] = {}\n",
    "\n",
    "\n",
    "def get_output_fake_tensors(func, *args, **kwargs):\n",
    "    # Create a fake mode\n",
    "    fake_mode = FakeTensorMode(allow_non_fake_inputs=True)\n",
    "    def convert_from_real_tensor(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return fake_mode.fake_tensor_converter.from_real_tensor(fake_mode, x)\n",
    "        return x\n",
    "    # Fakeify some real tensors\n",
    "    with fake_mode, disable_norm_dispatch():\n",
    "        args = tree_map(convert_from_real_tensor, args)\n",
    "        kwargs = tree_map(convert_from_real_tensor, kwargs)\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "\n",
    "def implements(torch_function):\n",
    "    \"\"\"Register a torch function override for ScalarTensor\"\"\"\n",
    "    def decorator(func):\n",
    "        # sig = inspect.signature(func)\n",
    "        # func = typechecked(func)\n",
    "        functools.update_wrapper(func, torch_function)\n",
    "        assert torch_function not in HANDLED_FUNCTIONS\n",
    "        HANDLED_FUNCTIONS[torch_function] = func\n",
    "        return func\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class DispatchLog(TorchDispatchMode):\n",
    "    def __torch_dispatch__(self, func, types, args, kwargs=None):\n",
    "        if ENABLE_NORM_DISPATCH:\n",
    "            print(f\"Dispatch Log: {func}(*{args}, **{kwargs})\")\n",
    "            if any(issubclass(t, NormTensorBase) for t in types):\n",
    "                return NotImplemented\n",
    "        return func(*args, **(kwargs or {}))\n",
    "\n",
    "\n",
    "class NormTensorBase(torch.Tensor):\n",
    "    @staticmethod\n",
    "    def __new__(cls, norm_size: Union[float, torch.Tensor], *, size: torch.Size,\n",
    "                dtype: torch.dtype, device: torch.device, requires_grad=None):\n",
    "        return cls._make_wrapper_subclass(cls, size, dtype=dtype, device=device, requires_grad=False)\n",
    "\n",
    "    def __init__(self, norm_size: Union[float, torch.Tensor], *, size: torch.Size,\n",
    "                 dtype: torch.dtype, device: torch.device, requires_grad=None):\n",
    "        if isinstance(norm_size, torch.Tensor):\n",
    "            assert requires_grad is None\n",
    "            self._norm_size = norm_size\n",
    "        else:\n",
    "            self._norm_size = torch.full((), norm_size, dtype=torch.float32, requires_grad=requires_grad)\n",
    "\n",
    "    @property\n",
    "    def norm_size(self) -> torch.Tensor:\n",
    "        return self._norm_size\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(norm_size={self.norm_size!r})\"\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n",
    "        print(f\"Dispatch Log: {func}(*{args}, **{kwargs})\")\n",
    "        if ENABLE_NORM_DISPATCH and func in HANDLED_FUNCTIONS:\n",
    "            with enable_reentrant_dispatch():\n",
    "                return HANDLED_FUNCTIONS[func](*args, **kwargs)\n",
    "        # for handler, sig in HANDLED_FUNCTIONS.get(func, []):\n",
    "        #     print(f\"Trying {handler}\", sig, args, kwargs)\n",
    "        #     try:\n",
    "        #         bound = sig.bind(*args, **kwargs)\n",
    "        #     except TypeError as e:\n",
    "        #         continue\n",
    "        #     with enable_reentrant_dispatch():\n",
    "        #         out = handler(*bound.args, **bound.kwargs)\n",
    "        #     print(out.norm_size.__class__)\n",
    "        #     print(out)\n",
    "        #     print(out.norm_size)\n",
    "        #     return out\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class RMS_NormTensor(NormTensorBase):\n",
    "    pass\n",
    "\n",
    "class RMS_RMS_NormTensor(NormTensorBase):\n",
    "    pass\n",
    "\n",
    "class L1_NormTensor(NormTensorBase):\n",
    "    pass\n",
    "\n",
    "\n",
    "@implements(torch.ops.aten.unsqueeze.default)\n",
    "@multimethod\n",
    "def unsqueeze(input: RMS_NormTensor, dim: int) -> RMS_NormTensor:\n",
    "    out_fake = get_output_fake_tensors(torch.ops.aten.unsqueeze.default, input, dim)\n",
    "    return RMS_NormTensor(input.norm_size, size=out_fake.size(), dtype=out_fake.dtype, device=out_fake.device)\n",
    "\n",
    "@implements(torch.ops.aten.squeeze_.dim)\n",
    "@multimethod\n",
    "def squeeze(input: RMS_NormTensor, dim: int) -> RMS_NormTensor:\n",
    "    out_fake = get_output_fake_tensors(torch.ops.aten.squeeze_.dim, input, dim)\n",
    "    return RMS_NormTensor(input.norm_size, size=out_fake.size(), dtype=out_fake.dtype, device=out_fake.device)\n",
    "\n",
    "@implements(torch.ops.aten.t.default)\n",
    "@multimethod\n",
    "def t(input: RMS_RMS_NormTensor) -> RMS_RMS_NormTensor:\n",
    "    assert input.ndim == 2\n",
    "    return RMS_RMS_NormTensor(input.norm_size, size=input.size()[::-1], dtype=input.dtype, device=input.device)\n",
    "\n",
    "# @t.register\n",
    "# def t(input: RMSNormTensor) -> RMSRMSNormTensor:\n",
    "#     assert input.ndim == 2\n",
    "#     print(input.size(), 'l')\n",
    "#     return RMSRMSNormTensor(input.norm_size + 2, size=input.size()[::-1], dtype=input.dtype, device=input.device)\n",
    "\n",
    "@implements(torch.ops.aten.addmm.default)\n",
    "@multimethod\n",
    "def addmm(input: RMS_NormTensor, mat1: RMS_NormTensor, mat2: RMS_RMS_NormTensor, *, beta: float = 1, alpha: float = 1) -> RMS_NormTensor:\n",
    "    # output = input * beta + mat1 @ mat2 * alpha\n",
    "    final_norm_size = input.norm_size * beta + mat1.norm_size * mat2.norm_size * alpha\n",
    "    out_fake = get_output_fake_tensors(torch.ops.aten.addmm.default, input, mat1, mat2, beta=beta, alpha=alpha)\n",
    "    return RMS_NormTensor(final_norm_size, size=out_fake.size(), dtype=out_fake.dtype, device=out_fake.device)\n",
    "\n",
    "@addmm.register\n",
    "def _(input: RMS_NormTensor, mat1: RMS_NormTensor, mat2: RMS_RMS_NormTensor, *, beta: float = 1, alpha: float = 1) -> RMS_NormTensor:\n",
    "    # output = input * beta + mat1 @ mat2 * alpha\n",
    "    final_norm_size = input.norm_size * beta + mat1.norm_size * mat2.norm_size * alpha\n",
    "    out_fake = get_output_fake_tensors(torch.ops.aten.addmm.default, input, mat1, mat2, beta=beta, alpha=alpha)\n",
    "    return RMS_NormTensor(final_norm_size, size=out_fake.size(), dtype=out_fake.dtype, device=out_fake.device)\n",
    "\n",
    "\n",
    "@implements(torch.ops.aten.mm.default)\n",
    "@multimethod\n",
    "def mm(input: RMS_NormTensor, mat1: RMS_RMS_NormTensor) -> RMS_NormTensor:\n",
    "    final_norm_size = input.norm_size * mat1.norm_size\n",
    "    out_fake = get_output_fake_tensors(torch.ops.aten.mm.default, input, mat1)\n",
    "    return RMS_NormTensor(final_norm_size, size=out_fake.size(), dtype=out_fake.dtype, device=out_fake.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: aten.full.default(*([], 3), **{'dtype': torch.float32, 'device': device(type='cpu'), 'pin_memory': False})\n",
      "Dispatch Log: aten.full.default(*([], 4), **{'dtype': torch.float32, 'device': device(type='cpu'), 'pin_memory': False})\n",
      "Dispatch Log: aten.t.default(*(RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True)),), **{})\n",
      "Dispatch Log: aten.t.default(*(RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True)),), **{})\n",
      "Dispatch Log: aten.unsqueeze.default(*(RMS_NormTensor(norm_size=tensor(3., requires_grad=True)), 0), **{})\n",
      "Dispatch Log: aten.unsqueeze.default(*(RMS_NormTensor(norm_size=tensor(3., requires_grad=True)), 0), **{})\n",
      "Dispatch Log: aten.mm.default(*(RMS_NormTensor(norm_size=tensor(3., requires_grad=True)), RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True))), **{})\n",
      "Dispatch Log: aten.mm.default(*(RMS_NormTensor(norm_size=tensor(3., requires_grad=True)), RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True))), **{})\n",
      "Dispatch Log: aten.mul.Tensor(*(tensor(3., requires_grad=True), tensor(4., requires_grad=True)), **{})\n",
      "Dispatch Log: aten.squeeze_.dim(*(RMS_NormTensor(norm_size=tensor(12., grad_fn=<MulBackward0>)), 0), **{})\n",
      "Dispatch Log: aten.squeeze_.dim(*(RMS_NormTensor(norm_size=tensor(12., grad_fn=<MulBackward0>)), 0), **{})\n",
      "RMS_NormTensor(norm_size=tensor(12., grad_fn=<MulBackward0>)) tensor(12., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with DispatchLog():\n",
    "    y = torch.ops.aten.linear.default(\n",
    "        RMS_NormTensor(3, size=(15,), dtype=torch.float32, device=torch.device(\"cpu\"), requires_grad=True),\n",
    "        RMS_RMS_NormTensor(4, size=(16, 15), dtype=torch.float32, device=torch.device(\"cpu\"), requires_grad=True),\n",
    "        # RMSNormTensor(3.5, size=(16,), dtype=torch.float32, device=torch.device(\"cpu\"), requires_grad=True),\n",
    "    )\n",
    "    print(y, y.norm_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional\n",
    "import torch\n",
    "\n",
    "# All of the tensor examples in this zoo inherit from BaseTensor.  Ideally,\n",
    "# however, they would inherit directly from Tensor.  This is just our staging\n",
    "# ground for applying behavior that hasn't yet made it into core but that\n",
    "# we would like to apply by default.\n",
    "class BaseTensor(torch.Tensor):\n",
    "    # See https://github.com/pytorch/pytorch/pull/73727 ; this is necessary\n",
    "    # to ensure that super().__new__ can cooperate with each other\n",
    "    @staticmethod\n",
    "    def __new__(cls, elem, *, requires_grad=None):\n",
    "        if requires_grad is None:\n",
    "            return super().__new__(cls, elem)\n",
    "        else:\n",
    "            return cls._make_subclass(cls, elem, requires_grad)\n",
    "\n",
    "    # To ensure constructors can cooperate with one another, must accept and\n",
    "    # ignore element tensor (TODO: is this right???)\n",
    "    def __init__(self, elem):\n",
    "        super().__init__()\n",
    "\n",
    "    # If __torch_dispatch__ is defined (which it will be for all our examples)\n",
    "    # the default torch function implementation (which preserves subclasses)\n",
    "    # typically must be disabled\n",
    "    __torch_function__ = torch._C._disabled_torch_function_impl\n",
    "\n",
    "from torch.utils._pytree import tree_map\n",
    "import contextlib\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from torch.utils._pytree import PyTree, tree_flatten, tree_unflatten\n",
    "\n",
    "# Dumping ground for utilities that should eventual make their way into\n",
    "# PyTorch proper\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def no_dispatch():\n",
    "    guard = torch._C._DisableTorchDispatch()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        del guard\n",
    "\n",
    "\n",
    "def tree_map2(fn: Any, pytree1: PyTree, pytree2: PyTree) -> PyTree:\n",
    "    flat_args1, spec1 = tree_flatten(pytree1)\n",
    "    flat_args2, spec2 = tree_flatten(pytree2)\n",
    "    assert spec1 == spec2\n",
    "    return tree_unflatten([fn(i, j) for i, j in zip(flat_args1, flat_args2)], spec1)\n",
    "\n",
    "\n",
    "# IDK if this is actually useful or not\n",
    "def unmake_subclass(tensor):\n",
    "    with no_dispatch():\n",
    "        return torch.Tensor._make_subclass(torch.Tensor, tensor)\n",
    "\n",
    "\n",
    "def fill_defaults(args, n, defaults_tail):\n",
    "    \"\"\"\n",
    "    __torch_dispatch__ doesn't guarantee the number of arguments you are\n",
    "    passed (e.g., defaulted arguments are not passed); but usually it is\n",
    "    convenient to pad out the arguments list with defaults.  This function\n",
    "    helps you do that.\n",
    "\n",
    "    Args:\n",
    "        args: the list of positional arguments passed to __torch_dispatch__\n",
    "        n: the number of arguments you are expecting to get\n",
    "        defaults_tail: default values for the arguments, starting from the\n",
    "            end of the list\n",
    "\n",
    "    Example:\n",
    "\n",
    "        >>> fill_defaults([1, 2, 3], 5, [3, 4, 5])\n",
    "        [1, 2, 3, 4, 5]\n",
    "        >>> fill_defaults([1, 2, 3], 5, [None, None, None])\n",
    "        [1, 2, 3, None, None]]\n",
    "    \"\"\"\n",
    "    if n - len(defaults_tail) > len(args):\n",
    "        raise RuntimeError(\"not enough defaults to fill arguments\")\n",
    "    r = list(args)\n",
    "    for i in range(len(args), n):\n",
    "        r.append(defaults_tail[i - n + len(defaults_tail)])\n",
    "    return r\n",
    "\n",
    "from torch.overrides import enable_reentrant_dispatch\n",
    "\n",
    "# This file describes how to use wrapper tensors (ala TrivialTensorViaComposition)\n",
    "# to override autograd from __torch_dispatch__.  Ordinarily,\n",
    "# __torch_dispatch__ runs after autograd, so you have no way of overriding\n",
    "# the autograd behavior (since it will be handled after you return).  However,\n",
    "# if we put the autograd tensor *inside* a wrapper tensor (which doesn't\n",
    "# itself require gradients), we get a chance to interpose (in __torch_dispatch__)\n",
    "# before you handle gradients on the inner element.\n",
    "#\n",
    "# Note that you can also use __torch_function__ instead to implement this\n",
    "# functionality, so this is mostly a question of whether or not you want to\n",
    "# target the public Python API, or the internal ATen operators API\n",
    "# (torch.ops.aten).\n",
    "\n",
    "\n",
    "class InnerAutogradTensor(BaseTensor):\n",
    "    @staticmethod\n",
    "    def __new__(cls, elem, *, requires_grad=None):\n",
    "        # Outer tensor's autograd is now disconnected from the inner\n",
    "        # tensors autograd...\n",
    "        return super().__new__(cls, elem, requires_grad=False)\n",
    "\n",
    "    def __init__(self, elem):\n",
    "        # ... but note that we save the inner tensor, so we can still\n",
    "        # do autograd on operations on the inside!\n",
    "        self.elem = elem\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n",
    "        def unwrap(t):\n",
    "            if isinstance(t, cls):\n",
    "                return t.elem\n",
    "            elif isinstance(t, torch.Tensor) and t.requires_grad:\n",
    "                # If any other argument at this level does require gradients\n",
    "                # it will not interact with our inner Tensor and thus this\n",
    "                # should fail.\n",
    "                raise RuntimeError(\"Bad mixup of autograd level\")\n",
    "            else:\n",
    "                return t\n",
    "\n",
    "        def wrap(t):\n",
    "            # Micro-optimization: not necessary to rewrap if the output tensor\n",
    "            # doesn't require gradients\n",
    "            if (\n",
    "                isinstance(t, torch.Tensor)\n",
    "                and not isinstance(t, cls)\n",
    "                and t.requires_grad\n",
    "            ):\n",
    "                return cls(t)\n",
    "            else:\n",
    "                return t\n",
    "\n",
    "        with enable_reentrant_dispatch():\n",
    "            # Override gradient behavior\n",
    "            x = torch.randn(3, requires_grad=True)\n",
    "            print(x, x + x, addself(x))\n",
    "            if func == torch.ops.aten.embedding.default:\n",
    "                args = fill_defaults(args, 5, [-1, False, False])\n",
    "                weight, indices, padding_idx, scale_grad_by_freq, _sparse = map(\n",
    "                    unwrap, args\n",
    "                )\n",
    "                assert not kwargs\n",
    "                # Force sparse gradients.  We could have also done this by\n",
    "                # defining a custom autograd function.\n",
    "                return cls(func(weight, indices, padding_idx, scale_grad_by_freq, True))\n",
    "\n",
    "            return tree_map(\n",
    "                wrap, func(*tree_map(unwrap, args), **tree_map(unwrap, kwargs))\n",
    "            )\n",
    "\n",
    "\n",
    "def addself(x):\n",
    "    return x + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aten::embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ops.aten.embedding.default._schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8549, -0.8576,  1.1171], requires_grad=True) tensor([-1.7098, -1.7153,  2.2342], grad_fn=<AddBackward0>) tensor([-1.7098, -1.7153,  2.2342], grad_fn=<AddBackward0>)\n",
      "tensor([0.7289, 1.0692, 0.3923], requires_grad=True) tensor([1.4578, 2.1384, 0.7846], grad_fn=<AddBackward0>) tensor([1.4578, 2.1384, 0.7846], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "weights = torch.rand(10, 3, requires_grad=True)\n",
    "embedding_matrix = InnerAutogradTensor(weights)\n",
    "r = torch.ops.aten.embedding.default(embedding_matrix, input)\n",
    "r.sum().elem.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[1, 2, 4, 5, 4, 3, 2, 9]]),\n",
       "       values=tensor([[1., 1., 1.],\n",
       "                      [1., 1., 1.],\n",
       "                      [1., 1., 1.],\n",
       "                      [1., 1., 1.],\n",
       "                      [1., 1., 1.],\n",
       "                      [1., 1., 1.],\n",
       "                      [1., 1., 1.],\n",
       "                      [1., 1., 1.]]),\n",
       "       size=(10, 3), nnz=8, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's always attach to a export graph node (which has a fake tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import inspect\n",
    "import contextlib\n",
    "from torch.utils._python_dispatch import TorchDispatchMode, _get_current_dispatch_mode\n",
    "from torch.overrides import enable_reentrant_dispatch, TorchFunctionMode, _get_current_function_mode, _get_current_function_mode_stack\n",
    "from collections import defaultdict\n",
    "from torch._subclasses.fake_tensor import FakeTensorMode, FakeTensor\n",
    "from torch.utils._pytree import tree_map\n",
    "from torch.fx.operator_schemas import (\n",
    "    _torchscript_schema_to_signature,\n",
    ")\n",
    "from torch._guards import detect_fake_mode, active_fake_mode\n",
    "import torch.utils._pytree as pytree\n",
    "import typing\n",
    "import numbers\n",
    "from typing import *\n",
    "from collections import OrderedDict\n",
    "from torch._ops import OpOverload, OpOverloadPacket, _has_script_object_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormedTensorBase(torch.Tensor):\n",
    "    _backing_tensor: Optional[torch.Tensor]\n",
    "\n",
    "    @staticmethod\n",
    "    def __new__(cls, norm_size: Union[float, torch.Tensor], elem_dims: Optional[Tuple[int, ...]] = None, *,\n",
    "                backing_tensor: Optional[torch.Tensor] = None, requires_grad: Optional[bool] = None):\n",
    "        if issubclass(cls.__base__, NormedTensorBase) and cls.__base__ != NormedTensorBase:\n",
    "            raise TypeError(f\"NormedTensorBase can only be subclassed with one level of inheritance\")\n",
    "        if backing_tensor is None:\n",
    "            backing_tensor = torch.empty((0,))  # this is a placeholder so that _make_wrapper_subclass doesn't fail, will have finalize=False\n",
    "        else:\n",
    "            assert type(backing_tensor) in (torch.Tensor, FakeTensor)\n",
    "        return cls._make_wrapper_subclass(cls, backing_tensor.size(), dtype=backing_tensor.dtype, device=backing_tensor.device,\n",
    "                                          requires_grad=False)  # NB: false here so that we can use reentrant dispatch on unwrapped normed tensors to get autograd on norms\n",
    "\n",
    "    def __init__(self, norm_size: Union[float, torch.Tensor], elem_dims: Optional[Tuple[int, ...]] = None, *,\n",
    "                 backing_tensor: Optional[torch.Tensor] = None, requires_grad: Optional[bool] = None):\n",
    "        if isinstance(norm_size, torch.Tensor):\n",
    "            assert requires_grad is None\n",
    "            self._norm_size = norm_size\n",
    "        else:\n",
    "            self._norm_size = torch.full((), norm_size, dtype=torch.float32, requires_grad=requires_grad)\n",
    "        if backing_tensor is not None:\n",
    "            # finalized\n",
    "            if elem_dims is None:\n",
    "                # default\n",
    "                elem_dims = tuple(range(backing_tensor.ndim))\n",
    "            elem_dims = tuple(sorted(d % backing_tensor.ndim for d in elem_dims))\n",
    "        self._elem_dims = elem_dims\n",
    "        self._backing_tensor = backing_tensor\n",
    "\n",
    "    def finalize(self, backing_tensor: torch.Tensor) -> Self:\n",
    "        assert not self._finalized\n",
    "        return self.__class__(self._norm_size, elem_dims=self._elem_dims, backing_tensor=backing_tensor)\n",
    "\n",
    "    def elem_dims_are(self, dims: Iterable[int]) -> bool:\n",
    "        # FIXME: figure out a good broadcasting API\n",
    "        assert self._finalized\n",
    "        return self._elem_dims == tuple(sorted(d % self.ndim for d in dims))\n",
    "\n",
    "    def same_elem_dims(self, other: 'NormedTensorBase') -> bool:\n",
    "        # broadcasting\n",
    "        assert self._finalized and other._finalized\n",
    "        _ = torch.broadcast_shapes(self.shape, other.shape)\n",
    "        # convert to negative indexing\n",
    "        return self.neg_elem_dims == other.neg_elem_dims\n",
    "\n",
    "    @property\n",
    "    def _finalized(self):\n",
    "        return self._backing_tensor is not None\n",
    "\n",
    "    @property\n",
    "    def norm_size(self) -> torch.Tensor:\n",
    "        assert self._finalized\n",
    "        return self._norm_size\n",
    "\n",
    "    @property\n",
    "    def elem_dims(self) -> Tuple[int, ...]:\n",
    "        assert self._finalized\n",
    "        return self._elem_dims\n",
    "\n",
    "    @property\n",
    "    def neg_elem_dims(self) -> Tuple[int, ...]:\n",
    "        return tuple(d - self.ndim for d in self._elem_dims)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self) -> torch.Tensor:\n",
    "        assert self._finalized\n",
    "        return self._backing_tensor\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self._finalized:\n",
    "            return f\"\"\"{self.__class__.__name__}(\n",
    "    norm_size={self.norm_size!r},\n",
    "    elem_dims={self.elem_dims!r},\n",
    "    unwrapped={self.unwrapped!r},\n",
    ")\"\"\"\n",
    "        else:\n",
    "            return f\"\"\"{self.__class__.__name__}(norm_size={self.norm_size!r}, NOT_FINALIZED)\"\"\"\n",
    "\n",
    "\n",
    "    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n",
    "        return NotImplemented\n",
    "        print(f\"base cls Dispatch Log: {func}, {types}\")\n",
    "        # with enable_reentrant_dispatch():\n",
    "        if func in REG_FAKE_NORM_OP_LOOKUP_VIA_CUSTOM_OP:\n",
    "            with enable_reentrant_dispatch(), torch.set_grad_enabled(True):\n",
    "                x = torch.randn(3, requires_grad=True)\n",
    "                print(x, x + x)\n",
    "                return REG_FAKE_NORM_OP_LOOKUP_VIA_CUSTOM_OP[func].normed_dispatcher(*args, **(kwargs or {}))\n",
    "        return func(*args, **(kwargs or {}))\n",
    "        return NotImplemented\n",
    "        print(f\"Dispatch Log: {func}(*{args}, **{kwargs})\")\n",
    "        if ENABLE_NORM_DISPATCH and func in HANDLED_FUNCTIONS:\n",
    "            with enable_reentrant_dispatch():\n",
    "                return HANDLED_FUNCTIONS[func](*args, **kwargs)\n",
    "        # for handler, sig in HANDLED_FUNCTIONS.get(func, []):\n",
    "        #     print(f\"Trying {handler}\", sig, args, kwargs)\n",
    "        #     try:\n",
    "        #         bound = sig.bind(*args, **kwargs)\n",
    "        #     except TypeError as e:\n",
    "        #         continue\n",
    "        #     with enable_reentrant_dispatch():\n",
    "        #         out = handler(*bound.args, **bound.kwargs)\n",
    "        #     print(out.norm_size.__class__)\n",
    "        #     print(out)\n",
    "        #     print(out.norm_size)\n",
    "        #     return out\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class RMS_NormTensor(NormedTensorBase):\n",
    "    pass\n",
    "\n",
    "class RMS_RMS_NormTensor(NormedTensorBase):\n",
    "    pass\n",
    "\n",
    "class L1_NormTensor(NormedTensorBase):\n",
    "    pass\n",
    "\n",
    "class Linf_NormTensor(NormedTensorBase):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.library.Library('modula', \"FRAGMENT\")._destroy()\n",
    "\n",
    "class NormedTensorDispatcher:\n",
    "    # dispatches things based on the classes of NormTensorBase arguments\n",
    "\n",
    "    def __init__(self, ref_sig: inspect.Signature, *, ignored_params: Iterable[str] = ()):\n",
    "        self.ref_sig = ref_sig\n",
    "        self.ignored_params = tuple(ignored_params)\n",
    "        self.handled_functions = OrderedDict()\n",
    "        functools.update_wrapper(self, ref_sig)\n",
    "\n",
    "        dispatch_key_arg_names = []\n",
    "        for param in self.ref_sig.parameters.values():\n",
    "            if inspect.isclass(param.annotation) and issubclass(param.annotation, torch.Tensor) and param.name not in self.ignored_params:\n",
    "                dispatch_key_arg_names.append(param.name)\n",
    "        self.dispatch_key_arg_names = tuple(sorted(dispatch_key_arg_names))\n",
    "\n",
    "    @staticmethod\n",
    "    def _assert_specialized(ref_sig: inspect.Signature, specialized_sig: inspect.Signature, *,\n",
    "                            allow_non_normed_tensor_inputs: bool = False):\n",
    "        try:\n",
    "            def only_normed_tensor(ty):\n",
    "                if origin := typing.get_origin(ty):\n",
    "                    return only_normed_tensor(origin) and all(only_normed_tensor(t) for t in typing.get_args(ty))\n",
    "                if inspect.isclass(ty) and issubclass(ty, torch.Tensor):\n",
    "                    return inspect.isclass(ty) and issubclass(ty, NormedTensorBase) # and ty != NormedTensorBase\n",
    "                return True\n",
    "\n",
    "            def is_compatible_type(ref_type, specialized_type):\n",
    "                # print(ref_type, specialized_type)\n",
    "                if ref_origin := typing.get_origin(ref_type):\n",
    "                    if not is_compatible_type(typing.get_origin(specialized_type), ref_origin):\n",
    "                        return False\n",
    "                    ref_args = typing.get_args(ref_type)\n",
    "                    specialized_args = typing.get_args(specialized_type)\n",
    "                    if len(ref_args) != len(specialized_args):\n",
    "                        return False\n",
    "                    return all(is_compatible_type(ref_t, specialized_t) for ref_t, specialized_t in zip(ref_args, specialized_args))\n",
    "                if ref_type == specialized_type:\n",
    "                    return True\n",
    "                if specialized_type is typing.Any:\n",
    "                    return True\n",
    "                if ref_type is numbers.Number:\n",
    "                    return specialized_type == float\n",
    "                if specialized_type in (torch.dtype, torch.layout) and ref_type is int:\n",
    "                    return True\n",
    "                if inspect.isclass(ref_type) and inspect.isclass(specialized_type):\n",
    "                    return issubclass(specialized_type, ref_type)\n",
    "                return False\n",
    "\n",
    "            assert set(ref_sig.parameters.keys()) == set(specialized_sig.parameters.keys()), f\"Function has a different signature\"\n",
    "            for param_name in ref_sig.parameters.keys():\n",
    "                ref_param = ref_sig.parameters[param_name]\n",
    "                specialized_param = specialized_sig.parameters[param_name]\n",
    "                if not allow_non_normed_tensor_inputs:\n",
    "                    assert only_normed_tensor(specialized_param.annotation), f\"Specialized {specialized_sig} has a non-normed tensor parameter {param_name}\"\n",
    "                assert is_compatible_type(ref_param.annotation, specialized_param.annotation), f\"Parameter {param_name} has a different type\"\n",
    "\n",
    "        except AssertionError as e:\n",
    "            raise TypeError(f\"Specialized {specialized_sig} has a different signature from {ref_sig}\") from e\n",
    "\n",
    "    def register(self, specialized_func: Optional[Callable] = None, *, allow_non_normed_tensor_inputs: bool = False):\n",
    "        def decorator(specialized_func):\n",
    "            specialized_sig = inspect.signature(specialized_func)\n",
    "            self._assert_specialized(self.ref_sig, specialized_sig, allow_non_normed_tensor_inputs=allow_non_normed_tensor_inputs)\n",
    "            dispatch_key = tuple(specialized_sig.parameters[name].annotation for name in self.dispatch_key_arg_names)\n",
    "            if not allow_non_normed_tensor_inputs:\n",
    "                assert all(inspect.isclass(t) and issubclass(t, NormedTensorBase) for t in dispatch_key)\n",
    "            assert dispatch_key not in self.handled_functions\n",
    "            # print(dispatch_key, specialized_func)\n",
    "            self.handled_functions[dispatch_key] = specialized_func\n",
    "            return specialized_func\n",
    "        if specialized_func is None:\n",
    "            return decorator\n",
    "        return decorator(specialized_func)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        bound = self.ref_sig.bind(*args, **kwargs)\n",
    "        dispatch_key = tuple(bound.arguments[name].__class__ for name in self.dispatch_key_arg_names)\n",
    "        for k, fn in self.handled_functions.items():\n",
    "            if all(issubclass(q, k) for q, k in zip(dispatch_key, k)):\n",
    "                return fn(*args, **kwargs)\n",
    "        raise NotImplementedError(f\"No dispatch rule found for {dispatch_key}\")\n",
    "\n",
    "\n",
    "REG_FAKE_NORM_OP_REGISTRY: Dict[Callable, 'RegFakeNormOp'] = {}\n",
    "REG_FAKE_NORM_OP_LOOKUP_VIA_CUSTOM_OP: Dict[Callable, 'RegFakeNormOp'] = {}\n",
    "\n",
    "class RegFakeNormOp:\n",
    "    reg_sig: inspect.Signature\n",
    "    wrapper_custom_op: torch.library.CustomOpDef\n",
    "    wrapper_custom_op_entrypoint: Callable\n",
    "    normed_dispatcher: NormedTensorDispatcher\n",
    "\n",
    "    @property\n",
    "    def register_norm(self):\n",
    "        return self.normed_dispatcher.register\n",
    "\n",
    "    @property\n",
    "    def register_fake(self):\n",
    "        return self.wrapper_custom_op.register_fake\n",
    "\n",
    "    def __init__(self, func: Callable, *, schema: Optional[str] = None, func_prefix: str = 'wrapper'):\n",
    "        if isinstance(func, OpOverload):\n",
    "            # for torch lib ops, we need the schema. inspect.signature gives (*args, **kwargs)\n",
    "            schema = str(func._schema)\n",
    "            reg_sig = _torchscript_schema_to_signature(func._schema)  # this overwrites the signature if provided\n",
    "        else:\n",
    "            if schema is not None:\n",
    "                reg_sig = _torchscript_schema_to_signature(torch._C.parse_schema(schema))\n",
    "            else:\n",
    "                # this may error, so last resort\n",
    "                reg_sig = inspect.signature(func)\n",
    "\n",
    "        for param in reg_sig.parameters.values():\n",
    "            assert param.kind not in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD), f\"Parameter {param.name} is var positional or var keyword\"\n",
    "\n",
    "        # register a new op\n",
    "        func_name = f\"op__{func_prefix}__{(func.__module__ + '.' + func.__qualname__).replace('::', '_').replace('.', '_')}__{id(func)}\"\n",
    "        op_id = f\"modula::{func_name}\"\n",
    "        func = functools.partial(func)\n",
    "        func.__signature__ = reg_sig\n",
    "        if schema is not None:\n",
    "            # name it nameless\n",
    "            nameless_schema = '(' + schema.split('(', 1)[1]\n",
    "        else:\n",
    "            nameless_schema = None\n",
    "        wrapper_custom_op: torch.library.CustomOpDef = torch.library.custom_op(op_id, func, mutates_args=(), schema=nameless_schema)\n",
    "        wrapper_custom_op.register_fake(func)  # can be modified by self.register_fake\n",
    "\n",
    "        # def torch_dispatch_wrapper(mode, func, types, args=(), kwargs=None):\n",
    "        #     kwargs = kwargs or {}\n",
    "        #     print(f\"COP Dispatch Log: {func}, {types}\")\n",
    "        #     with enable_reentrant_dispatch():\n",
    "        #         x = torch.randn(3, requires_grad=True)\n",
    "        #         print(x, x + x)\n",
    "        #         return self.normed_dispatcher(*args, **kwargs)\n",
    "\n",
    "        # wrapper_custom_op.register_torch_dispatch(RMS_NormTensor, torch_dispatch_wrapper)\n",
    "        # wrapper_custom_op.register_torch_dispatch(L1_NormTensor, torch_dispatch_wrapper)\n",
    "        # wrapper_custom_op.register_torch_dispatch(RMS_RMS_NormTensor, torch_dispatch_wrapper)\n",
    "\n",
    "        self.reg_sig = reg_sig\n",
    "        self.wrapper_custom_op = wrapper_custom_op\n",
    "        self.wrapper_custom_op_entrypoint = getattr(torch.ops.modula, func_name).default\n",
    "        self.normed_dispatcher = NormedTensorDispatcher(reg_sig)\n",
    "        functools.update_wrapper(self, func)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.wrapper_custom_op(*args, **kwargs)\n",
    "\n",
    "    # def call_fake(self, *args, fake_mode: Optional[FakeTensorMode] = None, **kwargs):\n",
    "    #     fake_mode = fake_mode or active_fake_mode()\n",
    "    #     if fake_mode is None:\n",
    "    #         fake_mode = FakeTensorMode()\n",
    "    #     def convert_from_real_tensor(x):\n",
    "    #         if isinstance(x, torch.Tensor):\n",
    "    #             return fake_mode.fake_tensor_converter.from_real_tensor(fake_mode, x)\n",
    "    #         return x\n",
    "    #     # Fakeify some real tensors\n",
    "    #     with fake_mode:\n",
    "    #         args = tree_map(convert_from_real_tensor, args)\n",
    "    #         kwargs = tree_map(convert_from_real_tensor, kwargs)\n",
    "    #         return self(*args, **kwargs)\n",
    "\n",
    "\n",
    "class ExportFakeFunctionMode(TorchFunctionMode):\n",
    "    # Used when exporting, to attach custom ops to the export graph.\n",
    "    # The resulting graph should only contain `wrapper_custom_op`, .\n",
    "    # Even ATen core IR ops should be wrapped in `wrapper_custom_op`.\n",
    "    def __torch_function__(self, func, types, args=(), kwargs=None):\n",
    "        # print(f\"Dispatch Log: {func}, {types}\")\n",
    "        kwargs = kwargs or {}\n",
    "        if func in REG_FAKE_NORM_OP_REGISTRY:\n",
    "            return REG_FAKE_NORM_OP_REGISTRY[func](*args, **kwargs)\n",
    "        # if any(issubclass(t, NormTensorBase) for t in types):\n",
    "        #     return NotImplemented\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "\n",
    "MODULAR_EXPORTING = False\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def modula_export():\n",
    "    global MODULAR_EXPORTING\n",
    "    assert not MODULAR_EXPORTING, \"Cannot nest modula_export\"\n",
    "    MODULAR_EXPORTING = True\n",
    "    with ExportFakeFunctionMode():\n",
    "        yield\n",
    "    MODULAR_EXPORTING = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def finalize_normed_out(unfinalized_normed_out, fake_out):\n",
    "    flat_fake_out, fake_out_tree_spec = pytree.tree_flatten(fake_out)\n",
    "    flat_unfinalized_normed_out, unfinalized_normed_out_tree_spec = pytree.tree_flatten(unfinalized_normed_out)\n",
    "    assert pytree.treespec_dumps(fake_out_tree_spec) == pytree.treespec_dumps(unfinalized_normed_out_tree_spec), f\"Tree spec mismatch\"\n",
    "    return pytree.tree_unflatten(\n",
    "        [\n",
    "            normed.finalize(out) for normed, out in zip(flat_unfinalized_normed_out, flat_fake_out)\n",
    "        ],\n",
    "        fake_out_tree_spec,\n",
    "    )\n",
    "\n",
    "\n",
    "class NormPropagateDispatchMode(TorchDispatchMode):\n",
    "    # Used when propagating norms on an exported graph, which contains only `wrapper_custom_op`.\n",
    "    # We handle here instead of `wrapper_custom_op.register_torch_dispatch(exact_type, ...)` because we want to\n",
    "    # capture all NormedTensorBase subclasses, and don't want to register a dispatch rule for each one.\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fake_mode = FakeTensorMode(allow_non_fake_inputs=True)\n",
    "\n",
    "    def _call_fake_with_normed_args(self, op: RegFakeNormOp, *args, **kwargs):\n",
    "        def convert_from_normed_tensor(x):\n",
    "            if isinstance(x, NormedTensorBase):\n",
    "                return self.fake_mode.fake_tensor_converter.from_real_tensor(self.fake_mode, x.unwrapped)  # also works on fake tensor\n",
    "            return x\n",
    "\n",
    "        with self.fake_mode:\n",
    "            args = tree_map(convert_from_normed_tensor, args)\n",
    "            kwargs = tree_map(convert_from_normed_tensor, kwargs)\n",
    "            return op(*args, **kwargs)\n",
    "\n",
    "    def __torch_dispatch__(self, func, types, args, kwargs):\n",
    "        # print(f\"Dispatch Log: {func}, {types}\", self.enabled,active_fake_mode())\n",
    "        kwargs = kwargs or {}\n",
    "        # if not any(issubclass(t, NormedTensorBase) for t in types):\n",
    "        if func in REG_FAKE_NORM_OP_LOOKUP_VIA_CUSTOM_OP:  # NB: actual factories like torch.empty won't be in here since this only contains wrapped versions\n",
    "            # normed mode\n",
    "            with enable_reentrant_dispatch(), self, torch.set_grad_enabled(True):\n",
    "                op = REG_FAKE_NORM_OP_LOOKUP_VIA_CUSTOM_OP[func]\n",
    "                unfinalized_normed = op.normed_dispatcher(*args, **kwargs)\n",
    "            fake = self._call_fake_with_normed_args(op, *args, **kwargs)\n",
    "            return finalize_normed_out(unfinalized_normed, fake)\n",
    "        # fake or real mode\n",
    "        assert not any(issubclass(t, NormedTensorBase) for t in types)\n",
    "        return func(*args, **kwargs)\n",
    "        return NotImplemented\n",
    "        # if any(issubclass(t, NormTensorBase) for t in types):\n",
    "        #     return NotImplemented\n",
    "        # return func(*args, **kwargs)\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def norm_propagate_dispatch():\n",
    "    with NormPropagateDispatchMode() as mode:\n",
    "        yield mode\n",
    "\n",
    "\n",
    "def reg_fake_norm_op(op: Optional[Callable] = None, *, schema: Optional[str] = None, func_prefix: str = 'wrapper') -> RegFakeNormOp:\n",
    "    def decorator(op):\n",
    "        if op not in REG_FAKE_NORM_OP_REGISTRY:\n",
    "            reg_fake_norm_op = RegFakeNormOp(op, schema=schema, func_prefix=func_prefix)\n",
    "            REG_FAKE_NORM_OP_REGISTRY[op] = reg_fake_norm_op\n",
    "            REG_FAKE_NORM_OP_LOOKUP_VIA_CUSTOM_OP[reg_fake_norm_op.wrapper_custom_op_entrypoint] = reg_fake_norm_op\n",
    "        return REG_FAKE_NORM_OP_REGISTRY[op]\n",
    "    if op is None:\n",
    "        return decorator\n",
    "    return decorator(op)\n",
    "\n",
    "\n",
    "\n",
    "class ConstantScaler(nn.Module):\n",
    "    @reg_fake_norm_op(func_prefix='constant_scaler_mul')\n",
    "    def _mul_with_scaler(input: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n",
    "        assert scale.ndim == 0\n",
    "        return input * scale\n",
    "\n",
    "    @_mul_with_scaler.register_norm(allow_non_normed_tensor_inputs=True)\n",
    "    def _(input: NormedTensorBase, scale: torch.Tensor) -> NormedTensorBase:\n",
    "        assert scale.ndim == 0\n",
    "        return input.__class__(input.norm_size * scale, elem_dims=input.elem_dims)\n",
    "\n",
    "    scale: torch.Tensor\n",
    "\n",
    "    def __init__(self, scale: float):\n",
    "        super().__init__()\n",
    "        self.register_buffer('scale', torch.tensor(scale, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return ConstantScaler._mul_with_scaler(x, self.scale)\n",
    "        if MODULAR_EXPORTING:\n",
    "            # do something that modula can detect\n",
    "            return ConstantScaler._OP(x, self.scale)\n",
    "        return x * self.scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_fake_norm_op(torch.nn.functional.linear, schema=\"linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor\").register_norm\n",
    "def linear(input: RMS_NormTensor, weight: RMS_RMS_NormTensor, bias: Optional[RMS_NormTensor] = None) -> RMS_NormTensor:\n",
    "    assert input.elem_dims_are(dims=(-1,))\n",
    "    assert weight.elem_dims_are(dims=(-1, -2))\n",
    "    final_norm_size = input.norm_size * weight.norm_size\n",
    "    if bias is not None:\n",
    "        assert bias.elem_dims_are(dims=(-1,))\n",
    "        final_norm_size += bias.norm_size\n",
    "    return RMS_NormTensor(final_norm_size, elem_dims=(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_fake_norm_op(torch.ops.aten.randn.default).register_norm\n",
    "def randn(size: List[int], *, dtype: Optional[torch.dtype] = None, layout: Optional[torch.layout] = torch.strided, device: Optional[torch.device] = None, pin_memory: Optional[bool] = False) -> RMS_NormTensor:\n",
    "    return RMS_NormTensor(1, elem_dims=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_fake_norm_op(torch.ops.aten.add.Tensor).register_norm\n",
    "def add(input: RMS_NormTensor, other: RMS_NormTensor, *, alpha: float = 1) -> RMS_NormTensor:\n",
    "    assert input.same_elem_dims(other)  # FIXME\n",
    "    return RMS_NormTensor(input.norm_size + other.norm_size * alpha, elem_dims=input.neg_elem_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_fake_norm_op(torch.nn.functional.layer_norm,\n",
    "                  schema=\"layer_norm(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05) -> Tensor\").register_norm\n",
    "def layer_norm(input: RMS_NormTensor, normalized_shape: List[int], weight: Optional[Linf_NormTensor] = None, bias: Optional[RMS_NormTensor] = None, eps: float = 1e-05) -> RMS_NormTensor:\n",
    "    # assert input.elem_dims_are(dims=normalized_shape)\n",
    "    # FIXME: this is wrong\n",
    "    output_norm_size = input.norm_size\n",
    "    if weight is not None:\n",
    "        output_norm_size += weight.norm_size\n",
    "    if bias is not None:\n",
    "        output_norm_size += bias.norm_size\n",
    "    return RMS_NormTensor(output_norm_size, elem_dims=input.elem_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aten::relu(Tensor self) -> Tensor"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ops.aten.relu.default._schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_fake_norm_op(torch.ops.aten.relu.default).register_norm\n",
    "def relu(input: RMS_NormTensor) -> RMS_NormTensor:\n",
    "    return RMS_NormTensor(input.norm_size / np.sqrt(2), elem_dims=input.elem_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_fake_norm_op(torch.nn.functional.scaled_dot_product_attention,\n",
    "                  schema=\"sdpa(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout=0.0, bool is_causal=False) -> Tensor\").register_norm\n",
    "def scaled_dot_product_attention(query: RMS_NormTensor, key: RMS_NormTensor, value: RMS_NormTensor, attn_mask: Optional[RMS_NormTensor] = None,\n",
    "                                 dropout: float = 0.0, is_causal: bool = False) -> RMS_NormTensor:\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GPT(4, 256, 3, 64, 256)\n",
    "\n",
    "example_input = torch.randint(0, 256, (2, 64)),\n",
    "with modula_export():\n",
    "    ep = torch.export.export(\n",
    "        net,\n",
    "        example_input,\n",
    "        dynamic_shapes=[{0: batch}]\n",
    "    )\n",
    "\n",
    "    ep = ep.run_decompositions()\n",
    "\n",
    "gm = ep.module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule(\n",
      "  (lm_head): Module()\n",
      "  (transformer): Module(\n",
      "    (wp_embedding): Module()\n",
      "    (h): Module(\n",
      "      (0): Module(\n",
      "        (ln_1): Module()\n",
      "        (attn): Module(\n",
      "          (in_proj): Module()\n",
      "          (out_proj): Module()\n",
      "        )\n",
      "        (ln_2): Module()\n",
      "        (mlp): Module(\n",
      "          (fc1): Module()\n",
      "          (fc2): Module()\n",
      "        )\n",
      "      )\n",
      "      (1): Module(\n",
      "        (ln_1): Module()\n",
      "        (attn): Module(\n",
      "          (in_proj): Module()\n",
      "          (out_proj): Module()\n",
      "        )\n",
      "        (ln_2): Module()\n",
      "        (mlp): Module(\n",
      "          (fc1): Module()\n",
      "          (fc2): Module()\n",
      "        )\n",
      "      )\n",
      "      (2): Module(\n",
      "        (ln_1): Module()\n",
      "        (attn): Module(\n",
      "          (in_proj): Module()\n",
      "          (out_proj): Module()\n",
      "        )\n",
      "        (ln_2): Module()\n",
      "        (mlp): Module(\n",
      "          (fc1): Module()\n",
      "          (fc2): Module()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): Module()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, idx):\n",
      "    idx, = fx_pytree.tree_flatten_spec(([idx], {}), self._in_spec)\n",
      "    lm_head_weight = self.lm_head.weight\n",
      "    transformer_wp_embedding_weight = self.transformer.wp_embedding.weight\n",
      "    transformer_h_0_ln_1_weight = getattr(self.transformer.h, \"0\").ln_1.weight\n",
      "    transformer_h_0_ln_1_bias = getattr(self.transformer.h, \"0\").ln_1.bias\n",
      "    transformer_h_0_attn_in_proj_weight = getattr(self.transformer.h, \"0\").attn.in_proj.weight\n",
      "    transformer_h_0_attn_out_proj_weight = getattr(self.transformer.h, \"0\").attn.out_proj.weight\n",
      "    transformer_h_0_ln_2_weight = getattr(self.transformer.h, \"0\").ln_2.weight\n",
      "    transformer_h_0_ln_2_bias = getattr(self.transformer.h, \"0\").ln_2.bias\n",
      "    transformer_h_0_mlp_fc1_weight = getattr(self.transformer.h, \"0\").mlp.fc1.weight\n",
      "    transformer_h_0_mlp_fc2_weight = getattr(self.transformer.h, \"0\").mlp.fc2.weight\n",
      "    transformer_h_1_ln_1_weight = getattr(self.transformer.h, \"1\").ln_1.weight\n",
      "    transformer_h_1_ln_1_bias = getattr(self.transformer.h, \"1\").ln_1.bias\n",
      "    transformer_h_1_attn_in_proj_weight = getattr(self.transformer.h, \"1\").attn.in_proj.weight\n",
      "    transformer_h_1_attn_out_proj_weight = getattr(self.transformer.h, \"1\").attn.out_proj.weight\n",
      "    transformer_h_1_ln_2_weight = getattr(self.transformer.h, \"1\").ln_2.weight\n",
      "    transformer_h_1_ln_2_bias = getattr(self.transformer.h, \"1\").ln_2.bias\n",
      "    transformer_h_1_mlp_fc1_weight = getattr(self.transformer.h, \"1\").mlp.fc1.weight\n",
      "    transformer_h_1_mlp_fc2_weight = getattr(self.transformer.h, \"1\").mlp.fc2.weight\n",
      "    transformer_h_2_ln_1_weight = getattr(self.transformer.h, \"2\").ln_1.weight\n",
      "    transformer_h_2_ln_1_bias = getattr(self.transformer.h, \"2\").ln_1.bias\n",
      "    transformer_h_2_attn_in_proj_weight = getattr(self.transformer.h, \"2\").attn.in_proj.weight\n",
      "    transformer_h_2_attn_out_proj_weight = getattr(self.transformer.h, \"2\").attn.out_proj.weight\n",
      "    transformer_h_2_ln_2_weight = getattr(self.transformer.h, \"2\").ln_2.weight\n",
      "    transformer_h_2_ln_2_bias = getattr(self.transformer.h, \"2\").ln_2.bias\n",
      "    transformer_h_2_mlp_fc1_weight = getattr(self.transformer.h, \"2\").mlp.fc1.weight\n",
      "    transformer_h_2_mlp_fc2_weight = getattr(self.transformer.h, \"2\").mlp.fc2.weight\n",
      "    transformer_ln_f_weight = self.transformer.ln_f.weight\n",
      "    transformer_ln_f_bias = self.transformer.ln_f.bias\n",
      "    lm_head_weight_1 = self.lm_head.weight\n",
      "    lm_head_bias = self.lm_head.bias\n",
      "    sym_size_int_1 = torch.ops.aten.sym_size.int(idx, 0)\n",
      "    arange = torch.ops.aten.arange.start_step(0, 64, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'), pin_memory = False)\n",
      "    embedding = torch.ops.aten.embedding.default(lm_head_weight, idx);  lm_head_weight = idx = None\n",
      "    embedding_1 = torch.ops.aten.embedding.default(transformer_wp_embedding_weight, arange);  transformer_wp_embedding_weight = arange = None\n",
      "    op__wrapper__torch__ops_aten_aten_add_tensor__4740564240 = torch.ops.modula.op__wrapper__torch__ops_aten_aten_add_Tensor__4740564240.default(embedding, embedding_1);  embedding = embedding_1 = None\n",
      "    op__wrapper__torch_nn_functional_layer_norm__4721199520 = torch.ops.modula.op__wrapper__torch_nn_functional_layer_norm__4721199520.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564240, [256], transformer_h_0_ln_1_weight, transformer_h_0_ln_1_bias);  transformer_h_0_ln_1_weight = transformer_h_0_ln_1_bias = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862672 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(op__wrapper__torch_nn_functional_layer_norm__4721199520, transformer_h_0_attn_in_proj_weight);  op__wrapper__torch_nn_functional_layer_norm__4721199520 = transformer_h_0_attn_in_proj_weight = None\n",
      "    split_with_sizes = torch.ops.aten.split_with_sizes.default(op__wrapper__torch__c__nn_linear__4535862672, [256, 256, 256], -1);  op__wrapper__torch__c__nn_linear__4535862672 = None\n",
      "    getitem = split_with_sizes[0]\n",
      "    getitem_1 = split_with_sizes[1]\n",
      "    getitem_2 = split_with_sizes[2];  split_with_sizes = None\n",
      "    view = torch.ops.aten.view.default(getitem, [sym_size_int_1, 64, 4, 64]);  getitem = None\n",
      "    permute = torch.ops.aten.permute.default(view, [0, 2, 1, 3]);  view = None\n",
      "    view_1 = torch.ops.aten.view.default(getitem_1, [sym_size_int_1, 64, 4, 64]);  getitem_1 = None\n",
      "    permute_1 = torch.ops.aten.permute.default(view_1, [0, 2, 1, 3]);  view_1 = None\n",
      "    view_2 = torch.ops.aten.view.default(getitem_2, [sym_size_int_1, 64, 4, 64]);  getitem_2 = None\n",
      "    permute_2 = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None\n",
      "    op__wrapper__torch__c__nn_scaled_dot_product_attention__4535865792 = torch.ops.modula.op__wrapper__torch__C__nn_scaled_dot_product_attention__4535865792.default(permute, permute_1, permute_2, None, 0.0, True);  permute = permute_1 = permute_2 = None\n",
      "    permute_3 = torch.ops.aten.permute.default(op__wrapper__torch__c__nn_scaled_dot_product_attention__4535865792, [0, 2, 1, 3]);  op__wrapper__torch__c__nn_scaled_dot_product_attention__4535865792 = None\n",
      "    view_3 = torch.ops.aten.view.default(permute_3, [sym_size_int_1, 64, 256]);  permute_3 = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862673 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(view_3, transformer_h_0_attn_out_proj_weight);  view_3 = transformer_h_0_attn_out_proj_weight = None\n",
      "    op__wrapper__torch__ops_aten_aten_add_tensor__4740564241 = torch.ops.modula.op__wrapper__torch__ops_aten_aten_add_Tensor__4740564240.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564240, op__wrapper__torch__c__nn_linear__4535862673);  op__wrapper__torch__ops_aten_aten_add_tensor__4740564240 = op__wrapper__torch__c__nn_linear__4535862673 = None\n",
      "    op__wrapper__torch_nn_functional_layer_norm__4721199521 = torch.ops.modula.op__wrapper__torch_nn_functional_layer_norm__4721199520.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564241, [256], transformer_h_0_ln_2_weight, transformer_h_0_ln_2_bias);  transformer_h_0_ln_2_weight = transformer_h_0_ln_2_bias = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862674 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(op__wrapper__torch_nn_functional_layer_norm__4721199521, transformer_h_0_mlp_fc1_weight);  op__wrapper__torch_nn_functional_layer_norm__4721199521 = transformer_h_0_mlp_fc1_weight = None\n",
      "    gelu = torch.ops.aten.gelu.default(op__wrapper__torch__c__nn_linear__4535862674);  op__wrapper__torch__c__nn_linear__4535862674 = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862675 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(gelu, transformer_h_0_mlp_fc2_weight);  gelu = transformer_h_0_mlp_fc2_weight = None\n",
      "    op__wrapper__torch__ops_aten_aten_add_tensor__4740564242 = torch.ops.modula.op__wrapper__torch__ops_aten_aten_add_Tensor__4740564240.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564241, op__wrapper__torch__c__nn_linear__4535862675);  op__wrapper__torch__ops_aten_aten_add_tensor__4740564241 = op__wrapper__torch__c__nn_linear__4535862675 = None\n",
      "    op__wrapper__torch_nn_functional_layer_norm__4721199522 = torch.ops.modula.op__wrapper__torch_nn_functional_layer_norm__4721199520.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564242, [256], transformer_h_1_ln_1_weight, transformer_h_1_ln_1_bias);  transformer_h_1_ln_1_weight = transformer_h_1_ln_1_bias = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862676 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(op__wrapper__torch_nn_functional_layer_norm__4721199522, transformer_h_1_attn_in_proj_weight);  op__wrapper__torch_nn_functional_layer_norm__4721199522 = transformer_h_1_attn_in_proj_weight = None\n",
      "    split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(op__wrapper__torch__c__nn_linear__4535862676, [256, 256, 256], -1);  op__wrapper__torch__c__nn_linear__4535862676 = None\n",
      "    getitem_3 = split_with_sizes_1[0]\n",
      "    getitem_4 = split_with_sizes_1[1]\n",
      "    getitem_5 = split_with_sizes_1[2];  split_with_sizes_1 = None\n",
      "    view_4 = torch.ops.aten.view.default(getitem_3, [sym_size_int_1, 64, 4, 64]);  getitem_3 = None\n",
      "    permute_4 = torch.ops.aten.permute.default(view_4, [0, 2, 1, 3]);  view_4 = None\n",
      "    view_5 = torch.ops.aten.view.default(getitem_4, [sym_size_int_1, 64, 4, 64]);  getitem_4 = None\n",
      "    permute_5 = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None\n",
      "    view_6 = torch.ops.aten.view.default(getitem_5, [sym_size_int_1, 64, 4, 64]);  getitem_5 = None\n",
      "    permute_6 = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None\n",
      "    op__wrapper__torch__c__nn_scaled_dot_product_attention__4535865793 = torch.ops.modula.op__wrapper__torch__C__nn_scaled_dot_product_attention__4535865792.default(permute_4, permute_5, permute_6, None, 0.0, True);  permute_4 = permute_5 = permute_6 = None\n",
      "    permute_7 = torch.ops.aten.permute.default(op__wrapper__torch__c__nn_scaled_dot_product_attention__4535865793, [0, 2, 1, 3]);  op__wrapper__torch__c__nn_scaled_dot_product_attention__4535865793 = None\n",
      "    view_7 = torch.ops.aten.view.default(permute_7, [sym_size_int_1, 64, 256]);  permute_7 = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862677 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(view_7, transformer_h_1_attn_out_proj_weight);  view_7 = transformer_h_1_attn_out_proj_weight = None\n",
      "    op__wrapper__torch__ops_aten_aten_add_tensor__4740564243 = torch.ops.modula.op__wrapper__torch__ops_aten_aten_add_Tensor__4740564240.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564242, op__wrapper__torch__c__nn_linear__4535862677);  op__wrapper__torch__ops_aten_aten_add_tensor__4740564242 = op__wrapper__torch__c__nn_linear__4535862677 = None\n",
      "    op__wrapper__torch_nn_functional_layer_norm__4721199523 = torch.ops.modula.op__wrapper__torch_nn_functional_layer_norm__4721199520.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564243, [256], transformer_h_1_ln_2_weight, transformer_h_1_ln_2_bias);  transformer_h_1_ln_2_weight = transformer_h_1_ln_2_bias = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862678 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(op__wrapper__torch_nn_functional_layer_norm__4721199523, transformer_h_1_mlp_fc1_weight);  op__wrapper__torch_nn_functional_layer_norm__4721199523 = transformer_h_1_mlp_fc1_weight = None\n",
      "    gelu_1 = torch.ops.aten.gelu.default(op__wrapper__torch__c__nn_linear__4535862678);  op__wrapper__torch__c__nn_linear__4535862678 = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862679 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(gelu_1, transformer_h_1_mlp_fc2_weight);  gelu_1 = transformer_h_1_mlp_fc2_weight = None\n",
      "    op__wrapper__torch__ops_aten_aten_add_tensor__4740564244 = torch.ops.modula.op__wrapper__torch__ops_aten_aten_add_Tensor__4740564240.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564243, op__wrapper__torch__c__nn_linear__4535862679);  op__wrapper__torch__ops_aten_aten_add_tensor__4740564243 = op__wrapper__torch__c__nn_linear__4535862679 = None\n",
      "    op__wrapper__torch_nn_functional_layer_norm__4721199524 = torch.ops.modula.op__wrapper__torch_nn_functional_layer_norm__4721199520.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564244, [256], transformer_h_2_ln_1_weight, transformer_h_2_ln_1_bias);  transformer_h_2_ln_1_weight = transformer_h_2_ln_1_bias = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862680 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(op__wrapper__torch_nn_functional_layer_norm__4721199524, transformer_h_2_attn_in_proj_weight);  op__wrapper__torch_nn_functional_layer_norm__4721199524 = transformer_h_2_attn_in_proj_weight = None\n",
      "    split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(op__wrapper__torch__c__nn_linear__4535862680, [256, 256, 256], -1);  op__wrapper__torch__c__nn_linear__4535862680 = None\n",
      "    getitem_6 = split_with_sizes_2[0]\n",
      "    getitem_7 = split_with_sizes_2[1]\n",
      "    getitem_8 = split_with_sizes_2[2];  split_with_sizes_2 = None\n",
      "    view_8 = torch.ops.aten.view.default(getitem_6, [sym_size_int_1, 64, 4, 64]);  getitem_6 = None\n",
      "    permute_8 = torch.ops.aten.permute.default(view_8, [0, 2, 1, 3]);  view_8 = None\n",
      "    view_9 = torch.ops.aten.view.default(getitem_7, [sym_size_int_1, 64, 4, 64]);  getitem_7 = None\n",
      "    permute_9 = torch.ops.aten.permute.default(view_9, [0, 2, 1, 3]);  view_9 = None\n",
      "    view_10 = torch.ops.aten.view.default(getitem_8, [sym_size_int_1, 64, 4, 64]);  getitem_8 = None\n",
      "    permute_10 = torch.ops.aten.permute.default(view_10, [0, 2, 1, 3]);  view_10 = None\n",
      "    op__wrapper__torch__c__nn_scaled_dot_product_attention__4535865794 = torch.ops.modula.op__wrapper__torch__C__nn_scaled_dot_product_attention__4535865792.default(permute_8, permute_9, permute_10, None, 0.0, True);  permute_8 = permute_9 = permute_10 = None\n",
      "    permute_11 = torch.ops.aten.permute.default(op__wrapper__torch__c__nn_scaled_dot_product_attention__4535865794, [0, 2, 1, 3]);  op__wrapper__torch__c__nn_scaled_dot_product_attention__4535865794 = None\n",
      "    view_11 = torch.ops.aten.view.default(permute_11, [sym_size_int_1, 64, 256]);  permute_11 = sym_size_int_1 = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862681 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(view_11, transformer_h_2_attn_out_proj_weight);  view_11 = transformer_h_2_attn_out_proj_weight = None\n",
      "    op__wrapper__torch__ops_aten_aten_add_tensor__4740564245 = torch.ops.modula.op__wrapper__torch__ops_aten_aten_add_Tensor__4740564240.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564244, op__wrapper__torch__c__nn_linear__4535862681);  op__wrapper__torch__ops_aten_aten_add_tensor__4740564244 = op__wrapper__torch__c__nn_linear__4535862681 = None\n",
      "    op__wrapper__torch_nn_functional_layer_norm__4721199525 = torch.ops.modula.op__wrapper__torch_nn_functional_layer_norm__4721199520.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564245, [256], transformer_h_2_ln_2_weight, transformer_h_2_ln_2_bias);  transformer_h_2_ln_2_weight = transformer_h_2_ln_2_bias = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862682 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(op__wrapper__torch_nn_functional_layer_norm__4721199525, transformer_h_2_mlp_fc1_weight);  op__wrapper__torch_nn_functional_layer_norm__4721199525 = transformer_h_2_mlp_fc1_weight = None\n",
      "    gelu_2 = torch.ops.aten.gelu.default(op__wrapper__torch__c__nn_linear__4535862682);  op__wrapper__torch__c__nn_linear__4535862682 = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862683 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(gelu_2, transformer_h_2_mlp_fc2_weight);  gelu_2 = transformer_h_2_mlp_fc2_weight = None\n",
      "    op__wrapper__torch__ops_aten_aten_add_tensor__4740564246 = torch.ops.modula.op__wrapper__torch__ops_aten_aten_add_Tensor__4740564240.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564245, op__wrapper__torch__c__nn_linear__4535862683);  op__wrapper__torch__ops_aten_aten_add_tensor__4740564245 = op__wrapper__torch__c__nn_linear__4535862683 = None\n",
      "    op__wrapper__torch_nn_functional_layer_norm__4721199526 = torch.ops.modula.op__wrapper__torch_nn_functional_layer_norm__4721199520.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564246, [256], transformer_ln_f_weight, transformer_ln_f_bias);  op__wrapper__torch__ops_aten_aten_add_tensor__4740564246 = transformer_ln_f_weight = transformer_ln_f_bias = None\n",
      "    op__wrapper__torch__c__nn_linear__4535862684 = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(op__wrapper__torch_nn_functional_layer_norm__4721199526, lm_head_weight_1, lm_head_bias);  op__wrapper__torch_nn_functional_layer_norm__4721199526 = lm_head_weight_1 = lm_head_bias = None\n",
      "    return pytree.tree_unflatten((op__wrapper__torch__c__nn_linear__4535862684,), self._out_spec)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "print(gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.export import Dim, ExportedProgram\n",
    "\n",
    "batch = Dim('batch')\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(15, 16)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(15, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "        )\n",
    "        self.scaler = ConstantScaler(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        v = self.scaler(x) + torch.randn(15)\n",
    "        return self.linear(v) + self.net(v)\n",
    "\n",
    "net = MyNet()\n",
    "example_input = torch.randn(10, 15, requires_grad=True),\n",
    "\n",
    "with modula_export():\n",
    "    ep: ExportedProgram = torch.export.export(\n",
    "        net,\n",
    "        example_input,\n",
    "        dynamic_shapes=[{0: batch}]\n",
    "    )\n",
    "\n",
    "    ep = ep.run_decompositions()\n",
    "\n",
    "gm = ep.module()\n",
    "# https://pytorch.org/docs/stable/export.ir_spec.html#node\n",
    "# gm = aot_module(gm, (torch.randn(2, 15, requires_grad=True),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExportedProgram:\n",
      "    class GraphModule(torch.nn.Module):\n",
      "        def forward(self, p_linear_weight: \"f32[16, 15]\", p_linear_bias: \"f32[16]\", p_net_0_weight: \"f32[16, 15]\", p_net_0_bias: \"f32[16]\", p_net_2_weight: \"f32[16, 16]\", p_net_2_bias: \"f32[16]\", p_net_4_weight: \"f32[16, 16]\", p_net_4_bias: \"f32[16]\", b_scaler_scale: \"f32[]\", x: \"f32[s0, 15]\"):\n",
      "             # File: /Users/S_sn/miniconda3/lib/python3.11/site-packages/torch/_library/custom_ops.py:669 in __call__, code: return self._opoverload(*args, **kwargs)\n",
      "            op__constant_scaler_mul____main___constant_scaler__mul_with_scaler__13450329472: \"f32[s0, 15]\" = torch.ops.modula.op__constant_scaler_mul____main___ConstantScaler__mul_with_scaler__13450329472.default(x, b_scaler_scale);  x = b_scaler_scale = None\n",
      "            \n",
      "             # File: /var/folders/ky/gxqpxwvx29ggdsqzf67zslfh0000gn/T/ipykernel_69251/3892711258.py:19 in forward, code: v = self.scaler(x) + torch.randn(15)\n",
      "            op__wrapper__torch__ops_aten_aten_randn__4804563472: \"f32[15]\" = torch.ops.modula.op__wrapper__torch__ops_aten_aten_randn__4804563472.default([15], device = device(type='cpu'), pin_memory = False)\n",
      "            op__wrapper__torch__ops_aten_aten_add_tensor__4740564240: \"f32[s0, 15]\" = torch.ops.modula.op__wrapper__torch__ops_aten_aten_add_Tensor__4740564240.default(op__constant_scaler_mul____main___constant_scaler__mul_with_scaler__13450329472, op__wrapper__torch__ops_aten_aten_randn__4804563472);  op__constant_scaler_mul____main___constant_scaler__mul_with_scaler__13450329472 = op__wrapper__torch__ops_aten_aten_randn__4804563472 = None\n",
      "            \n",
      "             # File: /var/folders/ky/gxqpxwvx29ggdsqzf67zslfh0000gn/T/ipykernel_69251/3892711258.py:20 in forward, code: return self.linear(v) + self.net(v)\n",
      "            op__wrapper__torch__c__nn_linear__4535862672: \"f32[s0, 16]\" = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564240, p_linear_weight, p_linear_bias);  p_linear_weight = p_linear_bias = None\n",
      "            op__wrapper__torch__c__nn_linear__4535862673: \"f32[s0, 16]\" = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(op__wrapper__torch__ops_aten_aten_add_tensor__4740564240, p_net_0_weight, p_net_0_bias);  op__wrapper__torch__ops_aten_aten_add_tensor__4740564240 = p_net_0_weight = p_net_0_bias = None\n",
      "            op__wrapper__torch__ops_aten_aten_relu__4806405200: \"f32[s0, 16]\" = torch.ops.modula.op__wrapper__torch__ops_aten_aten_relu__4806405200.default(op__wrapper__torch__c__nn_linear__4535862673);  op__wrapper__torch__c__nn_linear__4535862673 = None\n",
      "            op__wrapper__torch__c__nn_linear__4535862674: \"f32[s0, 16]\" = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(op__wrapper__torch__ops_aten_aten_relu__4806405200, p_net_2_weight, p_net_2_bias);  op__wrapper__torch__ops_aten_aten_relu__4806405200 = p_net_2_weight = p_net_2_bias = None\n",
      "            op__wrapper__torch__ops_aten_aten_relu__4806405201: \"f32[s0, 16]\" = torch.ops.modula.op__wrapper__torch__ops_aten_aten_relu__4806405200.default(op__wrapper__torch__c__nn_linear__4535862674);  op__wrapper__torch__c__nn_linear__4535862674 = None\n",
      "            op__wrapper__torch__c__nn_linear__4535862675: \"f32[s0, 16]\" = torch.ops.modula.op__wrapper__torch__C__nn_linear__4535862672.default(op__wrapper__torch__ops_aten_aten_relu__4806405201, p_net_4_weight, p_net_4_bias);  op__wrapper__torch__ops_aten_aten_relu__4806405201 = p_net_4_weight = p_net_4_bias = None\n",
      "            op__wrapper__torch__ops_aten_aten_add_tensor__4740564241: \"f32[s0, 16]\" = torch.ops.modula.op__wrapper__torch__ops_aten_aten_add_Tensor__4740564240.default(op__wrapper__torch__c__nn_linear__4535862672, op__wrapper__torch__c__nn_linear__4535862675);  op__wrapper__torch__c__nn_linear__4535862672 = op__wrapper__torch__c__nn_linear__4535862675 = None\n",
      "            return (op__wrapper__torch__ops_aten_aten_add_tensor__4740564241,)\n",
      "            \n",
      "Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_linear_weight'), target='linear.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_linear_bias'), target='linear.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_0_weight'), target='net.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_0_bias'), target='net.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_2_weight'), target='net.2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_2_bias'), target='net.2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_4_weight'), target='net.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_4_bias'), target='net.4.bias', persistent=None), InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='b_scaler_scale'), target='scaler.scale', persistent=True), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='op__wrapper__torch__ops_aten_aten_add_tensor__4740564241'), target=None)])\n",
      "Range constraints: {s0: VR[0, int_oo]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[p_linear_weight,\n",
       " p_linear_bias,\n",
       " p_net_0_weight,\n",
       " p_net_0_bias,\n",
       " p_net_2_weight,\n",
       " p_net_2_bias,\n",
       " p_net_4_weight,\n",
       " p_net_4_bias,\n",
       " b_scaler_scale,\n",
       " x,\n",
       " op__constant_scaler_mul____main___constant_scaler__mul_with_scaler__13450329472,\n",
       " op__wrapper__torch__ops_aten_aten_randn__4804563472,\n",
       " op__wrapper__torch__ops_aten_aten_add_tensor__4740564240,\n",
       " op__wrapper__torch__c__nn_linear__4535862672,\n",
       " op__wrapper__torch__c__nn_linear__4535862673,\n",
       " op__wrapper__torch__ops_aten_aten_relu__4806405200,\n",
       " op__wrapper__torch__c__nn_linear__4535862674,\n",
       " op__wrapper__torch__ops_aten_aten_relu__4806405201,\n",
       " op__wrapper__torch__c__nn_linear__4535862675,\n",
       " op__wrapper__torch__ops_aten_aten_add_tensor__4740564241,\n",
       " output]"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ep.graph_module.graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModuleCallEntry(fqn='', signature=ModuleCallSignature(inputs=[], outputs=[], in_spec=TreeSpec(tuple, None, [TreeSpec(tuple, None, [*]),\n",
       "   TreeSpec(dict, [], [])]), out_spec=*)),\n",
       " ModuleCallEntry(fqn='linear', signature=None),\n",
       " ModuleCallEntry(fqn='net', signature=None),\n",
       " ModuleCallEntry(fqn='net.0', signature=None),\n",
       " ModuleCallEntry(fqn='net.1', signature=None),\n",
       " ModuleCallEntry(fqn='net.2', signature=None),\n",
       " ModuleCallEntry(fqn='net.3', signature=None),\n",
       " ModuleCallEntry(fqn='net.4', signature=None),\n",
       " ModuleCallEntry(fqn='scaler', signature=None)]"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep.module_call_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(ep.graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%p_linear_weight : [num_users=1] = placeholder[target=p_linear_weight]'"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].format_node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_linear_weight'), target='linear.weight', persistent=None),\n",
       " InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_linear_bias'), target='linear.bias', persistent=None),\n",
       " InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_0_weight'), target='net.0.weight', persistent=None),\n",
       " InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_0_bias'), target='net.0.bias', persistent=None),\n",
       " InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_2_weight'), target='net.2.weight', persistent=None),\n",
       " InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_2_bias'), target='net.2.bias', persistent=None),\n",
       " InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_4_weight'), target='net.4.weight', persistent=None),\n",
       " InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_net_4_bias'), target='net.4.bias', persistent=None),\n",
       " InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='b_scaler_scale'), target='scaler.scale', persistent=True),\n",
       " InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None)]"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep.graph_signature.input_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enum 'InputKind'>"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep.graph_signature.input_specs[0].kind.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.export.graph_signature import InputKind\n",
    "inputs = []\n",
    "for node, spec in zip(nodes, ep.graph_signature.input_specs):\n",
    "    if spec.kind == InputKind.PARAMETER:\n",
    "        target = spec.target\n",
    "        if target.endswith('.weight'):\n",
    "            input = RMS_RMS_NormTensor(4, elem_dims=(-1, -2), backing_tensor=node.meta['val'], requires_grad=True)\n",
    "        elif target.endswith('.bias'):\n",
    "            input = RMS_NormTensor(2.5, elem_dims=(-1,), backing_tensor=node.meta['val'], requires_grad=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown parameter: {target}\")\n",
    "    elif spec.kind == InputKind.BUFFER:\n",
    "        assert spec.target == 'scaler.scale'\n",
    "        input = net.scaler.scale.clone().requires_grad_(True)\n",
    "    elif spec.kind == InputKind.USER_INPUT:\n",
    "        input = RMS_NormTensor(3, elem_dims=(-1,), backing_tensor=node.meta['val'], requires_grad=True)\n",
    "    inputs.append(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_linear_weight dict_keys(['val', 'tensor_meta', 'source_fn_stack', 'from_node', 'seq_nr', 'example_value'])\n",
      "--------------------\n",
      "p_linear_bias dict_keys(['val', 'tensor_meta', 'source_fn_stack', 'from_node', 'seq_nr', 'example_value'])\n",
      "--------------------\n",
      "b_scaler_scale dict_keys(['val', 'tensor_meta', 'from_node', 'seq_nr', 'example_value', 'source_fn_stack'])\n",
      "--------------------\n",
      "x dict_keys(['val', 'tensor_meta'])\n",
      "--------------------\n",
      "op__constant_scaler_mul____main___constant_scaler__mul_with_scaler__13464146048 dict_keys(['stack_trace', 'nn_module_stack', 'torch_fn', 'source_fn_stack', 'original_aten', 'from_node', 'seq_nr', 'val', 'tensor_meta'])\n",
      "--------------------\n",
      "op__wrapper__torch__ops_aten_aten_randn__4804563472 dict_keys(['stack_trace', 'nn_module_stack', 'torch_fn', 'source_fn_stack', 'original_aten', 'from_node', 'seq_nr', 'val', 'tensor_meta'])\n",
      "--------------------\n",
      "op__wrapper__torch__ops_aten_aten_add_tensor__4740564240 dict_keys(['stack_trace', 'nn_module_stack', 'torch_fn', 'source_fn_stack', 'original_aten', 'from_node', 'seq_nr', 'val', 'tensor_meta'])\n",
      "--------------------\n",
      "op__wrapper__torch__c__nn_linear__4535862672 dict_keys(['stack_trace', 'nn_module_stack', 'torch_fn', 'source_fn_stack', 'original_aten', 'from_node', 'seq_nr', 'val', 'tensor_meta'])\n",
      "--------------------\n",
      "output dict_keys([])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for node in nodes:\n",
    "    print(node, node.meta.keys())\n",
    "    print('--'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': FakeTensor(..., size=()),\n",
       " 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}),\n",
       " 'from_node': [('op__constant_scaler_mul____main___constant_scaler__mul_with_scaler__13464146048_default',\n",
       "   <OpOverload(op='modula.op__constant_scaler_mul____main___ConstantScaler__mul_with_scaler__13464146048', overload='default')>)],\n",
       " 'seq_nr': 9977,\n",
       " 'example_value': FakeTensor(..., size=()),\n",
       " 'source_fn_stack': [('op__constant_scaler_mul____main___constant_scaler__mul_with_scaler__13464146048_default',\n",
       "   <OpOverload(op='modula.op__constant_scaler_mul____main___ConstantScaler__mul_with_scaler__13464146048', overload='default')>)]}"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[2].meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  File \"/var/folders/ky/gxqpxwvx29ggdsqzf67zslfh0000gn/T/ipykernel_69251/3024122329.py\", line 12, in forward\\n    v = self.scaler(x) + torch.randn(15)\\n  File \"/var/folders/ky/gxqpxwvx29ggdsqzf67zslfh0000gn/T/ipykernel_69251/3538699669.py\", line 287, in forward\\n    return ConstantScaler._mul_with_scaler(x, self.scale)\\n  File \"/var/folders/ky/gxqpxwvx29ggdsqzf67zslfh0000gn/T/ipykernel_69251/3538699669.py\", line 150, in __call__\\n    return self.wrapper_custom_op(*args, **kwargs)\\n  File \"/Users/S_sn/miniconda3/lib/python3.11/site-packages/torch/_library/custom_ops.py\", line 669, in __call__\\n    return self._opoverload(*args, **kwargs)\\n'"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[4].stack_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMS_NormTensor(\n",
       "    norm_size=tensor(284.0711, grad_fn=<AddBackward0>),\n",
       "    elem_dims=(1,),\n",
       "    unwrapped=FakeTensor(..., size=(s0, 16)),\n",
       ")"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with norm_propagate_dispatch() as mode:\n",
    "    out, = ep.graph_module(*inputs)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(108.),)"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(out.norm_size, inputs[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: aten.full.default, () True None\n",
      "Dispatch Log: aten.full.default, () True None\n",
      "Dispatch Log: aten.full.default, () True None\n",
      "Dispatch Log: modula.op__constant_scaler_mul____main___ConstantScaler__mul_with_scaler__13464011456.default, (<class '__main__.RMS_NormTensor'>,) True None\n",
      "Dispatch Log: aten.mul.Tensor, () True None\n",
      "Dispatch Log: aten.empty.memory_format, () True None\n",
      "Dispatch Log: modula.op__wrapper__torch__ops_aten_aten_randn__4804563472.default, () True None\n",
      "Dispatch Log: aten.empty.memory_format, () True None\n",
      "Dispatch Log: aten.full.default, () True None\n",
      "Dispatch Log: modula.op__wrapper__torch__ops_aten_aten_add_Tensor__4740564240.default, (<class '__main__.RMS_NormTensor'>,) True None\n",
      "Dispatch Log: aten.mul.Tensor, () True None\n",
      "Dispatch Log: aten.add.Tensor, () True None\n",
      "Dispatch Log: aten.empty.memory_format, () True None\n",
      "Dispatch Log: modula.op__wrapper__torch__C__nn_linear__4535862672.default, (<class '__main__.RMS_NormTensor'>, <class '__main__.RMS_RMS_NormTensor'>) True None\n",
      "Dispatch Log: aten.mul.Tensor, () True None\n",
      "Dispatch Log: aten.add_.Tensor, () True None\n",
      "Dispatch Log: aten.empty.memory_format, () True None\n"
     ]
    }
   ],
   "source": [
    "with norm_propagate_dispatch():\n",
    "    out = ep.graph_module(\n",
    "        RMS_RMS_NormTensor(4, elem_dims=(-1, -2), backing_tensor=nodes[0].meta['val'], requires_grad=True),\n",
    "        RMS_NormTensor(2.5, elem_dims=(-1,), backing_tensor=nodes[1].meta['val'], requires_grad=True),\n",
    "        net.scaler.scale.requires_grad_(True),\n",
    "        RMS_NormTensor(3, elem_dims=(-1,), backing_tensor=nodes[3].meta['val'], requires_grad=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RMS_NormTensor(\n",
       "     norm_size=tensor(30.5000, grad_fn=<AddBackward0>),\n",
       "     elem_dims=(1,),\n",
       "     unwrapped=FakeTensor(..., size=(s0, 16)),\n",
       " ),)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0].meta['norm_val'] = RMS_RMS_NormTensor(4, elem_dims=(-1, -2), backing_tensor=nodes[0].meta['val'], requires_grad=True)\n",
    "nodes[1].meta['norm_val'] = RMS_NormTensor(2.5, elem_dims=(-1,), backing_tensor=nodes[1].meta['val'], requires_grad=True)\n",
    "nodes[2].meta['norm_val'] = net.scaler.scale.requires_grad_(True)\n",
    "# nodes[2].meta['norm_val'] = RMS_NormTensor(3, elem_dims=(-1,), backing_tensor=nodes[2].meta['val'], requires_grad=True)\n",
    "nodes[3].meta['norm_val'] = RMS_NormTensor(3, elem_dims=(-1,), backing_tensor=nodes[3].meta['val'], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: modula.op__constant_scaler_mul____main___ConstantScaler__mul_with_scaler__13506153504.default, (<class '__main__.RMS_NormTensor'>,)\n",
      "Dispatch Log: aten.mul.Tensor, ()\n",
      "Dispatch Log: aten.empty.memory_format, ()\n",
      "Dispatch Log: modula.op__constant_scaler_mul____main___ConstantScaler__mul_with_scaler__13506153504.default, (<class '__main__.RMS_NormTensor'>,)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[355], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node\u001b[38;5;241m.\u001b[39mtarget, \u001b[38;5;28mstr\u001b[39m): \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m flat_out, out_tree_spec \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_flatten(node\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m flat_unfinalized_normed_out, unfinalized_normed_out_tree_spec \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_flatten(\n\u001b[0;32m---> 10\u001b[0m     node\u001b[38;5;241m.\u001b[39mtarget(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;241m*\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm_val\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode) \u001b[38;5;28;01melse\u001b[39;00m x, node\u001b[38;5;241m.\u001b[39margs),\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm_val\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode) \u001b[38;5;28;01melse\u001b[39;00m x, node\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(flat_unfinalized_normed_out[0]._norm_size)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pytree\u001b[38;5;241m.\u001b[39mtreespec_dumps(out_tree_spec) \u001b[38;5;241m==\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtreespec_dumps(unfinalized_normed_out_tree_spec), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTree spec mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_ops.py:716\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_library/autograd.py:111\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.autograd_impl\u001b[0;34m(keyset, *args, **keyword_only_args)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautograd_impl\u001b[39m(keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeyword_only_args):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mis_grad_enabled() \u001b[38;5;129;01mand\u001b[39;00m _pytree\u001b[38;5;241m.\u001b[39mtree_any_only(\n\u001b[1;32m    109\u001b[0m         Tensor, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mrequires_grad, args, not_list_of_tensor\n\u001b[1;32m    110\u001b[0m     ):\n\u001b[0;32m--> 111\u001b[0m         result \u001b[38;5;241m=\u001b[39m Generated\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, Metadata(keyset, keyword_only_args))  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         result \u001b[38;5;241m=\u001b[39m forward_no_grad(\u001b[38;5;241m*\u001b[39margs, Metadata(keyset, keyword_only_args))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_library/autograd.py:50\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.forward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m     48\u001b[0m keyset \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyset\n\u001b[1;32m     49\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyword_only_args\n\u001b[0;32m---> 50\u001b[0m result \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mredispatch(keyset \u001b[38;5;241m&\u001b[39m _C\u001b[38;5;241m.\u001b[39m_after_autograd_keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39m_setup_context_fn:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# The Dispatcher will remove args that are equal to their default\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# values from (args, kwargs). We're going to add it back so that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# their setup_context (along with the rest of their operator\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# registrations)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mfill_defaults(op\u001b[38;5;241m.\u001b[39m_schema, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_ops.py:721\u001b[0m, in \u001b[0;36mOpOverload.redispatch\u001b[0;34m(self, keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mredispatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle\u001b[38;5;241m.\u001b[39mredispatch_boxed(keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[337], line 242\u001b[0m, in \u001b[0;36mNormPropagateDispatchMode.__torch_dispatch__\u001b[0;34m(mode, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         unfinalized_normed \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mnormed_dispatcher(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m mode\u001b[38;5;241m.\u001b[39mfake_mode:\n\u001b[0;32m--> 242\u001b[0m             fake \u001b[38;5;241m=\u001b[39m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m finalize_normed_out(unfinalized_normed, fake)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# fake or real mode\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[337], line 162\u001b[0m, in \u001b[0;36mRegFakeNormOp.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_custom_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_library/custom_ops.py:669\u001b[0m, in \u001b[0;36mCustomOpDef.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opoverload(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_ops.py:716\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_library/autograd.py:111\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.autograd_impl\u001b[0;34m(keyset, *args, **keyword_only_args)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautograd_impl\u001b[39m(keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeyword_only_args):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mis_grad_enabled() \u001b[38;5;129;01mand\u001b[39;00m _pytree\u001b[38;5;241m.\u001b[39mtree_any_only(\n\u001b[1;32m    109\u001b[0m         Tensor, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mrequires_grad, args, not_list_of_tensor\n\u001b[1;32m    110\u001b[0m     ):\n\u001b[0;32m--> 111\u001b[0m         result \u001b[38;5;241m=\u001b[39m Generated\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, Metadata(keyset, keyword_only_args))  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         result \u001b[38;5;241m=\u001b[39m forward_no_grad(\u001b[38;5;241m*\u001b[39margs, Metadata(keyset, keyword_only_args))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_library/autograd.py:50\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.forward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m     48\u001b[0m keyset \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyset\n\u001b[1;32m     49\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyword_only_args\n\u001b[0;32m---> 50\u001b[0m result \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mredispatch(keyset \u001b[38;5;241m&\u001b[39m _C\u001b[38;5;241m.\u001b[39m_after_autograd_keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39m_setup_context_fn:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# The Dispatcher will remove args that are equal to their default\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# values from (args, kwargs). We're going to add it back so that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# their setup_context (along with the rest of their operator\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# registrations)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mfill_defaults(op\u001b[38;5;241m.\u001b[39m_schema, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_ops.py:721\u001b[0m, in \u001b[0;36mOpOverload.redispatch\u001b[0;34m(self, keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mredispatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle\u001b[38;5;241m.\u001b[39mredispatch_boxed(keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[337], line 245\u001b[0m, in \u001b[0;36mNormPropagateDispatchMode.__torch_dispatch__\u001b[0;34m(mode, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m finalize_normed_out(unfinalized_normed, fake)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# fake or real mode\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28missubclass\u001b[39m(t, NormedTensorBase) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.utils._pytree as pytree\n",
    "\n",
    "# with NormPropagateDispatchMode():\n",
    "with norm_propagate_dispatch(global_dispatch=True):\n",
    "    for node in nodes[3:]:\n",
    "        if isinstance(node.target, str): continue\n",
    "        flat_out, out_tree_spec = pytree.tree_flatten(node.meta['val'])\n",
    "\n",
    "        flat_unfinalized_normed_out, unfinalized_normed_out_tree_spec = pytree.tree_flatten(\n",
    "            node.target(\n",
    "                *tree_map(lambda x: x.meta['norm_val'] if isinstance(x, torch.fx.Node) else x, node.args),\n",
    "                **tree_map(lambda x: x.meta['norm_val'] if isinstance(x, torch.fx.Node) else x, node.kwargs)\n",
    "            )\n",
    "        )\n",
    "        # print(flat_unfinalized_normed_out[0]._norm_size)\n",
    "        assert pytree.treespec_dumps(out_tree_spec) == pytree.treespec_dumps(unfinalized_normed_out_tree_spec), f\"Tree spec mismatch\"\n",
    "        node.meta['norm_val'] = pytree.tree_unflatten(\n",
    "            [\n",
    "                normed.finalize(out) for normed, out in zip(flat_unfinalized_normed_out, flat_out)\n",
    "            ],\n",
    "            out_tree_spec,\n",
    "        )\n",
    "        print(node.meta['norm_val'])\n",
    "        print('--'*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpOverload(op='modula.op__wrapper__aten_randn__5682461136', overload='default')>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.scaler.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12.),)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(nodes[-2].meta['norm_val'].norm_size, [net.scaler.scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<OpOverload(op='modula.op__constant_scaler_mul__aten_mul_Tensor__5206687248', overload='default')>: <__main__.RegFakeNormOp at 0x13690ca50>,\n",
       " <OpOverload(op='modula.op__wrapper__linear__4899357776', overload='default')>: <__main__.RegFakeNormOp at 0x30e135310>,\n",
       " <OpOverload(op='modula.op__wrapper__aten_randn__5211397008', overload='default')>: <__main__.RegFakeNormOp at 0x30e137650>,\n",
       " <OpOverload(op='modula.op__wrapper__aten_add_Tensor__5202966352', overload='default')>: <__main__.RegFakeNormOp at 0x177e30d10>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REG_FAKE_NORM_OP_LOOKUP_VIA_CUSTOM_OP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ops.modula.op__constant_scaler_mul__aten_mul_Tensor__5206687248.default in REG_FAKE_NORM_OP_LOOKUP_VIA_CUSTOM_OP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpOverload(op='modula.op__constant_scaler_mul__aten_mul_Tensor__5206687248', overload='default')>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(REG_FAKE_NORM_OP_LOOKUP_VIA_CUSTOM_OP.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMS_NormTensor(\n",
       "    norm_size=tensor(18.5000),\n",
       "    elem_dims=(1,),\n",
       "    unwrapped=FakeTensor(..., size=(s0, 16)),\n",
       ")"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[-2].meta['norm_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<OpOverloadPacket(op='modula.op_linear__4760847760')>: <__main__.RegFakeNormOp at 0x32bafd010>,\n",
       " <OpOverloadPacket(op='modula.op_aten_randn__5081374544')>: <__main__.RegFakeNormOp at 0x32baff090>,\n",
       " <OpOverloadPacket(op='modula.op_aten_add_Tensor__5074027472')>: <__main__.RegFakeNormOp at 0x32bd52350>}"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REG_FAKE_NORM_OP_LOOKUP_VIA_CUSTOM_OP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMS_RMS_NormTensor(\n",
       "    norm_size=tensor(4., requires_grad=True),\n",
       "    elem_dims=(0, 1),\n",
       "    unwrapped=FakeTensor(..., size=(16, 30), requires_grad=True),\n",
       ")"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].meta['norm_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMS_NormTensor(\n",
       "    norm_size=tensor(2.5000, requires_grad=True),\n",
       "    elem_dims=(0,),\n",
       "    unwrapped=FakeTensor(..., size=(16,), requires_grad=True),\n",
       ")"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[1].meta['norm_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS_NormTensor(\n",
      "    norm_size=tensor(3., requires_grad=True),\n",
      "    elem_dims=(1,),\n",
      "    unwrapped=FakeTensor(..., size=(s0, 30)),\n",
      ") RMS_RMS_NormTensor(\n",
      "    norm_size=tensor(4., requires_grad=True),\n",
      "    elem_dims=(0, 1),\n",
      "    unwrapped=FakeTensor(..., size=(16, 30), requires_grad=True),\n",
      ") RMS_NormTensor(\n",
      "    norm_size=tensor(2.5000, requires_grad=True),\n",
      "    elem_dims=(0,),\n",
      "    unwrapped=FakeTensor(..., size=(16,), requires_grad=True),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "node = nodes[3]\n",
    "node.meta['norm_val'] = node.target(\n",
    "    *[arg.meta['norm_val'] if isinstance(arg, torch.fx.Node) else arg for arg in node.args],\n",
    "    **{k: v.meta['norm_val'] for k, v in node.kwargs.items()}\n",
    ").finalize(node.meta['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMS_NormTensor(\n",
       "    norm_size=tensor(14.5000, grad_fn=<AddBackward0>),\n",
       "    elem_dims=(1,),\n",
       "    unwrapped=FakeTensor(..., size=(s0, 16)),\n",
       ")"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[3].meta['norm_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(1.), tensor(4.))"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(nodes[3].meta['norm_val'].norm_size, [nodes[0].meta['norm_val'].norm_size, nodes[1].meta['norm_val'].norm_size, nodes[2].meta['norm_val'].norm_size], allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ops.modula.op_linear__4760847760(torch.randn(10, 15, requires_grad=True), torch.randn(16, 15, requires_grad=True)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (input: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None) -> torch.Tensor>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._C.parse_schema(\"linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._C._nn.linear>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no signature found for builtin <built-in function gelu>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inspect\u001b[38;5;241m.\u001b[39msignature(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mgelu)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:3263\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Signature\u001b[38;5;241m.\u001b[39mfrom_callable(obj, follow_wrapped\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[1;32m   3264\u001b[0m                                    \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:3011\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   3008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   3009\u001b[0m                   follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3010\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   3012\u001b[0m                                     follow_wrapper_chains\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[1;32m   3013\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:2528\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[1;32m   2524\u001b[0m                                     skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg,\n\u001b[1;32m   2525\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n\u001b[1;32m   2527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[0;32m-> 2528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[1;32m   2529\u001b[0m                                    skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg)\n\u001b[1;32m   2531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, functools\u001b[38;5;241m.\u001b[39mpartial):\n\u001b[1;32m   2532\u001b[0m     wrapped_sig \u001b[38;5;241m=\u001b[39m _get_signature_of(obj\u001b[38;5;241m.\u001b[39mfunc)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:2326\u001b[0m, in \u001b[0;36m_signature_from_builtin\u001b[0;34m(cls, func, skip_bound_arg)\u001b[0m\n\u001b[1;32m   2324\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__text_signature__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s:\n\u001b[0;32m-> 2326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno signature found for builtin \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(func))\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _signature_fromstr(\u001b[38;5;28mcls\u001b[39m, func, s, skip_bound_arg)\n",
      "\u001b[0;31mValueError\u001b[0m: no signature found for builtin <built-in function gelu>"
     ]
    }
   ],
   "source": [
    "inspect.signature(torch.nn.functional.gelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "de@get_fake_norm_op().norm_register\n",
    "def unsqueeze(input: RMS_NormTensor, dim: int) -> RMS_NormTensor:\n",
    "    return RMS_NormTensor(input.norm_size, export_graph_node=get_current_export_graph_node())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpOverload(op='module.op_test__6275085504', overload='default')>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fake_norm_op(test).aten_or_custom_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no signature found for builtin <built-in function linear>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39minfer_schema(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear, mutates_args\u001b[38;5;241m=\u001b[39m())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_library/infer_schema.py:55\u001b[0m, in \u001b[0;36minfer_schema\u001b[0;34m(prototype_function, mutates_args, op_name)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Parses the schema of a given function with type hints. The schema is inferred from the\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mfunction's type hints, and can be used to define a new operator.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    (Tensor x) -> Tensor\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m UNKNOWN_MUTATES \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 55\u001b[0m sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(prototype_function)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_fn\u001b[39m(what):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer_schema(func): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot func with signature \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:3263\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Signature\u001b[38;5;241m.\u001b[39mfrom_callable(obj, follow_wrapped\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[1;32m   3264\u001b[0m                                    \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:3011\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   3008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   3009\u001b[0m                   follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3010\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   3012\u001b[0m                                     follow_wrapper_chains\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[1;32m   3013\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:2528\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[1;32m   2524\u001b[0m                                     skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg,\n\u001b[1;32m   2525\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n\u001b[1;32m   2527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[0;32m-> 2528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[1;32m   2529\u001b[0m                                    skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg)\n\u001b[1;32m   2531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, functools\u001b[38;5;241m.\u001b[39mpartial):\n\u001b[1;32m   2532\u001b[0m     wrapped_sig \u001b[38;5;241m=\u001b[39m _get_signature_of(obj\u001b[38;5;241m.\u001b[39mfunc)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/inspect.py:2326\u001b[0m, in \u001b[0;36m_signature_from_builtin\u001b[0;34m(cls, func, skip_bound_arg)\u001b[0m\n\u001b[1;32m   2324\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__text_signature__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s:\n\u001b[0;32m-> 2326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno signature found for builtin \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(func))\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _signature_fromstr(\u001b[38;5;28mcls\u001b[39m, func, s, skip_bound_arg)\n",
      "\u001b[0;31mValueError\u001b[0m: no signature found for builtin <built-in function linear>"
     ]
    }
   ],
   "source": [
    "torch.library.infer_schema(torch.nn.functional.linear, mutates_args=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpOverload(op='aten.unsqueeze', overload='default')>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fake_norm_op(torch.ops.aten.unsqueeze.default).aten_or_custom_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CustomOpDef(mylib::foo)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.library.custom_op(\"mylib::foo\", mutates_args={})\n",
    "def foo(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x.clone()\n",
    "\n",
    "foo.register_fake(foo)\n",
    "foo.register_fake(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import inspect\n",
    "import contextlib\n",
    "from torch.utils._python_dispatch import TorchDispatchMode\n",
    "from torch.overrides import enable_reentrant_dispatch, TorchFunctionMode\n",
    "from collections import defaultdict\n",
    "from torch._subclasses.fake_tensor import FakeTensorMode, FakeTensor\n",
    "from torch.utils._pytree import tree_map\n",
    "from typeguard import typechecked\n",
    "from torch.fx.operator_schemas import (\n",
    "    _torchscript_schema_to_signature,\n",
    ")\n",
    "import typing\n",
    "import numbers\n",
    "\n",
    "\n",
    "\n",
    "ENABLE_NORM_DISPATCH = True\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def disable_norm_dispatch():\n",
    "    global ENABLE_NORM_DISPATCH\n",
    "    old_flag = ENABLE_NORM_DISPATCH\n",
    "    ENABLE_NORM_DISPATCH = False\n",
    "    yield\n",
    "    ENABLE_NORM_DISPATCH = old_flag\n",
    "\n",
    "HANDLED_FUNCTIONS: Dict[Callable, NormedTensorDispatcher] = {}\n",
    "#defaultdict(lambda: Dispatcher(ignored_specialized_params=('_export_graph_node',)))\n",
    "\n",
    "\n",
    "def get_output_fake_tensors(func, *args, **kwargs):\n",
    "    # Create a fake mode\n",
    "    fake_mode = FakeTensorMode(allow_non_fake_inputs=True)\n",
    "    def convert_from_real_tensor(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return fake_mode.fake_tensor_converter.from_real_tensor(fake_mode, x)\n",
    "        return x\n",
    "    # Fakeify some real tensors\n",
    "    with fake_mode, disable_norm_dispatch():\n",
    "        args = tree_map(convert_from_real_tensor, args)\n",
    "        kwargs = tree_map(convert_from_real_tensor, kwargs)\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "\n",
    "def implements(torch_op):\n",
    "    \"\"\"Register a torch function override for ScalarTensor\"\"\"\n",
    "    def decorator(func):\n",
    "        # sig = inspect.signature(func)\n",
    "        # func = typechecked(func)\n",
    "        if torch_op not in HANDLED_FUNCTIONS:\n",
    "            HANDLED_FUNCTIONS[torch_op] = NormedTensorDispatcher(torch_op, ignored_params=('_export_graph_node',))\n",
    "        return HANDLED_FUNCTIONS[torch_op].register(func)\n",
    "        # assert_specialized(torch_function, func)\n",
    "        # dispatch_key = get_dispatch_key_func(torch_function, func)\n",
    "        # functools.update_wrapper(func, torch_function)\n",
    "        # assert torch_function not in HANDLED_FUNCTIONS\n",
    "        # assert dispatch_key not in HANDLED_FUNCTIONS[torch_function]\n",
    "        # HANDLED_FUNCTIONS[torch_function][dispatch_key] = func\n",
    "        return func\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class NormedOpRegularTensorMode(TorchFunctionMode):\n",
    "    def __torch_function__(self, func, types, args, kwargs=None):\n",
    "        if ENABLE_NORM_DISPATCH:\n",
    "            print(f\"Dispatch Log: {func}(*{args}, **{kwargs})\")\n",
    "            if any(issubclass(t, NormTensorBase) for t in types):\n",
    "                return NotImplemented\n",
    "        return func(*args, **(kwargs or {}))\n",
    "\n",
    "\n",
    "class NormedTensorMode(TorchDispatchMode):\n",
    "    def __torch_dispatch__(self, func, types, args, kwargs=None):\n",
    "        if ENABLE_NORM_DISPATCH:\n",
    "            print(f\"Dispatch Log: {func}(*{args}, **{kwargs})\")\n",
    "            if any(issubclass(t, NormTensorBase) for t in types):\n",
    "                return NotImplemented\n",
    "        return func(*args, **(kwargs or {}))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class SubclassOnce(type):\n",
    "#     _subclass_depth: int\n",
    "\n",
    "#     def __new__(cls, name, bases, classdict):\n",
    "#         restricted_bases = [b for b in bases if isinstance(b, SubclassOnce)]\n",
    "#         if len(restricted_bases) > 0:\n",
    "#             subclass_depth = max(b._subclass_depth for b in restricted_bases) + 1\n",
    "#         else:\n",
    "#             subclass_depth = 0\n",
    "#         if subclass_depth > 1:\n",
    "#             raise TypeError(f\"Type {cls.__name__} has multiple base types {bases}\")\n",
    "#         classdict = dict(classdict)\n",
    "#         assert '_subclass_depth' not in classdict\n",
    "#         classdict['_subclass_depth'] = subclass_depth\n",
    "#         return type.__new__(cls, name, bases, classdict)\n",
    "\n",
    "#     @property\n",
    "#     def subclassable(cls):\n",
    "#         return cls._subclass_depth < 1\n",
    "\n",
    "\n",
    "CURRENT_EXPORT_GRAPH_NODE: Optional[torch.fx.Node] = None\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def processing_export_graph_node(node: torch.fx.Node):\n",
    "    global CURRENT_EXPORT_GRAPH_NODE\n",
    "    old_node = CURRENT_EXPORT_GRAPH_NODE\n",
    "    CURRENT_EXPORT_GRAPH_NODE = node\n",
    "    yield node\n",
    "    CURRENT_EXPORT_GRAPH_NODE = old_node\n",
    "\n",
    "def get_current_export_graph_node():\n",
    "    assert CURRENT_EXPORT_GRAPH_NODE is not None, \"No export graph node is being processed\"\n",
    "    return CURRENT_EXPORT_GRAPH_NODE\n",
    "\n",
    "@implements(torch.ops.aten.unsqueeze.default)\n",
    "def unsqueeze(input: RMS_NormTensor, dim: int) -> RMS_NormTensor:\n",
    "    return RMS_NormTensor(input.norm_size, export_graph_node=get_current_export_graph_node())\n",
    "\n",
    "@implements(torch.ops.aten.squeeze_.dim)\n",
    "def squeeze(input: RMS_NormTensor, dim: int) -> RMS_NormTensor:\n",
    "    return RMS_NormTensor(input.norm_size, export_graph_node=get_current_export_graph_node())\n",
    "\n",
    "@implements(torch.ops.aten.permute.default)\n",
    "def permute(input: RMS_RMS_NormTensor, dims: List[int]) -> RMS_RMS_NormTensor:\n",
    "    assert input.ndim == 2\n",
    "    return RMS_RMS_NormTensor(input.norm_size, export_graph_node=get_current_export_graph_node())\n",
    "\n",
    "# @t.register\n",
    "# def t(input: RMSNormTensor) -> RMSRMSNormTensor:\n",
    "#     assert input.ndim == 2\n",
    "#     print(input.size(), 'l')\n",
    "#     return RMSRMSNormTensor(input.norm_size + 2, size=input.size()[::-1], dtype=input.dtype, device=input.device)\n",
    "\n",
    "@implements(torch.ops.aten.addmm.default)\n",
    "def addmm(input: RMS_NormTensor, mat1: RMS_NormTensor, mat2: RMS_RMS_NormTensor, *, beta: float = 1, alpha: float = 1) -> RMS_NormTensor:\n",
    "    # output = input * beta + mat1 @ mat2 * alpha\n",
    "    final_norm_size = input.norm_size * beta + mat1.norm_size * mat2.norm_size * alpha\n",
    "    return RMS_NormTensor(final_norm_size, export_graph_node=get_current_export_graph_node())\n",
    "\n",
    "@addmm.register\n",
    "def _(input: RMS_NormTensor, mat1: RMS_RMS_NormTensor, mat2: RMS_NormTensor, *, beta: float = 1, alpha: float = 1) -> RMS_NormTensor:\n",
    "    # output = input * beta + mat1 @ mat2 * alpha\n",
    "    final_norm_size = input.norm_size * beta + mat1.norm_size * mat2.norm_size * alpha\n",
    "    return RMS_NormTensor(final_norm_size, export_graph_node=get_current_export_graph_node())\n",
    "\n",
    "@implements(torch.ops.aten.mm.default)\n",
    "def mm(input: RMS_NormTensor, mat2: RMS_RMS_NormTensor) -> RMS_NormTensor:\n",
    "    final_norm_size = input.norm_size * mat2.norm_size\n",
    "    return RMS_NormTensor(final_norm_size, export_graph_node=get_current_export_graph_node())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real\n",
    "# fake\n",
    "# normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "from torch.export import Dim\n",
    "\n",
    "batch = Dim('batch', min=10)\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(15, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x.split(15, dim=1)[0])\n",
    "\n",
    "ep = torch.export.export(\n",
    "    nn.Linear(30, 16),\n",
    "    # MyNet(),\n",
    "    (torch.randn(10, 30, requires_grad=True),),\n",
    "    dynamic_shapes=[{0: batch}]\n",
    ")\n",
    "\n",
    "# ep = ep.run_decompositions()\n",
    "\n",
    "gm = ep.module()\n",
    "# https://pytorch.org/docs/stable/export.ir_spec.html#node\n",
    "# gm = aot_module(gm, (torch.randn(2, 15, requires_grad=True),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, input):\n",
      "    input, = fx_pytree.tree_flatten_spec(([input], {}), self._in_spec)\n",
      "    weight = self.weight\n",
      "    bias = self.bias\n",
      "    input_1 = input\n",
      "    linear = torch.ops.aten.linear.default(input_1, weight, bias);  input_1 = weight = bias = None\n",
      "    return pytree.tree_unflatten((linear,), self._out_spec)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "print(gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['%p_fn_weight : [num_users=1] = placeholder[target=p_fn_weight]',\n",
       " '%p_fn_bias : [num_users=1] = placeholder[target=p_fn_bias]',\n",
       " '%input : [num_users=1] = placeholder[target=input]',\n",
       " '%linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%input, %p_fn_weight, %p_fn_bias), kwargs = {})',\n",
       " 'return (linear,)']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = list(ep.graph.nodes)\n",
    "[\n",
    "    node.format_node()\n",
    "    for node in nodes\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (input: torch.Tensor, mat1: torch.Tensor, mat2: torch.Tensor, *, beta: numbers.Number = 1, alpha: numbers.Number = 1) -> torch.Tensor>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_torchscript_schema_to_signature(\n",
    "    torch.ops.aten.addmm.default._schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0].meta['norm_val'] = RMS_RMS_NormTensor(4, backing_tensor=nodes[0].meta['val'])\n",
    "nodes[1].meta['norm_val'] = RMS_NormTensor(2.5, backing_tensor=nodes[1].meta['val'])\n",
    "nodes[2].meta['norm_val'] = RMS_NormTensor(3, backing_tensor=nodes[2].meta['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[2].meta['val'].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: aten.linear.default(*(RMS_NormTensor(\n",
      "    norm_size=tensor(3.),\n",
      "    unwrapped=FakeTensor(..., size=(s0, 30), requires_grad=True),\n",
      "), RMS_RMS_NormTensor(\n",
      "    norm_size=tensor(4.),\n",
      "    unwrapped=FakeTensor(..., size=(16, 30), requires_grad=True),\n",
      "), RMS_NormTensor(\n",
      "    norm_size=tensor(2.5000),\n",
      "    unwrapped=FakeTensor(..., size=(16,), requires_grad=True),\n",
      ")), **{})\n",
      "Dispatch Log: aten.t.default(*(RMS_RMS_NormTensor(\n",
      "    norm_size=tensor(4.),\n",
      "    unwrapped=FakeTensor(..., size=(16, 30), requires_grad=True),\n",
      "),), **{})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Multiple dispatch failed for 'torch._ops.aten.t.default'; all __torch_dispatch__ handlers returned NotImplemented:\n\n  - tensor subclass <class '__main__.RMS_RMS_NormTensor'>\n\nFor more information, try re-running with TORCH_LOGS=not_implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m NormedOpRegularTensorMode(), NormedTensorMode(), processing_export_graph_node(nodes[\u001b[38;5;241m3\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m node:\n\u001b[0;32m----> 2\u001b[0m     node\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm_val\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mtarget(\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;241m*\u001b[39m[arg\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm_val\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode) \u001b[38;5;28;01melse\u001b[39;00m arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39margs],\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm_val\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      5\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_ops.py:716\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[69], line 70\u001b[0m, in \u001b[0;36mNormedOpRegularTensorMode.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28missubclass\u001b[39m(t, NormTensorBase) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types):\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_ops.py:716\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[69], line 79\u001b[0m, in \u001b[0;36mNormedTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28missubclass\u001b[39m(t, NormTensorBase) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types):\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_ops.py:716\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Multiple dispatch failed for 'torch._ops.aten.t.default'; all __torch_dispatch__ handlers returned NotImplemented:\n\n  - tensor subclass <class '__main__.RMS_RMS_NormTensor'>\n\nFor more information, try re-running with TORCH_LOGS=not_implemented"
     ]
    }
   ],
   "source": [
    "with NormedOpRegularTensorMode(), NormedTensorMode(), processing_export_graph_node(nodes[3]) as node:\n",
    "    node.meta['norm_val'] = node.target(\n",
    "        *[arg.meta['norm_val'] if isinstance(arg, torch.fx.Node) else arg for arg in node.args],\n",
    "        **{k: v.meta['norm_val'] for k, v in node.kwargs.items()}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMS_RMS_NormTensor(\n",
       "    norm_size=tensor(4.),\n",
       "    fake=FakeTensor(..., size=(15, 16)),\n",
       "    node=(\n",
       "        %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%p_linear_weight, [1, 0]), kwargs = {})\n",
       "    ),\n",
       ")"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.meta['norm_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(p_fn_bias, input, permute)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[p_fn_weight, p_fn_bias, input, permute, addmm, output]\n",
      "%p_fn_weight : [num_users=1] = placeholder[target=p_fn_weight]\n",
      "%p_fn_bias : [num_users=1] = placeholder[target=p_fn_bias]\n",
      "%input : [num_users=1] = placeholder[target=input]\n",
      "%permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%p_fn_weight, [1, 0]), kwargs = {})\n",
      "%addmm : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%p_fn_bias, %input, %permute), kwargs = {})\n",
      "return (addmm,)\n"
     ]
    }
   ],
   "source": [
    "print(list(ep.graph.nodes))\n",
    "\n",
    "node: torch.fx.node.Node\n",
    "for node in ep.graph.nodes:\n",
    "    print(node.format_node())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.args[0][0].meta['val'] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FakeTensor(..., size=(s0, 16))"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ep.graph.nodes)[-2].meta['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FakeTensor(..., size=(15, 16))"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ep.graph.nodes)[-3].meta['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: aten.full.default(*([], 3), **{'dtype': torch.float32, 'device': device(type='cpu'), 'pin_memory': False})\n",
      "Dispatch Log: aten.full.default(*([], 4), **{'dtype': torch.float32, 'device': device(type='cpu'), 'pin_memory': False})\n",
      "Dispatch Log: aten.t.default(*(RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True)),), **{})\n",
      "Dispatch Log: aten.t.default(*(RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True)),), **{})\n",
      "Dispatch Log: aten.unsqueeze.default(*(RMS_NormTensor(norm_size=tensor(3., requires_grad=True)), 0), **{})\n",
      "Dispatch Log: aten.unsqueeze.default(*(RMS_NormTensor(norm_size=tensor(3., requires_grad=True)), 0), **{})\n",
      "Dispatch Log: aten.mm.default(*(RMS_NormTensor(norm_size=tensor(3., requires_grad=True)), RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True))), **{})\n",
      "Dispatch Log: aten.mm.default(*(RMS_NormTensor(norm_size=tensor(3., requires_grad=True)), RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True))), **{})\n",
      "Dispatch Log: aten.mul.Tensor(*(tensor(3., requires_grad=True), tensor(4., requires_grad=True)), **{})\n",
      "Dispatch Log: aten.squeeze_.dim(*(RMS_NormTensor(norm_size=tensor(12., grad_fn=<MulBackward0>)), 0), **{})\n",
      "Dispatch Log: aten.squeeze_.dim(*(RMS_NormTensor(norm_size=tensor(12., grad_fn=<MulBackward0>)), 0), **{})\n",
      "RMS_NormTensor(norm_size=tensor(12., grad_fn=<MulBackward0>)) tensor(12., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: aten.add.Tensor(*(RMSNormTensor(norm_size=3, requires_grad=False), RMSRMSNormTensor(norm_size=4, requires_grad=False)), **{})\n",
      "tensor(7., grad_fn=<AddBackward0>)\n",
      "z: RMSNormTensor(norm_size=7, requires_grad=False) tensor(7., grad_fn=<AddBackward0>) [tensor(3., requires_grad=True), tensor(4.)] tensor(7., grad_fn=<AddBackward0>)\n",
      "RMSNormTensor(norm_size=7, requires_grad=False) tensor(7., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = (x:=RMSNormTensor(3, requires_grad=True)) + RMSRMSNormTensor(4)\n",
    "print(y, y.norm_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(y\u001b[38;5;241m.\u001b[39mnorm_size, [x\u001b[38;5;241m.\u001b[39mnorm_size])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m _engine_run_backward(\n\u001b[1;32m    497\u001b[0m         outputs,\n\u001b[1;32m    498\u001b[0m         grad_outputs_,\n\u001b[1;32m    499\u001b[0m         retain_graph,\n\u001b[1;32m    500\u001b[0m         create_graph,\n\u001b[1;32m    501\u001b[0m         inputs,\n\u001b[1;32m    502\u001b[0m         allow_unused,\n\u001b[1;32m    503\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(y.norm_size, [x.norm_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: aten.add.Tensor(*(RMSNormTensor(norm_size=3.0), RMSRMSNormTensor(norm_size=4.0)), **{})\n",
      "RMSNormTensor(norm_size=7.0) tensor(7.)\n",
      "Dispatch Log: aten.ones_like.default(*(RMSNormTensor(norm_size=7.0),), **{'pin_memory': False, 'memory_format': torch.preserve_format})\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m y \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m:=\u001b[39mRMSNormTensor(\u001b[38;5;241m3\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)) \u001b[38;5;241m+\u001b[39m RMSRMSNormTensor(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(y, y\u001b[38;5;241m.\u001b[39mnorm_size)\n\u001b[0;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward([y])\n\u001b[1;32m      4\u001b[0m y, x\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:220\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    219\u001b[0m         new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 220\u001b[0m             torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[71], line 55\u001b[0m, in \u001b[0;36mNormTensorBase.__torch_dispatch__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39madd\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mnorm_size, args)))\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ENABLE_NORM_DISPATCH:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "y = (x:=RMSNormTensor(3, requires_grad=True)) + RMSRMSNormTensor(4)\n",
    "print(y, y.norm_size)\n",
    "torch.autograd.backward([y])\n",
    "y, x.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y%debu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9366, -2.1385,  0.2499,  0.1769,  3.3125,  0.5611,  0.2110, -0.9544,\n",
       "         -1.4054, -1.0516],\n",
       "        [-1.3193,  2.8552, -2.4230,  2.6803,  4.9918,  3.8054, -1.6162, -3.6952,\n",
       "         -3.6532, -1.4474],\n",
       "        [-1.9989,  3.0092, -8.1097,  0.5183,  2.0837, -0.9222,  0.3195, -5.3470,\n",
       "         -3.0957,  2.3705],\n",
       "        [ 5.2813,  2.8777,  0.0169, -0.6770, -3.8537,  2.0995, -3.8720, -1.3664,\n",
       "         -2.2780,  2.0936],\n",
       "        [-0.3890, -0.7412,  2.1590,  0.5803, -0.2978,  0.4573, -0.6891,  0.1847,\n",
       "          0.4717, -1.0158],\n",
       "        [-4.2937,  4.5814,  0.6280,  0.9427, -4.7592, -3.3714, -5.1919, -6.9244,\n",
       "         -0.7811,  4.4841],\n",
       "        [-3.6631, -4.5173,  1.1486, -3.5798, -0.8339, -1.9358,  3.3141,  1.8897,\n",
       "          1.1224,  1.5577],\n",
       "        [ 2.7387, -2.0768,  0.9836, -0.7142,  3.7925, -3.0117, -0.3672, -1.0644,\n",
       "          4.0226, -2.9036],\n",
       "        [-3.9206,  0.3760,  3.8509, -0.3813, -1.1643,  0.2692, -2.0817, -1.4092,\n",
       "         -0.0249,  0.6661],\n",
       "        [ 7.7065,  1.2497, -0.7021, -1.2242,  1.4426,  3.8796, -0.4188, -0.3122,\n",
       "          1.5511, -1.3689]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.ops.aten.linear(torch.randn(10, 10), torch.randn(10, 10), torch.randn(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
