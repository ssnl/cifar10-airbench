{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import abc\n",
    "import torch.nn.modules.module\n",
    "from torch.export import export, Dim\n",
    "from torch.export.exported_program import ExportedProgram, InputKind, OutputKind\n",
    "import uuid\n",
    "import torch._dynamo\n",
    "from torch._functorch.aot_autograd import aot_module, aot_module_simplified\n",
    "from torch._dynamo.backends.common import aot_autograd\n",
    "from functorch.compile import make_boxed_func\n",
    "from torch._decomp import core_aten_decompositions\n",
    "from torch._dynamo.backends.inductor import inductor\n",
    "from torch._subclasses.fake_tensor import FakeTensorMode\n",
    "from torch._guards import detect_fake_mode\n",
    "from typing import *\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import functools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import inspect\n",
    "import contextlib\n",
    "from torch.utils._python_dispatch import TorchDispatchMode\n",
    "from torch.overrides import enable_reentrant_dispatch\n",
    "from collections import defaultdict\n",
    "from torch._subclasses.fake_tensor import FakeTensorMode\n",
    "from torch.utils._pytree import tree_map\n",
    "from typeguard import typechecked\n",
    "from multimethod import multimethod\n",
    "from torch.overrides import TorchFunctionMode, resolve_name\n",
    "from torch.utils._python_dispatch import TorchDispatchMode\n",
    "\n",
    "ENABLE_NORM_DISPATCH = True\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def disable_norm_dispatch():\n",
    "    global ENABLE_NORM_DISPATCH\n",
    "    old_flag = ENABLE_NORM_DISPATCH\n",
    "    ENABLE_NORM_DISPATCH = False\n",
    "    yield\n",
    "    ENABLE_NORM_DISPATCH = old_flag\n",
    "\n",
    "HANDLED_FUNCTIONS: Dict[Callable, Callable] = {}\n",
    "\n",
    "\n",
    "def get_output_fake_tensors(func, *args, **kwargs):\n",
    "    # Create a fake mode\n",
    "    fake_mode = FakeTensorMode(allow_non_fake_inputs=True)\n",
    "    def convert_from_real_tensor(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return fake_mode.fake_tensor_converter.from_real_tensor(fake_mode, x)\n",
    "        return x\n",
    "    # Fakeify some real tensors\n",
    "    with fake_mode, disable_norm_dispatch():\n",
    "        args = tree_map(convert_from_real_tensor, args)\n",
    "        kwargs = tree_map(convert_from_real_tensor, kwargs)\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "\n",
    "def implements(torch_function):\n",
    "    \"\"\"Register a torch function override for ScalarTensor\"\"\"\n",
    "    def decorator(func):\n",
    "        # sig = inspect.signature(func)\n",
    "        # func = typechecked(func)\n",
    "        functools.update_wrapper(func, torch_function)\n",
    "        assert torch_function not in HANDLED_FUNCTIONS\n",
    "        HANDLED_FUNCTIONS[torch_function] = func\n",
    "        return func\n",
    "    return decorator\n",
    "\n",
    "\n",
    "\n",
    "class FunctionLog(TorchFunctionMode):\n",
    "    def __torch_function__(self, func, types, args, kwargs=None):\n",
    "        if ENABLE_NORM_DISPATCH:\n",
    "            print(f\"Function Log: {func}(*{args}, **{kwargs})\")\n",
    "            if any(issubclass(t, NormTensorBase) for t in types) and func in HANDLED_FUNCTIONS:\n",
    "                return NotImplemented\n",
    "        return func(*args, **(kwargs or {}))\n",
    "\n",
    "class DispatchLog(TorchDispatchMode):\n",
    "    def __torch_dispatch__(self, func, types, args, kwargs=None):\n",
    "        if ENABLE_NORM_DISPATCH:\n",
    "            print(f\"Dispatch Log: {func}(*{args}, **{kwargs})\")\n",
    "            if any(issubclass(t, NormTensorBase) for t in types):\n",
    "                return NotImplemented\n",
    "        return func(*args, **(kwargs or {}))\n",
    "\n",
    "\n",
    "class NormTensorBase(torch.Tensor):\n",
    "    # a tensor container\n",
    "\n",
    "    @staticmethod\n",
    "    def __new__(cls, norm_size: Union[float, torch.Tensor], *, size: torch.Size,\n",
    "                dtype: torch.dtype, device: torch.device, requires_grad=None):\n",
    "        return cls._make_wrapper_subclass(cls, size, dtype=dtype, device=device, requires_grad=False)\n",
    "\n",
    "    def __init__(self, norm_size: Union[float, torch.Tensor], *, size: torch.Size,\n",
    "                 dtype: torch.dtype, device: torch.device, requires_grad=None):\n",
    "        if isinstance(norm_size, torch.Tensor):\n",
    "            assert requires_grad is None\n",
    "            self._norm_size = norm_size\n",
    "        else:\n",
    "            self._norm_size = torch.full((), norm_size, dtype=torch.float32, requires_grad=requires_grad)\n",
    "\n",
    "    @classmethod\n",
    "    def from_tensor(cls, norm_size: Union[float, torch.Tensor], tensor: torch.Tensor, *, requires_grad=None):\n",
    "        return cls(norm_size, size=tensor.size(), dtype=tensor.dtype, device=tensor.device, requires_grad=requires_grad)\n",
    "\n",
    "    @property\n",
    "    def norm_size(self) -> torch.Tensor:\n",
    "        return self._norm_size\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(norm_size={self.norm_size!r})\"\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        if ENABLE_NORM_DISPATCH and func in HANDLED_FUNCTIONS:\n",
    "            print(f\"cls Function Log: {func}(*{args}, **{kwargs})\")\n",
    "            with disable_norm_dispatch():\n",
    "                return HANDLED_FUNCTIONS[func](*args, **kwargs)\n",
    "        # for handler, sig in HANDLED_FUNCTIONS.get(func, []):\n",
    "        #     print(f\"Trying {handler}\", sig, args, kwargs)\n",
    "        #     try:\n",
    "        #         bound = sig.bind(*args, **kwargs)\n",
    "        #     except TypeError as e:\n",
    "        #         continue\n",
    "        #     with enable_reentrant_dispatch():\n",
    "        #         out = handler(*bound.args, **bound.kwargs)\n",
    "        #     print(out.norm_size.__class__)\n",
    "        #     print(out)\n",
    "        #     print(out.norm_size)\n",
    "        #     return out\n",
    "        return super().__torch_function__(func, types, args, kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        return NotImplemented\n",
    "        if ENABLE_NORM_DISPATCH and func in HANDLED_FUNCTIONS:\n",
    "            print(f\"cls Dispatch Log: {func}(*{args}, **{kwargs})\")\n",
    "            with enable_reentrant_dispatch():\n",
    "                return HANDLED_FUNCTIONS[func](*args, **kwargs)\n",
    "        # for handler, sig in HANDLED_FUNCTIONS.get(func, []):\n",
    "        #     print(f\"Trying {handler}\", sig, args, kwargs)\n",
    "        #     try:\n",
    "        #         bound = sig.bind(*args, **kwargs)\n",
    "        #     except TypeError as e:\n",
    "        #         continue\n",
    "        #     with enable_reentrant_dispatch():\n",
    "        #         out = handler(*bound.args, **bound.kwargs)\n",
    "        #     print(out.norm_size.__class__)\n",
    "        #     print(out)\n",
    "        #     print(out.norm_size)\n",
    "        #     return out\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class RMS_NormTensor(NormTensorBase):\n",
    "    pass\n",
    "\n",
    "class RMS_RMS_NormTensor(NormTensorBase):\n",
    "    pass\n",
    "\n",
    "class L1_NormTensor(NormTensorBase):\n",
    "    pass\n",
    "\n",
    "\n",
    "@implements(torch.ops.aten.linear.default)\n",
    "@implements(torch.nn.functional.linear)\n",
    "@multimethod\n",
    "def linear(input: RMS_NormTensor, weight: RMS_RMS_NormTensor, bias: Optional[RMS_NormTensor] = None) -> RMS_NormTensor:\n",
    "    final_norm_size = input.norm_size * weight.norm_size\n",
    "    if bias is not None:\n",
    "        final_norm_size += bias.norm_size\n",
    "    out_fake = get_output_fake_tensors(torch.nn.functional.linear, input, weight, bias=bias)\n",
    "    return RMS_NormTensor(final_norm_size, size=out_fake.size(), dtype=out_fake.dtype, device=out_fake.device)\n",
    "\n",
    "\n",
    "# @implements(torch.ops.aten.unsqueeze.default)\n",
    "# @multimethod\n",
    "# def unsqueeze(input: RMS_NormTensor, dim: int) -> RMS_NormTensor:\n",
    "#     out_fake = get_output_fake_tensors(torch.ops.aten.unsqueeze.default, input, dim)\n",
    "#     return RMS_NormTensor(input.norm_size, size=out_fake.size(), dtype=out_fake.dtype, device=out_fake.device)\n",
    "\n",
    "# @implements(torch.ops.aten.squeeze_.dim)\n",
    "# @multimethod\n",
    "# def squeeze(input: RMS_NormTensor, dim: int) -> RMS_NormTensor:\n",
    "#     out_fake = get_output_fake_tensors(torch.ops.aten.squeeze_.dim, input, dim)\n",
    "#     return RMS_NormTensor(input.norm_size, size=out_fake.size(), dtype=out_fake.dtype, device=out_fake.device)\n",
    "\n",
    "# @implements(torch.ops.aten.t.default)\n",
    "# @multimethod\n",
    "# def t(input: RMS_RMS_NormTensor) -> RMS_RMS_NormTensor:\n",
    "#     assert input.ndim == 2\n",
    "#     return RMS_RMS_NormTensor(input.norm_size, size=input.size()[::-1], dtype=input.dtype, device=input.device)\n",
    "\n",
    "# # @t.register\n",
    "# # def t(input: RMSNormTensor) -> RMSRMSNormTensor:\n",
    "# #     assert input.ndim == 2\n",
    "# #     print(input.size(), 'l')\n",
    "# #     return RMSRMSNormTensor(input.norm_size + 2, size=input.size()[::-1], dtype=input.dtype, device=input.device)\n",
    "\n",
    "# @implements(torch.ops.aten.addmm.default)\n",
    "# @multimethod\n",
    "# def addmm(input: RMS_NormTensor, mat1: RMS_NormTensor, mat2: RMS_RMS_NormTensor, *, beta: float = 1, alpha: float = 1) -> RMS_NormTensor:\n",
    "#     # output = input * beta + mat1 @ mat2 * alpha\n",
    "#     final_norm_size = input.norm_size * beta + mat1.norm_size * mat2.norm_size * alpha\n",
    "#     out_fake = get_output_fake_tensors(torch.ops.aten.addmm.default, input, mat1, mat2, beta=beta, alpha=alpha)\n",
    "#     return RMS_NormTensor(final_norm_size, size=out_fake.size(), dtype=out_fake.dtype, device=out_fake.device)\n",
    "\n",
    "# @addmm.register\n",
    "# def _(input: RMS_NormTensor, mat1: RMS_NormTensor, mat2: RMS_RMS_NormTensor, *, beta: float = 1, alpha: float = 1) -> RMS_NormTensor:\n",
    "#     # output = input * beta + mat1 @ mat2 * alpha\n",
    "#     final_norm_size = input.norm_size * beta + mat1.norm_size * mat2.norm_size * alpha\n",
    "#     out_fake = get_output_fake_tensors(torch.ops.aten.addmm.default, input, mat1, mat2, beta=beta, alpha=alpha)\n",
    "#     return RMS_NormTensor(final_norm_size, size=out_fake.size(), dtype=out_fake.dtype, device=out_fake.device)\n",
    "\n",
    "\n",
    "# @implements(torch.ops.aten.mm.default)\n",
    "# @multimethod\n",
    "# def mm(input: RMS_NormTensor, mat1: RMS_RMS_NormTensor) -> RMS_NormTensor:\n",
    "#     final_norm_size = input.norm_size * mat1.norm_size\n",
    "#     out_fake = get_output_fake_tensors(torch.ops.aten.mm.default, input, mat1)\n",
    "#     return RMS_NormTensor(final_norm_size, size=out_fake.size(), dtype=out_fake.dtype, device=out_fake.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name         target               args                             kwargs\n",
      "-------------  -----------  -------------------  -------------------------------  --------\n",
      "placeholder    p_fn_weight  p_fn_weight          ()                               {}\n",
      "placeholder    p_fn_bias    p_fn_bias            ()                               {}\n",
      "placeholder    input        input                ()                               {}\n",
      "call_function  linear       aten.linear.default  (input, p_fn_weight, p_fn_bias)  {}\n",
      "output         output       output               ((linear,),)                     {}\n"
     ]
    }
   ],
   "source": [
    "torch.export.export(\n",
    "    nn.Linear(15, 16),\n",
    "    (torch.randn(2, 15, requires_grad=True),)\n",
    ").graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.__version__ > '2.5.1':\n",
    "    def default_decompositions():\n",
    "        return torch.export.default_decompositions()\n",
    "else:\n",
    "    from torch._decomp import core_aten_decompositions\n",
    "    def default_decompositions():\n",
    "        return core_aten_decompositions()\n",
    "\n",
    "\n",
    "decomps = default_decompositions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aten.upsample_linear1d.default\n",
      "<function upsample_linear1d at 0x13074a520>\n",
      "aten.upsample_linear1d.out\n",
      "<function upsample_linear1d at 0x13074a520>\n",
      "aten.upsample_bilinear2d.vec\n",
      "<function _upsample_linear_vec at 0x13074a5c0>\n",
      "aten.upsample_bilinear2d.default\n",
      "<function upsample_bilinear2d at 0x13074a8e0>\n",
      "aten.upsample_bilinear2d.out\n",
      "<function upsample_bilinear2d at 0x13074a8e0>\n",
      "aten.upsample_trilinear3d.vec\n",
      "<function _upsample_linear_vec at 0x13074a5c0>\n",
      "aten.upsample_trilinear3d.default\n",
      "<function upsample_trilinear3d at 0x13074aac0>\n",
      "aten.upsample_trilinear3d.out\n",
      "<function upsample_trilinear3d at 0x13074aac0>\n"
     ]
    }
   ],
   "source": [
    "for op_overload, decomp in decomps.items():\n",
    "    if 'linear' in str(op_overload):\n",
    "        print(op_overload)\n",
    "        print(decomp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'addcdiv'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(decomps.keys()))._opname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_can_decompose',\n",
       " '_defined_in_python',\n",
       " '_dispatch_cache',\n",
       " '_get_dispatch',\n",
       " '_handle',\n",
       " '_lazy_handle',\n",
       " '_name',\n",
       " '_namespace',\n",
       " '_op',\n",
       " '_op_dk',\n",
       " '_opname',\n",
       " '_overloadname',\n",
       " '_overloadpacket',\n",
       " '_schema',\n",
       " '_tags',\n",
       " '_uncache_dispatch',\n",
       " 'decompose',\n",
       " 'functorch_table',\n",
       " 'has_kernel_for_any_dispatch_key',\n",
       " 'has_kernel_for_dispatch_key',\n",
       " 'is_view',\n",
       " 'name',\n",
       " 'namespace',\n",
       " 'op',\n",
       " 'overloadpacket',\n",
       " 'py_functionalize_impl',\n",
       " 'py_impl',\n",
       " 'py_kernels',\n",
       " 'python_key_table',\n",
       " 'redispatch',\n",
       " 'tags']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(next(iter(decomps.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "\n",
    "ep = torch.export.export(\n",
    "    nn.Linear(15, 16),\n",
    "    (torch.randn(2, 15, requires_grad=True),)\n",
    ")\n",
    "\n",
    "# ep = ep.run_decompositions()\n",
    "\n",
    "gm = ep.module()\n",
    "\n",
    "# gm = aot_module(gm, (torch.randn(2, 15, requires_grad=True),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule()"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, input):\n",
      "    input, = fx_pytree.tree_flatten_spec(([input], {}), self._in_spec)\n",
      "    weight = self.weight\n",
      "    bias = self.bias\n",
      "    input_1 = input\n",
      "    linear = torch.ops.aten.linear.default(input_1, weight, bias);  input_1 = weight = bias = None\n",
      "    return pytree.tree_unflatten((linear,), self._out_spec)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "print(gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Log: <method 'size' of 'torch._C.TensorBase' objects>(*(Parameter containing:\n",
      "tensor([[ 2.5037e-01, -2.3509e-01,  6.9403e-02,  1.5359e-01,  2.1446e-01,\n",
      "          5.0863e-02, -1.3417e-01,  2.2040e-01, -3.1974e-02, -1.8737e-01,\n",
      "         -6.5651e-02,  2.3970e-01,  2.1037e-01,  8.7696e-02, -3.6649e-02],\n",
      "        [-1.6843e-01,  2.1243e-01, -2.1920e-01,  8.5752e-02, -2.2146e-01,\n",
      "          2.3418e-01,  1.7371e-01, -2.2969e-01,  1.9210e-02, -3.2548e-02,\n",
      "          1.6985e-01, -5.8576e-02, -9.1809e-02, -2.4581e-01,  1.3280e-01],\n",
      "        [ 1.4920e-01, -4.6759e-02, -2.2853e-02,  1.2846e-01,  1.4195e-01,\n",
      "         -8.8691e-02, -6.5738e-02, -1.0860e-01, -7.6861e-02,  8.1323e-02,\n",
      "         -2.2642e-01,  8.6144e-02, -1.0688e-01,  1.4019e-01,  1.7083e-02],\n",
      "        [ 1.6495e-01,  2.1578e-01,  1.8354e-01,  2.5663e-01,  1.5602e-01,\n",
      "          1.6883e-01,  7.8164e-02,  1.2637e-01,  7.6798e-02,  9.3937e-02,\n",
      "         -6.5100e-02,  4.1794e-02, -1.6774e-01, -2.1921e-01,  2.3303e-01],\n",
      "        [-3.3647e-02,  7.1409e-03, -2.3209e-01,  6.0832e-02,  1.0686e-01,\n",
      "          6.4775e-02,  5.8640e-02, -2.0078e-01,  1.8785e-01,  2.1656e-01,\n",
      "          7.0221e-02,  2.8086e-02, -2.1000e-01,  1.9074e-01, -1.8811e-01],\n",
      "        [-1.8118e-01,  8.5744e-02,  1.6110e-01, -6.5567e-02, -1.3153e-01,\n",
      "         -1.1017e-01,  7.7430e-02, -6.1043e-02, -9.7026e-02, -2.1099e-01,\n",
      "         -1.8124e-01, -1.1203e-01, -2.1991e-01, -2.1409e-01,  2.1444e-01],\n",
      "        [ 9.3355e-02,  2.0404e-01,  2.4695e-01, -8.2530e-02,  7.1842e-03,\n",
      "         -1.6390e-01,  9.9441e-02,  2.0966e-01,  2.3532e-01,  1.0550e-01,\n",
      "          1.8343e-01,  1.6935e-01, -1.5907e-01,  1.1382e-04, -1.5775e-02],\n",
      "        [-2.0223e-01, -1.7808e-01, -1.9969e-02, -8.6500e-02, -4.1941e-02,\n",
      "          1.9889e-01,  2.0238e-02,  1.1266e-01,  3.3219e-02,  8.7555e-02,\n",
      "         -6.1654e-02,  3.7336e-05,  1.1238e-01,  1.6802e-01, -1.6763e-01],\n",
      "        [ 1.5994e-01, -2.0544e-01, -9.5396e-02, -2.0850e-01, -1.7030e-02,\n",
      "          1.2458e-01, -1.7764e-01,  5.9321e-02, -1.3039e-01,  2.1672e-01,\n",
      "          1.3721e-01,  6.3015e-02,  1.9828e-01,  5.7262e-02, -1.7078e-01],\n",
      "        [-2.1254e-01,  2.5228e-01,  1.9220e-01, -2.3268e-01,  1.4102e-01,\n",
      "         -1.3068e-01,  1.3655e-01,  9.8393e-02,  2.0189e-01, -2.2676e-01,\n",
      "          2.1754e-01,  2.4026e-01,  1.5296e-01, -9.3619e-02, -2.1871e-01],\n",
      "        [-2.2090e-01, -6.3834e-02,  6.0726e-02,  3.3254e-02,  2.1663e-01,\n",
      "         -2.2778e-01,  1.4975e-01,  1.4552e-01,  1.8184e-01, -5.7763e-02,\n",
      "          7.4233e-02, -1.1054e-01, -2.3942e-01, -3.2384e-02, -8.0311e-02],\n",
      "        [ 1.5356e-01,  2.1501e-01, -6.1557e-03,  2.0728e-01,  8.9547e-02,\n",
      "          7.5263e-02,  1.9113e-01,  5.1504e-02,  5.1861e-03, -1.9099e-01,\n",
      "          1.0978e-01,  1.0049e-01,  3.2063e-02, -1.1929e-01, -8.8777e-02],\n",
      "        [ 1.7460e-01, -1.0354e-01, -1.1722e-01,  1.6641e-01, -3.7035e-02,\n",
      "         -1.3463e-01, -1.7752e-02, -2.1752e-01,  7.3834e-02,  2.5306e-01,\n",
      "         -3.2046e-02, -7.8537e-02, -2.2420e-01,  1.9464e-01,  1.9729e-01],\n",
      "        [-2.0962e-01, -1.8654e-02,  2.5114e-01,  4.6222e-02,  5.9695e-02,\n",
      "          5.2022e-02,  4.0997e-02,  3.0311e-02,  1.6443e-01, -2.3179e-01,\n",
      "         -1.2687e-01, -2.2793e-01, -2.8400e-02,  2.4817e-01, -1.3321e-01],\n",
      "        [ 1.5006e-01, -1.3045e-02, -6.0170e-02, -1.5266e-01, -1.2434e-03,\n",
      "          1.6462e-01,  2.4565e-01, -2.3929e-01, -1.7770e-02,  1.8976e-01,\n",
      "         -8.7978e-02,  1.8230e-01, -2.2593e-01,  1.6819e-01, -9.9225e-02],\n",
      "        [ 1.8456e-01,  1.8851e-01,  9.4678e-02,  9.6644e-02,  2.2232e-01,\n",
      "          1.2750e-01,  1.0078e-01, -2.5603e-01,  1.6254e-01,  2.0000e-01,\n",
      "          1.7139e-01,  1.4915e-01, -1.3834e-01, -1.8881e-01, -2.5528e-01]],\n",
      "       requires_grad=True),), **None)\n",
      "Function Log: <method-wrapper '__get__' of getset_descriptor object at 0x111025940>(*(Parameter containing:\n",
      "tensor([[ 2.5037e-01, -2.3509e-01,  6.9403e-02,  1.5359e-01,  2.1446e-01,\n",
      "          5.0863e-02, -1.3417e-01,  2.2040e-01, -3.1974e-02, -1.8737e-01,\n",
      "         -6.5651e-02,  2.3970e-01,  2.1037e-01,  8.7696e-02, -3.6649e-02],\n",
      "        [-1.6843e-01,  2.1243e-01, -2.1920e-01,  8.5752e-02, -2.2146e-01,\n",
      "          2.3418e-01,  1.7371e-01, -2.2969e-01,  1.9210e-02, -3.2548e-02,\n",
      "          1.6985e-01, -5.8576e-02, -9.1809e-02, -2.4581e-01,  1.3280e-01],\n",
      "        [ 1.4920e-01, -4.6759e-02, -2.2853e-02,  1.2846e-01,  1.4195e-01,\n",
      "         -8.8691e-02, -6.5738e-02, -1.0860e-01, -7.6861e-02,  8.1323e-02,\n",
      "         -2.2642e-01,  8.6144e-02, -1.0688e-01,  1.4019e-01,  1.7083e-02],\n",
      "        [ 1.6495e-01,  2.1578e-01,  1.8354e-01,  2.5663e-01,  1.5602e-01,\n",
      "          1.6883e-01,  7.8164e-02,  1.2637e-01,  7.6798e-02,  9.3937e-02,\n",
      "         -6.5100e-02,  4.1794e-02, -1.6774e-01, -2.1921e-01,  2.3303e-01],\n",
      "        [-3.3647e-02,  7.1409e-03, -2.3209e-01,  6.0832e-02,  1.0686e-01,\n",
      "          6.4775e-02,  5.8640e-02, -2.0078e-01,  1.8785e-01,  2.1656e-01,\n",
      "          7.0221e-02,  2.8086e-02, -2.1000e-01,  1.9074e-01, -1.8811e-01],\n",
      "        [-1.8118e-01,  8.5744e-02,  1.6110e-01, -6.5567e-02, -1.3153e-01,\n",
      "         -1.1017e-01,  7.7430e-02, -6.1043e-02, -9.7026e-02, -2.1099e-01,\n",
      "         -1.8124e-01, -1.1203e-01, -2.1991e-01, -2.1409e-01,  2.1444e-01],\n",
      "        [ 9.3355e-02,  2.0404e-01,  2.4695e-01, -8.2530e-02,  7.1842e-03,\n",
      "         -1.6390e-01,  9.9441e-02,  2.0966e-01,  2.3532e-01,  1.0550e-01,\n",
      "          1.8343e-01,  1.6935e-01, -1.5907e-01,  1.1382e-04, -1.5775e-02],\n",
      "        [-2.0223e-01, -1.7808e-01, -1.9969e-02, -8.6500e-02, -4.1941e-02,\n",
      "          1.9889e-01,  2.0238e-02,  1.1266e-01,  3.3219e-02,  8.7555e-02,\n",
      "         -6.1654e-02,  3.7336e-05,  1.1238e-01,  1.6802e-01, -1.6763e-01],\n",
      "        [ 1.5994e-01, -2.0544e-01, -9.5396e-02, -2.0850e-01, -1.7030e-02,\n",
      "          1.2458e-01, -1.7764e-01,  5.9321e-02, -1.3039e-01,  2.1672e-01,\n",
      "          1.3721e-01,  6.3015e-02,  1.9828e-01,  5.7262e-02, -1.7078e-01],\n",
      "        [-2.1254e-01,  2.5228e-01,  1.9220e-01, -2.3268e-01,  1.4102e-01,\n",
      "         -1.3068e-01,  1.3655e-01,  9.8393e-02,  2.0189e-01, -2.2676e-01,\n",
      "          2.1754e-01,  2.4026e-01,  1.5296e-01, -9.3619e-02, -2.1871e-01],\n",
      "        [-2.2090e-01, -6.3834e-02,  6.0726e-02,  3.3254e-02,  2.1663e-01,\n",
      "         -2.2778e-01,  1.4975e-01,  1.4552e-01,  1.8184e-01, -5.7763e-02,\n",
      "          7.4233e-02, -1.1054e-01, -2.3942e-01, -3.2384e-02, -8.0311e-02],\n",
      "        [ 1.5356e-01,  2.1501e-01, -6.1557e-03,  2.0728e-01,  8.9547e-02,\n",
      "          7.5263e-02,  1.9113e-01,  5.1504e-02,  5.1861e-03, -1.9099e-01,\n",
      "          1.0978e-01,  1.0049e-01,  3.2063e-02, -1.1929e-01, -8.8777e-02],\n",
      "        [ 1.7460e-01, -1.0354e-01, -1.1722e-01,  1.6641e-01, -3.7035e-02,\n",
      "         -1.3463e-01, -1.7752e-02, -2.1752e-01,  7.3834e-02,  2.5306e-01,\n",
      "         -3.2046e-02, -7.8537e-02, -2.2420e-01,  1.9464e-01,  1.9729e-01],\n",
      "        [-2.0962e-01, -1.8654e-02,  2.5114e-01,  4.6222e-02,  5.9695e-02,\n",
      "          5.2022e-02,  4.0997e-02,  3.0311e-02,  1.6443e-01, -2.3179e-01,\n",
      "         -1.2687e-01, -2.2793e-01, -2.8400e-02,  2.4817e-01, -1.3321e-01],\n",
      "        [ 1.5006e-01, -1.3045e-02, -6.0170e-02, -1.5266e-01, -1.2434e-03,\n",
      "          1.6462e-01,  2.4565e-01, -2.3929e-01, -1.7770e-02,  1.8976e-01,\n",
      "         -8.7978e-02,  1.8230e-01, -2.2593e-01,  1.6819e-01, -9.9225e-02],\n",
      "        [ 1.8456e-01,  1.8851e-01,  9.4678e-02,  9.6644e-02,  2.2232e-01,\n",
      "          1.2750e-01,  1.0078e-01, -2.5603e-01,  1.6254e-01,  2.0000e-01,\n",
      "          1.7139e-01,  1.4915e-01, -1.3834e-01, -1.8881e-01, -2.5528e-01]],\n",
      "       requires_grad=True),), **None)\n",
      "Function Log: <method-wrapper '__get__' of getset_descriptor object at 0x1110259c0>(*(Parameter containing:\n",
      "tensor([[ 2.5037e-01, -2.3509e-01,  6.9403e-02,  1.5359e-01,  2.1446e-01,\n",
      "          5.0863e-02, -1.3417e-01,  2.2040e-01, -3.1974e-02, -1.8737e-01,\n",
      "         -6.5651e-02,  2.3970e-01,  2.1037e-01,  8.7696e-02, -3.6649e-02],\n",
      "        [-1.6843e-01,  2.1243e-01, -2.1920e-01,  8.5752e-02, -2.2146e-01,\n",
      "          2.3418e-01,  1.7371e-01, -2.2969e-01,  1.9210e-02, -3.2548e-02,\n",
      "          1.6985e-01, -5.8576e-02, -9.1809e-02, -2.4581e-01,  1.3280e-01],\n",
      "        [ 1.4920e-01, -4.6759e-02, -2.2853e-02,  1.2846e-01,  1.4195e-01,\n",
      "         -8.8691e-02, -6.5738e-02, -1.0860e-01, -7.6861e-02,  8.1323e-02,\n",
      "         -2.2642e-01,  8.6144e-02, -1.0688e-01,  1.4019e-01,  1.7083e-02],\n",
      "        [ 1.6495e-01,  2.1578e-01,  1.8354e-01,  2.5663e-01,  1.5602e-01,\n",
      "          1.6883e-01,  7.8164e-02,  1.2637e-01,  7.6798e-02,  9.3937e-02,\n",
      "         -6.5100e-02,  4.1794e-02, -1.6774e-01, -2.1921e-01,  2.3303e-01],\n",
      "        [-3.3647e-02,  7.1409e-03, -2.3209e-01,  6.0832e-02,  1.0686e-01,\n",
      "          6.4775e-02,  5.8640e-02, -2.0078e-01,  1.8785e-01,  2.1656e-01,\n",
      "          7.0221e-02,  2.8086e-02, -2.1000e-01,  1.9074e-01, -1.8811e-01],\n",
      "        [-1.8118e-01,  8.5744e-02,  1.6110e-01, -6.5567e-02, -1.3153e-01,\n",
      "         -1.1017e-01,  7.7430e-02, -6.1043e-02, -9.7026e-02, -2.1099e-01,\n",
      "         -1.8124e-01, -1.1203e-01, -2.1991e-01, -2.1409e-01,  2.1444e-01],\n",
      "        [ 9.3355e-02,  2.0404e-01,  2.4695e-01, -8.2530e-02,  7.1842e-03,\n",
      "         -1.6390e-01,  9.9441e-02,  2.0966e-01,  2.3532e-01,  1.0550e-01,\n",
      "          1.8343e-01,  1.6935e-01, -1.5907e-01,  1.1382e-04, -1.5775e-02],\n",
      "        [-2.0223e-01, -1.7808e-01, -1.9969e-02, -8.6500e-02, -4.1941e-02,\n",
      "          1.9889e-01,  2.0238e-02,  1.1266e-01,  3.3219e-02,  8.7555e-02,\n",
      "         -6.1654e-02,  3.7336e-05,  1.1238e-01,  1.6802e-01, -1.6763e-01],\n",
      "        [ 1.5994e-01, -2.0544e-01, -9.5396e-02, -2.0850e-01, -1.7030e-02,\n",
      "          1.2458e-01, -1.7764e-01,  5.9321e-02, -1.3039e-01,  2.1672e-01,\n",
      "          1.3721e-01,  6.3015e-02,  1.9828e-01,  5.7262e-02, -1.7078e-01],\n",
      "        [-2.1254e-01,  2.5228e-01,  1.9220e-01, -2.3268e-01,  1.4102e-01,\n",
      "         -1.3068e-01,  1.3655e-01,  9.8393e-02,  2.0189e-01, -2.2676e-01,\n",
      "          2.1754e-01,  2.4026e-01,  1.5296e-01, -9.3619e-02, -2.1871e-01],\n",
      "        [-2.2090e-01, -6.3834e-02,  6.0726e-02,  3.3254e-02,  2.1663e-01,\n",
      "         -2.2778e-01,  1.4975e-01,  1.4552e-01,  1.8184e-01, -5.7763e-02,\n",
      "          7.4233e-02, -1.1054e-01, -2.3942e-01, -3.2384e-02, -8.0311e-02],\n",
      "        [ 1.5356e-01,  2.1501e-01, -6.1557e-03,  2.0728e-01,  8.9547e-02,\n",
      "          7.5263e-02,  1.9113e-01,  5.1504e-02,  5.1861e-03, -1.9099e-01,\n",
      "          1.0978e-01,  1.0049e-01,  3.2063e-02, -1.1929e-01, -8.8777e-02],\n",
      "        [ 1.7460e-01, -1.0354e-01, -1.1722e-01,  1.6641e-01, -3.7035e-02,\n",
      "         -1.3463e-01, -1.7752e-02, -2.1752e-01,  7.3834e-02,  2.5306e-01,\n",
      "         -3.2046e-02, -7.8537e-02, -2.2420e-01,  1.9464e-01,  1.9729e-01],\n",
      "        [-2.0962e-01, -1.8654e-02,  2.5114e-01,  4.6222e-02,  5.9695e-02,\n",
      "          5.2022e-02,  4.0997e-02,  3.0311e-02,  1.6443e-01, -2.3179e-01,\n",
      "         -1.2687e-01, -2.2793e-01, -2.8400e-02,  2.4817e-01, -1.3321e-01],\n",
      "        [ 1.5006e-01, -1.3045e-02, -6.0170e-02, -1.5266e-01, -1.2434e-03,\n",
      "          1.6462e-01,  2.4565e-01, -2.3929e-01, -1.7770e-02,  1.8976e-01,\n",
      "         -8.7978e-02,  1.8230e-01, -2.2593e-01,  1.6819e-01, -9.9225e-02],\n",
      "        [ 1.8456e-01,  1.8851e-01,  9.4678e-02,  9.6644e-02,  2.2232e-01,\n",
      "          1.2750e-01,  1.0078e-01, -2.5603e-01,  1.6254e-01,  2.0000e-01,\n",
      "          1.7139e-01,  1.4915e-01, -1.3834e-01, -1.8881e-01, -2.5528e-01]],\n",
      "       requires_grad=True),), **None)\n",
      "Function Log: <built-in method full of type object at 0x124604008>(*((), 4), **{'dtype': torch.float32, 'requires_grad': True})\n",
      "Function Log: <method 'size' of 'torch._C.TensorBase' objects>(*(Parameter containing:\n",
      "tensor([-0.0514, -0.0972, -0.2225,  0.2108,  0.1205, -0.1807,  0.2533, -0.1265,\n",
      "         0.1779,  0.1766,  0.1095,  0.1713,  0.0340,  0.0789,  0.0867,  0.1194],\n",
      "       requires_grad=True),), **None)\n",
      "Function Log: <method-wrapper '__get__' of getset_descriptor object at 0x111025940>(*(Parameter containing:\n",
      "tensor([-0.0514, -0.0972, -0.2225,  0.2108,  0.1205, -0.1807,  0.2533, -0.1265,\n",
      "         0.1779,  0.1766,  0.1095,  0.1713,  0.0340,  0.0789,  0.0867,  0.1194],\n",
      "       requires_grad=True),), **None)\n",
      "Function Log: <method-wrapper '__get__' of getset_descriptor object at 0x1110259c0>(*(Parameter containing:\n",
      "tensor([-0.0514, -0.0972, -0.2225,  0.2108,  0.1205, -0.1807,  0.2533, -0.1265,\n",
      "         0.1779,  0.1766,  0.1095,  0.1713,  0.0340,  0.0789,  0.0867,  0.1194],\n",
      "       requires_grad=True),), **None)\n",
      "Function Log: <built-in method full of type object at 0x124604008>(*((), 5), **{'dtype': torch.float32, 'requires_grad': True})\n",
      "Function Log: <built-in method randn of type object at 0x124604008>(*(2, 15), **{'requires_grad': True})\n",
      "Function Log: <method 'size' of 'torch._C.TensorBase' objects>(*(tensor([[ 1.1812,  0.5787,  0.3751,  0.4725,  0.0950, -0.4775, -0.4634,  0.0400,\n",
      "          0.7263,  0.2787,  0.7421, -0.3201,  0.1069,  0.5789, -1.1438],\n",
      "        [ 0.4109,  0.8136, -0.6921, -1.7455, -0.0226, -1.6567, -0.3646, -0.4124,\n",
      "          0.2064,  0.3814, -0.5082, -0.0092, -1.1174, -0.4950,  0.0687]],\n",
      "       requires_grad=True),), **None)\n",
      "Function Log: <method-wrapper '__get__' of getset_descriptor object at 0x111025940>(*(tensor([[ 1.1812,  0.5787,  0.3751,  0.4725,  0.0950, -0.4775, -0.4634,  0.0400,\n",
      "          0.7263,  0.2787,  0.7421, -0.3201,  0.1069,  0.5789, -1.1438],\n",
      "        [ 0.4109,  0.8136, -0.6921, -1.7455, -0.0226, -1.6567, -0.3646, -0.4124,\n",
      "          0.2064,  0.3814, -0.5082, -0.0092, -1.1174, -0.4950,  0.0687]],\n",
      "       requires_grad=True),), **None)\n",
      "Function Log: <method-wrapper '__get__' of getset_descriptor object at 0x1110259c0>(*(tensor([[ 1.1812,  0.5787,  0.3751,  0.4725,  0.0950, -0.4775, -0.4634,  0.0400,\n",
      "          0.7263,  0.2787,  0.7421, -0.3201,  0.1069,  0.5789, -1.1438],\n",
      "        [ 0.4109,  0.8136, -0.6921, -1.7455, -0.0226, -1.6567, -0.3646, -0.4124,\n",
      "          0.2064,  0.3814, -0.5082, -0.0092, -1.1174, -0.4950,  0.0687]],\n",
      "       requires_grad=True),), **None)\n",
      "Function Log: <built-in method full of type object at 0x124604008>(*((), 3), **{'dtype': torch.float32, 'requires_grad': None})\n",
      "Function Log: <method-wrapper '__get__' of getset_descriptor object at 0x111025200>(*(FakeTensor(..., size=(2, 15), requires_grad=True),), **None)\n",
      "Function Log: <method-wrapper '__get__' of getset_descriptor object at 0x111025200>(*(RMS_NormTensor(norm_size=tensor(3.)),), **None)\n",
      "Function Log: <method-wrapper '__get__' of getset_descriptor object at 0x111025200>(*(RMS_NormTensor(norm_size=tensor(3.)),), **None)\n",
      "Function Log: <method-wrapper '__get__' of getset_descriptor object at 0x111025200>(*(FakeTensor(..., size=(2, 15), requires_grad=True),), **None)\n",
      "Function Log: aten.linear.default(*(RMS_NormTensor(norm_size=tensor(3.)), RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True)), RMS_NormTensor(norm_size=tensor(5., requires_grad=True))), **{})\n",
      "Function Log: <function Tensor.__repr__ at 0x122611bc0>(*(tensor(3.),), **{'tensor_contents': None})\n",
      "Function Log: <function Tensor.__repr__ at 0x122611bc0>(*(tensor(4., requires_grad=True),), **{'tensor_contents': None})\n",
      "Function Log: <function Tensor.__repr__ at 0x122611bc0>(*(tensor(5., requires_grad=True),), **{'tensor_contents': None})\n",
      "cls Function Log: aten.linear.default(*(RMS_NormTensor(norm_size=tensor(3.)), RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True)), RMS_NormTensor(norm_size=tensor(5., requires_grad=True))), **{})\n",
      "Function Log: <function Tensor.__repr__ at 0x122611bc0>(*(tensor(17., grad_fn=<AddBackward0>),), **{'tensor_contents': None})\n",
      "RMS_NormTensor(norm_size=tensor(17., grad_fn=<AddBackward0>)) Function Log: <function Tensor.__repr__ at 0x122611bc0>(*(tensor(17., grad_fn=<AddBackward0>),), **{'tensor_contents': None})\n",
      "tensor(17., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with FunctionLog():\n",
    "    object.__setattr__(gm, 'weight', RMS_RMS_NormTensor.from_tensor(4, gm.weight, requires_grad=True))\n",
    "    object.__setattr__(gm, 'bias', RMS_NormTensor.from_tensor(5, gm.bias, requires_grad=True))\n",
    "    y = gm(\n",
    "        RMS_NormTensor.from_tensor(3, torch.randn(2, 15, requires_grad=True)),\n",
    "    )\n",
    "    print(y, y.norm_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(1.))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(y.norm_size, [gm.weight.norm_size, gm.bias.norm_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Log: <class 'torch.device'>(*('cpu',), **None)\n",
      "Function Log: <built-in method full of type object at 0x124604008>(*((), 3), **{'dtype': torch.float32, 'requires_grad': True})\n",
      "Function Log: <class 'torch.device'>(*('cpu',), **None)\n",
      "Function Log: <built-in method full of type object at 0x124604008>(*((), 4), **{'dtype': torch.float32, 'requires_grad': True})\n",
      "Function Log: <class 'torch.device'>(*('cpu',), **None)\n",
      "Function Log: <built-in method full of type object at 0x124604008>(*((), 5), **{'dtype': torch.float32, 'requires_grad': True})\n",
      "Function Log: <built-in function linear>(*(RMS_NormTensor(norm_size=tensor(3., requires_grad=True)), RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True)), RMS_NormTensor(norm_size=tensor(5., requires_grad=True))), **None)\n",
      "Function Log: <function Tensor.__repr__ at 0x122611bc0>(*(tensor(3., requires_grad=True),), **{'tensor_contents': None})\n",
      "Function Log: <function Tensor.__repr__ at 0x122611bc0>(*(tensor(4., requires_grad=True),), **{'tensor_contents': None})\n",
      "Function Log: <function Tensor.__repr__ at 0x122611bc0>(*(tensor(5., requires_grad=True),), **{'tensor_contents': None})\n",
      "cls Function Log: <built-in function linear>(*(RMS_NormTensor(norm_size=tensor(3., requires_grad=True)), RMS_RMS_NormTensor(norm_size=tensor(4., requires_grad=True)), RMS_NormTensor(norm_size=tensor(5., requires_grad=True))), **{})\n",
      "Function Log: <function Tensor.__repr__ at 0x122611bc0>(*(tensor(17., grad_fn=<AddBackward0>),), **{'tensor_contents': None})\n",
      "RMS_NormTensor(norm_size=tensor(17., grad_fn=<AddBackward0>)) Function Log: <function Tensor.__repr__ at 0x122611bc0>(*(tensor(17., grad_fn=<AddBackward0>),), **{'tensor_contents': None})\n",
      "tensor(17., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with FunctionLog():\n",
    "    # torch.randn(1, 15, requires_grad=True)\n",
    "    y = torch.nn.functional.linear(\n",
    "        RMS_NormTensor(3, size=(1, 15), dtype=torch.float32, device=torch.device(\"cpu\"), requires_grad=True),\n",
    "        RMS_RMS_NormTensor(4, size=(16, 15), dtype=torch.float32, device=torch.device(\"cpu\"), requires_grad=True),\n",
    "        RMS_NormTensor(5, size=(16,), dtype=torch.float32, device=torch.device(\"cpu\"), requires_grad=True)\n",
    "    )\n",
    "    print(y, y.norm_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: aten.add.Tensor(*(RMSNormTensor(norm_size=3, requires_grad=False), RMSRMSNormTensor(norm_size=4, requires_grad=False)), **{})\n",
      "tensor(7., grad_fn=<AddBackward0>)\n",
      "z: RMSNormTensor(norm_size=7, requires_grad=False) tensor(7., grad_fn=<AddBackward0>) [tensor(3., requires_grad=True), tensor(4.)] tensor(7., grad_fn=<AddBackward0>)\n",
      "RMSNormTensor(norm_size=7, requires_grad=False) tensor(7., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = (x:=RMSNormTensor(3, requires_grad=True)) + RMSRMSNormTensor(4)\n",
    "print(y, y.norm_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(y\u001b[38;5;241m.\u001b[39mnorm_size, [x\u001b[38;5;241m.\u001b[39mnorm_size])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m _engine_run_backward(\n\u001b[1;32m    497\u001b[0m         outputs,\n\u001b[1;32m    498\u001b[0m         grad_outputs_,\n\u001b[1;32m    499\u001b[0m         retain_graph,\n\u001b[1;32m    500\u001b[0m         create_graph,\n\u001b[1;32m    501\u001b[0m         inputs,\n\u001b[1;32m    502\u001b[0m         allow_unused,\n\u001b[1;32m    503\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(y.norm_size, [x.norm_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch Log: aten.add.Tensor(*(RMSNormTensor(norm_size=3.0), RMSRMSNormTensor(norm_size=4.0)), **{})\n",
      "RMSNormTensor(norm_size=7.0) tensor(7.)\n",
      "Dispatch Log: aten.ones_like.default(*(RMSNormTensor(norm_size=7.0),), **{'pin_memory': False, 'memory_format': torch.preserve_format})\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m y \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m:=\u001b[39mRMSNormTensor(\u001b[38;5;241m3\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)) \u001b[38;5;241m+\u001b[39m RMSRMSNormTensor(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(y, y\u001b[38;5;241m.\u001b[39mnorm_size)\n\u001b[0;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward([y])\n\u001b[1;32m      4\u001b[0m y, x\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:220\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    219\u001b[0m         new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 220\u001b[0m             torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[71], line 55\u001b[0m, in \u001b[0;36mNormTensorBase.__torch_dispatch__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39madd\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mnorm_size, args)))\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ENABLE_NORM_DISPATCH:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "y = (x:=RMSNormTensor(3, requires_grad=True)) + RMSRMSNormTensor(4)\n",
    "print(y, y.norm_size)\n",
    "torch.autograd.backward([y])\n",
    "y, x.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y%debu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9366, -2.1385,  0.2499,  0.1769,  3.3125,  0.5611,  0.2110, -0.9544,\n",
       "         -1.4054, -1.0516],\n",
       "        [-1.3193,  2.8552, -2.4230,  2.6803,  4.9918,  3.8054, -1.6162, -3.6952,\n",
       "         -3.6532, -1.4474],\n",
       "        [-1.9989,  3.0092, -8.1097,  0.5183,  2.0837, -0.9222,  0.3195, -5.3470,\n",
       "         -3.0957,  2.3705],\n",
       "        [ 5.2813,  2.8777,  0.0169, -0.6770, -3.8537,  2.0995, -3.8720, -1.3664,\n",
       "         -2.2780,  2.0936],\n",
       "        [-0.3890, -0.7412,  2.1590,  0.5803, -0.2978,  0.4573, -0.6891,  0.1847,\n",
       "          0.4717, -1.0158],\n",
       "        [-4.2937,  4.5814,  0.6280,  0.9427, -4.7592, -3.3714, -5.1919, -6.9244,\n",
       "         -0.7811,  4.4841],\n",
       "        [-3.6631, -4.5173,  1.1486, -3.5798, -0.8339, -1.9358,  3.3141,  1.8897,\n",
       "          1.1224,  1.5577],\n",
       "        [ 2.7387, -2.0768,  0.9836, -0.7142,  3.7925, -3.0117, -0.3672, -1.0644,\n",
       "          4.0226, -2.9036],\n",
       "        [-3.9206,  0.3760,  3.8509, -0.3813, -1.1643,  0.2692, -2.0817, -1.4092,\n",
       "         -0.0249,  0.6661],\n",
       "        [ 7.7065,  1.2497, -0.7021, -1.2242,  1.4426,  3.8796, -0.4188, -0.3122,\n",
       "          1.5511, -1.3689]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.ops.aten.linear(torch.randn(10, 10), torch.randn(10, 10), torch.randn(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
